
    

<!DOCTYPE html>
<html lang="en" class="no-js">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="applicable-device" content="pc,mobile">
    <meta name="access" content="Yes">

    
    
    <meta name="twitter:site" content="@SpringerLink"/>
    <meta name="twitter:card" content="summary_large_image"/>
    <meta name="twitter:image:alt" content="Content cover image"/>
    <meta name="twitter:title" content="Knowledge-driven description synthesis for floor plan interpretation"/>
    <meta name="twitter:description" content="International Journal on Document Analysis and Recognition (IJDAR) - Image captioning is a widely known problem in the area of AI. Caption generation from floor plan images has applications in..."/>
    <meta name="twitter:image" content="https://static-content.springer.com/cover/journal/10032/24/1.jpg"/>
    <meta name="journal_id" content="10032"/>
    <meta name="dc.title" content="Knowledge-driven description synthesis for floor plan interpretation"/>
    <meta name="dc.source" content="International Journal on Document Analysis and Recognition (IJDAR) 2021 24:1"/>
    <meta name="dc.format" content="text/html"/>
    <meta name="dc.publisher" content="Springer"/>
    <meta name="dc.date" content="2021-04-26"/>
    <meta name="dc.type" content="OriginalPaper"/>
    <meta name="dc.language" content="En"/>
    <meta name="dc.copyright" content="2021 The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature"/>
    <meta name="dc.rights" content="2021 The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature"/>
    <meta name="dc.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="dc.description" content="Image captioning is a widely known problem in the area of AI. Caption generation from floor plan images has applications in indoor path planning, real estate, and providing architectural solutions. Several methods have been explored in the literature for generating captions or semi-structured descriptions from floor plan images. Since only the caption is insufficient to capture fine-grained details, researchers also proposed descriptive paragraphs from images. However, these descriptions have a rigid structure and lack flexibility, making it difficult to use them in real-time scenarios. This paper offers two models, description synthesis from image cue (DSIC) and transformer-based description generation (TBDG), for text generation from floor plan images. These two models take advantage of modern deep neural networks for visual feature extraction and text generation. The difference between both models is in the way they take input from the floor plan image. The DSIC model takes only visual features automatically extracted by a deep neural network, while the TBDG model learns textual captions extracted from input floor plan images with paragraphs. The specific keywords generated in TBDG and understanding them with paragraphs make it more robust in a general floor plan image. Experiments were carried out on a large-scale publicly available dataset and compared with state-of-the-art techniques to show the proposed model&#8217;s superiority."/>
    <meta name="prism.issn" content="1433-2825"/>
    <meta name="prism.publicationName" content="International Journal on Document Analysis and Recognition (IJDAR)"/>
    <meta name="prism.publicationDate" content="2021-04-26"/>
    <meta name="prism.volume" content="24"/>
    <meta name="prism.number" content="1"/>
    <meta name="prism.section" content="OriginalPaper"/>
    <meta name="prism.startingPage" content="19"/>
    <meta name="prism.endingPage" content="32"/>
    <meta name="prism.copyright" content="2021 The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature"/>
    <meta name="prism.rightsAgent" content="journalpermissions@springernature.com"/>
    <meta name="prism.url" content="https://link.springer.com/article/10.1007/s10032-021-00367-3"/>
    <meta name="prism.doi" content="doi:10.1007/s10032-021-00367-3"/>
    <meta name="citation_pdf_url" content="https://link.springer.com/content/pdf/10.1007/s10032-021-00367-3.pdf"/>
    <meta name="citation_fulltext_html_url" content="https://link.springer.com/article/10.1007/s10032-021-00367-3"/>
    <meta name="citation_journal_title" content="International Journal on Document Analysis and Recognition (IJDAR)"/>
    <meta name="citation_journal_abbrev" content="IJDAR"/>
    <meta name="citation_publisher" content="Springer Berlin Heidelberg"/>
    <meta name="citation_issn" content="1433-2825"/>
    <meta name="citation_title" content="Knowledge-driven description synthesis for floor plan interpretation"/>
    <meta name="citation_volume" content="24"/>
    <meta name="citation_issue" content="1"/>
    <meta name="citation_publication_date" content="2021/06"/>
    <meta name="citation_online_date" content="2021/04/26"/>
    <meta name="citation_firstpage" content="19"/>
    <meta name="citation_lastpage" content="32"/>
    <meta name="citation_article_type" content="Original Paper"/>
    <meta name="citation_language" content="en"/>
    <meta name="dc.identifier" content="doi:10.1007/s10032-021-00367-3"/>
    <meta name="DOI" content="10.1007/s10032-021-00367-3"/>
    <meta name="size" content="233103"/>
    <meta name="citation_doi" content="10.1007/s10032-021-00367-3"/>
    <meta name="citation_springer_api_url" content="http://api.springer.com/xmldata/jats?q=doi:10.1007/s10032-021-00367-3&amp;api_key="/>
    <meta name="description" content="Image captioning is a widely known problem in the area of AI. Caption generation from floor plan images has applications in indoor path planning, real esta"/>
    <meta name="dc.creator" content="Goyal, Shreya"/>
    <meta name="dc.creator" content="Chattopadhyay, Chiranjoy"/>
    <meta name="dc.creator" content="Bhatnagar, Gaurav"/>
    <meta name="dc.subject" content="Image Processing and Computer Vision"/>
    <meta name="dc.subject" content="Pattern Recognition"/>
    <meta name="citation_reference" content="citation_journal_title=IJDAR; citation_title=Symbol and character recognition: application to engineering drawings; citation_author=S Adam, JM Ogier, C Cariou, R Mullot, J Labiche, J Gardes; citation_volume=3; citation_issue=2; citation_publication_date=2000; citation_pages=89-101; citation_doi=10.1007/s100320000033; citation_id=CR1"/>
    <meta name="citation_reference" content="Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning to align and translate. arXiv preprint 
                  arXiv:1409.0473
                  
                 (2014)"/>
    <meta name="citation_reference" content="Barducci, A., Marinai, S.: Object recognition in floor plans by graphs of white connected components. In: ICPR (2012)"/>
    <meta name="citation_reference" content="Chatterjee, M., Schwing, A.G.: Diverse and coherent paragraph generation from images. In: ECCV (2018)"/>
    <meta name="citation_reference" content="Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Doll&#225;r, P., Zitnick, C.L.: Microsoft COCO captions: data collection and evaluation server. arXiv preprint 
                  arXiv:1504.00325
                  
                 (2015)"/>
    <meta name="citation_reference" content="Cho, K., Van&#160;Merri&#235;nboer, B., Bahdanau, D., Bengio, Y.: On the properties of neural machine translation: encoder-decoder approaches. arXiv preprint 
                  arXiv:1409.1259
                  
                 (2014)"/>
    <meta name="citation_reference" content="de&#160;las Heras, L.P., Terrades, O.R., Robles, S., S&#225;nchez, G.: CVC-FP and SGT: a new database for structural floor plan analysis and its groundtruthing tool. IJDAR 18(1), 15&#8211;30 (2015)"/>
    <meta name="citation_reference" content="citation_journal_title=IJDAR; citation_title=Generation of synthetic documents for performance evaluation of symbol recognition &amp; spotting systems; citation_author=M Delalandre, E Valveny, T Pridmore, D Karatzas; citation_volume=13; citation_issue=3; citation_publication_date=2010; citation_pages=187-207; citation_doi=10.1007/s10032-010-0120-x; citation_id=CR8"/>
    <meta name="citation_reference" content="Dutta, A., Llados, J., Pal, U.: Symbol spotting in line drawings through graph paths hashing. In: ICDAR (2011)"/>
    <meta name="citation_reference" content="citation_journal_title=Pattern Recognit.; citation_title=A symbol spotting approach in graphical documents by hashing serialized graphs; citation_author=A Dutta, J Llad&#243;s, U Pal; citation_volume=46; citation_issue=3; citation_publication_date=2013; citation_pages=752-768; citation_doi=10.1016/j.patcog.2012.10.003; citation_id=CR10"/>
    <meta name="citation_reference" content="Farhadi, A., Hejrati, M., Sadeghi, M.A., Young, P., Rashtchian, C., Hockenmaier, J., Forsyth, D.: Every picture tells a story: generating sentences from images. In: ECCV (2010)"/>
    <meta name="citation_reference" content="Girshick, R.: Fast R-CNN. In: ICCV (2015)"/>
    <meta name="citation_reference" content="citation_journal_title=Image Process.; citation_title=SUGAMAN: describing floor plans for visually impaired by annotation learning and proximity-based grammar; citation_author=S Goyal, S Bhavsar, S Patel, C Chattopadhyay, G Bhatnagar; citation_volume=13; citation_issue=13; citation_publication_date=2019; citation_pages=2623-2635; citation_doi=10.1049/iet-ipr.2018.5627; citation_id=CR13"/>
    <meta name="citation_reference" content="Goyal, S., Chattopadhyay, C., Bhatnagar, G.: ASYSST: a framework for synopsis synthesis empowering visually impaired. In: MAHCI (2018)"/>
    <meta name="citation_reference" content="Goyal, S., Chattopadhyay, C., Bhatnagar, G.: Plan2Text: a framework for describing building floor plan images from first person perspective. In: CSPA (2018)"/>
    <meta name="citation_reference" content="Goyal, S., Mistry, V., Chattopadhyay, C., Bhatnagar, G.: BRIDGE: building plan repository for image description generation, and evaluation. In: ICDAR (2019)"/>
    <meta name="citation_reference" content="He, K., Gkioxari, G., Doll&#225;r, P., Girshick, R.: Mask R-CNN. In: ICCV (2017)"/>
    <meta name="citation_reference" content="citation_journal_title=T-PAMI; citation_title=Spatial pyramid pooling in deep convolutional networks for visual recognition; citation_author=K He, X Zhang, S Ren, J Sun; citation_volume=37; citation_issue=9; citation_publication_date=2015; citation_pages=1904-1916; citation_doi=10.1109/TPAMI.2015.2389824; citation_id=CR18"/>
    <meta name="citation_reference" content="citation_journal_title=Neural Comput.; citation_title=Long short-term memory; citation_author=S Hochreiter, J Schmidhuber; citation_volume=9; citation_issue=8; citation_publication_date=1997; citation_pages=1735-1780; citation_doi=10.1162/neco.1997.9.8.1735; citation_id=CR19"/>
    <meta name="citation_reference" content="Johnson, J., Karpathy, A., Fei-Fei, L.: Densecap: fully convolutional localization networks for dense captioning. In: CVPR (2016)"/>
    <meta name="citation_reference" content="citation_journal_title=Multimedia Tools Appl.; citation_title=A comparative study of graphic symbol recognition methods; citation_author=I Khan, N Islam, HU Rehman, M Khan; citation_volume=79; citation_issue=13; citation_publication_date=2020; citation_pages=8695-8725; citation_doi=10.1007/s11042-018-6289-6; citation_id=CR21"/>
    <meta name="citation_reference" content="Krause, J., Johnson, J., Krishna, R., Fei-Fei, L.: A hierarchical approach for generating descriptive image paragraphs. In: CVPR (2017)"/>
    <meta name="citation_reference" content="citation_journal_title=IJCV; citation_title=Visual genome: connecting language and vision using crowdsourced dense image annotations; citation_author=R Krishna, Y Zhu, O Groth, J Johnson, K Hata, J Kravitz, S Chen, Y Kalantidis, LJ Li, DA Shamma; citation_volume=123; citation_issue=1; citation_publication_date=2017; citation_pages=32-73; citation_doi=10.1007/s11263-016-0981-7; citation_id=CR23"/>
    <meta name="citation_reference" content="citation_journal_title=T-PAMI; citation_title=Babytalk: understanding and generating simple image descriptions; citation_author=G Kulkarni, V Premraj, V Ordonez, S Dhar, S Li, Y Choi, AC Berg, TL Berg; citation_volume=35; citation_issue=12; citation_publication_date=2013; citation_pages=2891-2903; citation_doi=10.1109/TPAMI.2012.162; citation_id=CR24"/>
    <meta name="citation_reference" content="Li, S., Kulkarni, G., Berg, T.L., Berg, A.C., Choi, Y.: Composing simple image descriptions using web-scale n-grams. In: CoNLL (2011)"/>
    <meta name="citation_reference" content="Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll&#225;r, P., Zitnick, C.L.: Microsoft COCO: common objects in context. In: ECCV (2014)"/>
    <meta name="citation_reference" content="Liu, Y., Fu, J., Mei, T., Chen, C.W.: Let your photos talk: Generating narrative paragraph for photo stream via bidirectional attention recurrent neural networks. In: AAAI (2017)"/>
    <meta name="citation_reference" content="Luong, M.T., Pham, H., Manning, C.D.: Effective approaches to attention-based neural machine translation. arXiv preprint 
                  arXiv:1508.04025
                  
                 (2015)"/>
    <meta name="citation_reference" content="citation_journal_title=ACM T-ACCESS; citation_title=Creating accessible online floor plans for visually impaired readers; citation_author=A Madugalla, K Marriott, S Marinai, S Capobianco, C Goncu; citation_volume=13; citation_issue=4; citation_publication_date=2020; citation_pages=1-37; citation_doi=10.1145/3410446; citation_id=CR29"/>
    <meta name="citation_reference" content="Mao, Y., Zhou, C., Wang, X., Li, R.: Show and tell more: topic-oriented multi-sentence image captioning. In: IJCAI (2018)"/>
    <meta name="citation_reference" content="Marcus, M., Santorini, B., Marcinkiewicz, M.A.: Building a large annotated corpus of English: The Penn Treebank (1993)"/>
    <meta name="citation_reference" content="Nallapati, R., Zhou, B., Gulcehre, C., Xiang, B., et&#160;al.: Abstractive text summarization using sequence-to-sequence RNNs and beyond. arXiv preprint 
                  arXiv:1602.06023
                  
                 (2016)"/>
    <meta name="citation_reference" content="Ordonez, V., Kulkarni, G., Berg, T.L.: Im2Text: describing images using 1 million captioned photographs. In: NIPS (2011)"/>
    <meta name="citation_reference" content="Park, C.C., Kim, G.: Expressing an image stream with a sequence of natural sentences. In: NIPS (2015)"/>
    <meta name="citation_reference" content="Qureshi, R.J., Ramel, J.Y., Barret, D., Cardot, H.: Spotting symbols in line drawing images using graph representations. In: GREC (2007)"/>
    <meta name="citation_reference" content="Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: unified, real-time object detection. In: CVPR (2016)"/>
    <meta name="citation_reference" content="Redmon, J., Farhadi, A.: YOLO9000: better, faster, stronger. In: CVPR (2017)"/>
    <meta name="citation_reference" content="Redmon, J., Farhadi, A.: YOLOv3: an incremental improvement. arXiv preprint 
                  arXiv:1804.02767
                  
                 (2018)"/>
    <meta name="citation_reference" content="Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time object detection with region proposal networks. In: NIPS (2015)"/>
    <meta name="citation_reference" content="Rezvanifar, A., Cote, M., Branzan&#160;Albu, A.: Symbol spotting on digital architectural floor plans using a deep learning-based framework. In: Proceedings of the IEEE/CVF CVPR Workshops, pp. 568&#8211;569 (2020)"/>
    <meta name="citation_reference" content="Rush, A.M., Chopra, S., Weston, J.: A neural attention model for abstractive sentence summarization. arXiv preprint 
                  arXiv:1509.00685
                  
                 (2015)"/>
    <meta name="citation_reference" content="Sabour, S., Frosst, N., Hinton, G.E.: Dynamic routing between capsules. In: Advances in neural information processing systems, pp. 3856&#8211;3866 (2017)"/>
    <meta name="citation_reference" content="Saha, R., Mondal, A., Jawahar, C.: Graphical Object Detection in Document Images. In: ICDAR (2019)"/>
    <meta name="citation_reference" content="Schreiber, S., Agne, S., Wolf, I., Dengel, A., Ahmed, S.: Deepdesrt: deep learning for detection and structure recognition of tables in document images. In: ICDAR (2017)"/>
    <meta name="citation_reference" content="Sharma, D., Gupta, N., Chattopadhyay, C., Mehta, S.: DANIEL: A deep architecture for automatic analysis and retrieval of building floor plans. In: ICDAR (2017)"/>
    <meta name="citation_reference" content="Sharma, N., Mandal, R., Sharma, R., Pal, U., Blumenstein, M.: Signature and Logo Detection using Deep CNN for Document Image Retrieval. In: ICFHR (2018)"/>
    <meta name="citation_reference" content="citation_journal_title=Pattern Recognition; citation_title=Scalable logo detection by self co-learning; citation_author=H Su, S Gong, X Zhu; citation_volume=97; citation_publication_date=2020; citation_pages=107003; citation_doi=10.1016/j.patcog.2019.107003; citation_id=CR47"/>
    <meta name="citation_reference" content="Sutskever, I., Vinyals, O., Le, Q.: Sequence to sequence learning with neural networks. NIPS (2014)"/>
    <meta name="citation_reference" content="Viola, P., Jones, M.: Rapid object detection using a boosted cascade of simple features. In: CVPR (2001)"/>
    <meta name="citation_reference" content="Wang, Q., Chan, A.B.: CNN+CNN: convolutional decoders for image captioning. arXiv preprint 
                  arXiv:1805.09019
                  
                 (2018)"/>
    <meta name="citation_reference" content="Wang, Z., Luo, Y., Li, Y., Huang, Z., Yin, H.: Look Deeper See Richer: Depth-aware Image Paragraph Captioning. In: ACM MM (2018)"/>
    <meta name="citation_reference" content="Yao, T., Pan, Y., Li, Y., Qiu, Z., Mei, T.: Boosting image captioning with attributes. In: ICCV (2017)"/>
    <meta name="citation_reference" content="Yi, X., Gao, L., Liao, Y., Zhang, X., Liu, R., Jiang, Z.: CNN based page object detection in document images. In: ICDAR (2017)"/>
    <meta name="citation_reference" content="Ziran, Z., Marinai, S.: Object detection in floor plan images. In: IAPR Workshop on Artificial Neural Networks in Pattern Recognition (2018)"/>
    <meta name="citation_author" content="Goyal, Shreya"/>
    <meta name="citation_author_institution" content="Indian Institute of Technology, Jodhpur, India"/>
    <meta name="citation_author" content="Chattopadhyay, Chiranjoy"/>
    <meta name="citation_author_email" content="chiranjoy@iitj.ac.in"/>
    <meta name="citation_author_institution" content="Indian Institute of Technology, Jodhpur, India"/>
    <meta name="citation_author" content="Bhatnagar, Gaurav"/>
    <meta name="citation_author_institution" content="Indian Institute of Technology, Jodhpur, India"/>
    <meta name="format-detection" content="telephone=no"/>
    <meta name="citation_cover_date" content="2021/06/01"/>
    

    
    
    <meta property="og:url" content="https://link.springer.com/article/10.1007/s10032-021-00367-3"/>
    <meta property="og:type" content="article"/>
    <meta property="og:site_name" content="SpringerLink"/>
    <meta property="og:title" content="Knowledge-driven description synthesis for floor plan interpretation - International Journal on Document Analysis and Recognition (IJDAR)"/>
    <meta property="og:description" content="Image captioning is a widely known problem in the area of AI. Caption generation from floor plan images has applications in indoor path planning, real estate, and providing architectural solutions. Several methods have been explored in the literature for generating captions or semi-structured descriptions from floor plan images. Since only the caption is insufficient to capture fine-grained details, researchers also proposed descriptive paragraphs from images. However, these descriptions have a rigid structure and lack flexibility, making it difficult to use them in real-time scenarios. This paper offers two models, description synthesis from image cue (DSIC) and transformer-based description generation (TBDG), for text generation from floor plan images. These two models take advantage of modern deep neural networks for visual feature extraction and text generation. The difference between both models is in the way they take input from the floor plan image. The DSIC model takes only visual features automatically extracted by a deep neural network, while the TBDG model learns textual captions extracted from input floor plan images with paragraphs. The specific keywords generated in TBDG and understanding them with paragraphs make it more robust in a general floor plan image. Experiments were carried out on a large-scale publicly available dataset and compared with state-of-the-art techniques to show the proposed model&#8217;s superiority."/>
    <meta property="og:image" content="https://media.springernature.com/w200/springer-static/cover/journal/10032.jpg"/>
    


    <title>Knowledge-driven description synthesis for floor plan interpretation | SpringerLink</title>

    <link rel="shortcut icon" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16 32x32 48x48" href=/oscar-static/images/favicons/springerlink/favicon-eb9f5576a3.ico />
<link rel="icon" sizes="16x16" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-16x16-8bd8c1c945.png />
<link rel="icon" sizes="32x32" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-32x32-61a52d80ab.png />
<link rel="icon" sizes="48x48" type="image/png" href=/oscar-static/images/favicons/springerlink/favicon-48x48-0ec46b6b10.png />
<link rel="apple-touch-icon" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />
<link rel="apple-touch-icon" sizes="72x72" href=/oscar-static/images/favicons/springerlink/ic_launcher_hdpi-f77cda7f65.png />
<link rel="apple-touch-icon" sizes="76x76" href=/oscar-static/images/favicons/springerlink/app-icon-ipad-c3fd26520d.png />
<link rel="apple-touch-icon" sizes="114x114" href=/oscar-static/images/favicons/springerlink/app-icon-114x114-3d7d4cf9f3.png />
<link rel="apple-touch-icon" sizes="120x120" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@2x-67b35150b3.png />
<link rel="apple-touch-icon" sizes="144x144" href=/oscar-static/images/favicons/springerlink/ic_launcher_xxhdpi-986442de7b.png />
<link rel="apple-touch-icon" sizes="152x152" href=/oscar-static/images/favicons/springerlink/app-icon-ipad@2x-677ba24d04.png />
<link rel="apple-touch-icon" sizes="180x180" href=/oscar-static/images/favicons/springerlink/app-icon-iphone@3x-f259d46347.png />


    
    <script>(function(H){H.className=H.className.replace(/\bno-js\b/,'js')})(document.documentElement)</script>

    
    
        <style>@media only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark) { html{text-size-adjust:100%;-webkit-font-smoothing:subpixel-antialiased;box-sizing:border-box;color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:100%;height:100%;line-height:1.61803;overflow-y:scroll}body,img{max-width:100%}body{background:#fcfcfc;font-size:1.125rem;line-height:1.5;min-height:100%}main{display:block}h1{font-family:Georgia,Palatino,serif;font-size:min(max(1.75rem,4vw),2.25rem);font-style:normal;font-weight:400;line-height:1.4;margin:.67em 0}a{background-color:transparent;color:#004b83;overflow-wrap:break-word;text-decoration:underline .0625rem;text-decoration-skip-ink:auto;text-underline-offset:.08em;word-break:break-word}b{font-weight:bolder}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}img{border:0;height:auto;vertical-align:middle}button,input{font-family:inherit;font-size:100%}input{line-height:1.15}button,input{overflow:visible}button{text-transform:none}[type=submit],button{appearance:button}[hidden]{display:none}button{border-radius:0;cursor:pointer;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;line-height:inherit}*{margin:0}h2{font-size:min(max(1.5rem,3.5vw),1.75rem)}h2,h3{font-family:Georgia,Palatino,serif;font-weight:400;line-height:1.4}h3{font-size:min(max(1.25rem,3vw),1.5rem)}h2,h3{font-style:normal}label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}*{box-sizing:inherit}body,button,div,form,input,p{margin:0;padding:0}a>img{vertical-align:middle}p{overflow-wrap:break-word;word-break:break-word}.c-ad{text-align:center}@media only screen and (min-width:320px){.c-ad{padding:8px}}.c-ad--728x90{background-color:#ccc;display:none}.c-ad--728x90 .c-ad__inner{min-height:calc(1.5em + 94px)}.c-ad--728x90 iframe{height:90px;max-width:970px}@media only screen and (min-width:768px){.js .c-ad--728x90{display:none}.js .u-show-following-ad+.c-ad--728x90{display:block}}.c-ad iframe{border:0;overflow:auto;vertical-align:top}.c-ad__label{color:#333;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;font-weight:400;line-height:1.5;margin-bottom:4px}.c-breadcrumbs>li{display:inline}.c-skip-link{background:#f7fbfe;color:#004b83;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:.875rem;inset:0 0 auto;padding:8px;position:absolute;text-align:center;transform:translateY(-100%);z-index:9999}@media (prefers-reduced-motion:reduce){.c-skip-link{transition:top .3s ease-in-out 0s}}@media print{.c-skip-link{display:none}}.c-skip-link:link{color:#004b83}.c-status-message{align-items:center;box-sizing:border-box;display:flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;position:relative;width:100%}.c-status-message :last-child{margin-bottom:0}.c-status-message--boxed{background-color:#fff;border:1px solid #ccc;border-radius:2px;line-height:1.4;padding:16px}.c-status-message__icon{fill:currentcolor;display:inline-block;flex:0 0 auto;height:1.5em;margin-right:8px;transform:translate(0);vertical-align:text-top;width:1.5em}.c-status-message--info .c-status-message__icon{color:#003f8d}.c-status-message--boxed.c-status-message--info{border-bottom:4px solid #003f8d}.c-pagination{align-items:center;display:flex;flex-wrap:wrap;font-size:.875rem;list-style:none;margin:0;padding:16px}@media only screen and (min-width:540px){.c-pagination{justify-content:center}}.c-pagination__item{margin-bottom:8px;margin-right:16px}.c-pagination__item:last-child{margin-right:0}.c-pagination__link{align-items:center;background-color:#f2f2f2;background-image:linear-gradient(#fff,#f2f2f2);border:1px solid #ccc;border-radius:2px;color:#004b83;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;min-width:30px;padding:8px;position:relative;text-align:center;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.c-pagination__link svg,.c-pagination__link--disabled svg{fill:currentcolor}.c-pagination__link:visited{color:#004b83}.c-pagination__link:focus,.c-pagination__link:hover{border:1px solid #666;text-decoration:none}.c-pagination__link:focus,.c-pagination__link:hover{background-color:#666;background-image:none;color:#fff}.c-pagination__link:focus svg path,.c-pagination__link:hover svg path{fill:#fff}.c-pagination__link--disabled{align-items:center;background-color:transparent;background-image:none;border-radius:2px;color:#333;cursor:default;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;opacity:.67;padding:8px;position:relative;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.c-pagination__link--disabled:visited{color:#333}.c-pagination__link--disabled,.c-pagination__link--disabled:focus,.c-pagination__link--disabled:hover{border:1px solid #ccc;text-decoration:none}.c-pagination__link--disabled:focus,.c-pagination__link--disabled:hover{background-color:transparent;background-image:none;color:#333}.c-pagination__link--disabled:focus svg path,.c-pagination__link--disabled:hover svg path{fill:#333}.c-pagination__link--active{background-color:#666;background-image:none;border-color:#666;color:#fff;cursor:default}.c-pagination__ellipsis{background:0 0;border:0;min-width:auto;padding-left:0;padding-right:0}.c-pagination__icon{fill:#999;height:12px;width:16px}.c-pagination__icon--active{fill:#004b83}.c-breadcrumbs{color:#000;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;list-style:none;margin:0;padding:0}.c-breadcrumbs__link{color:#666}svg.c-breadcrumbs__chevron{fill:#666;height:10px;margin:4px 4px 0;width:10px}.c-article-header{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;margin-bottom:40px}.c-article-identifiers{color:#6f6f6f;display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3;list-style:none;margin:0 0 8px;padding:0}.c-article-identifiers__item{border-right:1px solid #6f6f6f;list-style:none;margin-right:8px;padding-right:8px}.c-article-identifiers__item:last-child{border-right:0;margin-right:0;padding-right:0}.c-article-title{font-size:1.5rem;line-height:1.25;margin:0 0 16px}@media only screen and (min-width:768px){.c-article-title{font-size:1.875rem;line-height:1.2}}.c-article-author-list{display:inline;font-size:1rem;list-style:none;margin:0 8px 0 0;padding:0;width:100%}.c-article-author-list__item{display:inline;padding-right:0}.c-article-author-list svg{margin-left:4px}.c-article-author-list__show-more{display:none;margin-right:4px}.c-article-author-list__button,.js .c-article-author-list__item--hide,.js .c-article-author-list__show-more{display:none}.js .c-article-author-list--long .c-article-author-list__show-more,.js .c-article-author-list--long+.c-article-author-list__button{display:inline}@media only screen and (max-width:539px){.js .c-article-author-list__item--hide-small-screen{display:none}.js .c-article-author-list--short .c-article-author-list__show-more,.js .c-article-author-list--short+.c-article-author-list__button{display:inline}}#uptodate-client,.js .c-article-author-list--expanded .c-article-author-list__show-more{display:none!important}.js .c-article-author-list--expanded .c-article-author-list__item--hide-small-screen{display:inline!important}.c-article-author-list__button,.c-button-author-list{background:#ebf1f5;border:4px solid #ebf1f5;border-radius:20px;color:#666;font-size:.875rem;line-height:1.4;padding:2px 11px 2px 8px;text-decoration:none}.c-article-author-list__button svg,.c-button-author-list svg{margin:1px 4px 0 0}.c-article-author-list__button:hover,.c-button-author-list:hover{background:#069;border-color:transparent;color:#fff}.c-article-info-details{font-size:1rem;margin-bottom:8px;margin-top:16px}.c-article-info-details__cite-as{border-left:1px solid #6f6f6f;margin-left:8px;padding-left:8px}.c-article-metrics-bar{display:flex;flex-wrap:wrap;font-size:1rem;line-height:1.3}.c-article-metrics-bar__wrapper{margin:16px 0}.c-article-metrics-bar__item{align-items:baseline;border-right:1px solid #6f6f6f;margin-right:8px}.c-article-metrics-bar__item:last-child{border-right:0}.c-article-metrics-bar__count{font-weight:700;margin:0}.c-article-metrics-bar__label{color:#626262;font-style:normal;font-weight:400;margin:0 10px 0 5px}.c-article-metrics-bar__details{margin:0}.c-article-main-column{font-family:Georgia,Palatino,serif;margin-right:8.6%;width:60.2%}@media only screen and (max-width:1023px){.c-article-main-column{margin-right:0;width:100%}}.c-article-extras{float:left;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;width:31.2%}@media only screen and (max-width:1023px){.c-article-extras{display:none}}.c-article-section__title{border-bottom:2px solid #d5d5d5;font-size:1.25rem;margin:0;padding-bottom:8px}@media only screen and (min-width:768px){.c-article-section__title{font-size:1.5rem;line-height:1.24}}.c-article-body p{margin-bottom:24px;margin-top:0}.c-article-section{clear:both}.c-article-section__content{margin-bottom:40px;padding-top:8px}@media only screen and (max-width:1023px){.c-article-section__content{padding-left:0}}.c-article-authors-search{margin-bottom:24px;margin-top:0}.c-article-authors-search__item,.c-article-authors-search__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-authors-search__title{color:#626262;font-size:1.05rem;font-weight:700;margin:0;padding:0}.c-article-authors-search__item{font-size:1rem}.c-article-authors-search__text{margin:0}.c-code-block{border:1px solid #f2f2f2;font-family:monospace;margin:0 0 24px;padding:20px}.c-code-block__heading{font-weight:400;margin-bottom:16px}.c-code-block__line{display:block;overflow-wrap:break-word;white-space:pre-wrap}.c-article-share-box__no-sharelink-info{font-size:.813rem;font-weight:700;margin-bottom:24px;padding-top:4px}.c-article-share-box__only-read-input{border:1px solid #d5d5d5;box-sizing:content-box;display:inline-block;font-size:.875rem;font-weight:700;height:24px;margin-bottom:8px;padding:8px 10px}.c-article-share-box__button--link-like{background-color:transparent;border:0;color:#069;cursor:pointer;font-size:.875rem;margin-bottom:8px;margin-left:10px}.c-article-associated-content__container .c-article-associated-content__collection-label{line-height:1.4}.c-article-associated-content__container .c-article-associated-content__collection-title{line-height:1.3}.c-context-bar{box-shadow:0 0 10px 0 rgba(51,51,51,.2);position:relative;width:100%}.c-context-bar__title{display:none}.c-reading-companion{clear:both;min-height:389px}.c-reading-companion__sticky{max-width:389px}.c-reading-companion__scroll-pane{margin:0;min-height:200px;overflow:hidden auto}.c-reading-companion__tabs{display:flex;flex-flow:row nowrap;font-size:1rem;list-style:none;margin:0 0 8px;padding:0}.c-reading-companion__tabs>li{flex-grow:1}.c-reading-companion__tab{background-color:#eee;border:1px solid #d5d5d5;border-image:initial;border-left-width:0;color:#069;font-size:1rem;padding:8px 8px 8px 15px;text-align:left;width:100%}.c-reading-companion__tabs li:first-child .c-reading-companion__tab{border-left-width:1px}.c-reading-companion__tab--active{background-color:#fcfcfc;border-bottom:1px solid #fcfcfc;color:#222;font-weight:700}.c-reading-companion__sections-list{list-style:none;padding:0}.c-reading-companion__figures-list,.c-reading-companion__references-list{list-style:none;min-height:389px;padding:0}.c-reading-companion__references-list--numeric{list-style:decimal inside}.c-reading-companion__sections-list{margin:0 0 8px;min-height:50px}.c-reading-companion__section-item{font-size:1rem;padding:0}.c-reading-companion__section-item a{display:block;line-height:1.5;overflow:hidden;padding:8px 0 8px 16px;text-overflow:ellipsis;white-space:nowrap}.c-reading-companion__figure-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:16px 8px 16px 0}.c-reading-companion__figure-item:first-child{border-top:none;padding-top:8px}.c-reading-companion__reference-item{border-top:1px solid #d5d5d5;font-size:1rem;padding:8px 8px 8px 16px}.c-reading-companion__reference-item:first-child{border-top:none}.c-reading-companion__reference-item a{word-break:break-word}.c-reading-companion__reference-citation{display:inline}.c-reading-companion__reference-links{font-size:.813rem;font-weight:700;list-style:none;margin:8px 0 0;padding:0;text-align:right}.c-reading-companion__reference-links>a{display:inline-block;padding-left:8px}.c-reading-companion__reference-links>a:first-child{display:inline-block;padding-left:0}.c-reading-companion__figure-title{display:block;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:min(max(1.125rem,2.5vw),1.25rem);font-style:normal;font-weight:700;line-height:1.4;margin:0 0 8px}.c-reading-companion__figure-links{display:flex;justify-content:space-between;margin:8px 0 0}.c-reading-companion__figure-links>a{align-items:center;display:flex}.c-reading-companion__figure-full-link svg{height:.8em;margin-left:2px}.c-reading-companion__panel{border-top:none;display:none;margin-top:0;padding-top:0}.c-reading-companion__panel--active{display:block}.c-article-section__figure-description{font-size:1rem}.c-article-section__figure-description>*{margin-bottom:0}.c-cod{display:block;font-size:1rem;width:100%}.c-cod__form{background:#ebf0f3}.c-cod__prompt{font-size:1.125rem;line-height:1.3;margin:0 0 24px}.c-cod__label{display:block;margin:0 0 4px}.c-cod__row{display:flex;margin:0 0 16px}.c-cod__row:last-child{margin:0}.c-cod__input{border:1px solid #d5d5d5;border-radius:2px;flex-basis:75%;flex-shrink:0;margin:0;padding:13px}.c-cod__input--submit{color:#fff;flex-shrink:1;margin-left:8px;transition:background-color .2s ease-out 0s,color .2s ease-out 0s}.c-cod__input--submit-single{flex-basis:100%;flex-shrink:0;margin:0}.c-cod__input--submit:focus,.c-cod__input--submit:hover{background-color:#fff}.c-pdf-download__link .u-icon{padding-top:2px}.save-data .c-article-author-institutional-author__sub-division,.save-data .c-article-equation__number,.save-data .c-article-figure-description,.save-data .c-article-fullwidth-content,.save-data .c-article-main-column,.save-data .c-article-satellite-article-link,.save-data .c-article-satellite-subtitle,.save-data .c-article-table-container,.save-data .c-blockquote__body,.save-data .c-code-block__heading,.save-data .c-reading-companion__figure-title,.save-data .c-reading-companion__reference-citation,.save-data .c-site-messages--nature-briefing-email-variant .serif,.save-data .c-site-messages--nature-briefing-email-variant.serif,.save-data .serif,.save-data .u-serif,.save-data h1,.save-data h2,.save-data h3{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-pdf-download{display:flex;margin-bottom:16px;max-height:48px}@media only screen and (min-width:540px){.c-pdf-download{max-height:none}}@media only screen and (min-width:1024px){.c-pdf-download{max-height:48px}}.c-pdf-download__link{display:flex;flex:1 1 0%}.c-pdf-download__text{padding-right:4px}@media only screen and (max-width:539px){.c-pdf-download__text{text-transform:capitalize}}@media only screen and (min-width:540px){.c-pdf-download__text{padding-right:8px}}.c-pdf-container{display:flex;justify-content:flex-end}@media only screen and (max-width:539px){.c-pdf-container .c-pdf-download{display:flex;flex-basis:100%}}.c-article-extras .c-pdf-container{flex-wrap:wrap;width:100%}.c-article-extras .c-pdf-container .c-pdf-download{width:100%}.u-button svg,.u-button--primary svg{fill:currentcolor}.app-checklist-banner{border:2px solid #ebf1f5;display:flex;flex:1 1 auto;font-size:1rem;justify-content:space-between;margin-bottom:16px;padding:16px}.app-checklist-banner--on-mobile{display:block;margin-bottom:32px}@media only screen and (min-width:1024px){.app-checklist-banner--on-mobile{display:none}}.app-checklist-banner__title{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.125rem;font-weight:700;margin-bottom:0;margin-top:0}.app-checklist-banner__icon-container{align-items:center;display:flex;flex:0 0 60px;justify-content:flex-end;width:60px}.app-checklist-banner__link{align-items:center;color:#004b83;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.app-checklist-banner__arrow-icon,.app-checklist-banner__paper-icon{fill:currentcolor;display:inline-block;transform:translate(0);vertical-align:text-top}.app-checklist-banner__paper-icon{height:36px!important;width:36px!important}.app-checklist-banner__arrow-icon{height:11px;margin:4px 0 0 8px;width:16px}.app-elements .c-header{background-color:#fff;border-bottom:2px solid #01324b;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:16px;line-height:1.4;padding:8px 0 0}.app-elements .c-header__container{align-items:center;display:flex;flex-wrap:nowrap;gap:8px 16px;justify-content:space-between;margin:0 auto;max-width:1280px;padding:0 8px;position:relative}.app-elements .c-header__nav{border-top:2px solid #cedbe0;margin-top:8px;padding-top:4px;position:relative}.app-elements .c-header__nav-container{align-items:center;display:flex;flex-wrap:wrap;margin:0 auto 4px;max-width:1280px;padding:0 8px;position:relative}.app-elements .c-header__nav-container>:not(:last-child){margin-right:32px}.app-elements .c-header__link-container{align-items:center;display:flex;flex:1 0 auto;gap:8px 16px;justify-content:space-between}.app-elements .c-header__list{list-style:none;margin:0;padding:0}.app-elements .c-header__list-item{font-weight:700;margin:0 auto;max-width:1280px;padding:8px}.app-elements .c-header__list-item:not(:last-child){border-bottom:2px solid #cedbe0}.app-elements .c-header__item{color:inherit}@media only screen and (min-width:540px){.app-elements .c-header__item--menu{display:none;visibility:hidden}.app-elements .c-header__item--menu:first-child+*{margin-block-start:0}}.app-elements .c-header__item--inline-links{display:none;visibility:hidden}@media only screen and (min-width:540px){.app-elements .c-header__item--inline-links{display:flex;gap:16px;visibility:visible}}.app-elements .c-header__item--divider:before{border-left:2px solid #cedbe0;content:"";height:calc(100% - 16px);margin-left:-15px;position:absolute;top:8px}.app-elements .c-header__brand a{display:block;line-height:1;padding:16px 8px;text-decoration:none}.app-elements .c-header__brand img{height:24px;width:auto}.app-elements .c-header__link{color:inherit;display:inline-block;font-weight:700;padding:16px 8px;position:relative;text-decoration-color:transparent;word-break:normal}.app-elements .c-header__link--static{flex:0 0 auto}.app-elements .c-header__icon{fill:currentcolor;display:inline-block;font-size:24px;height:1em;transform:translate(0);vertical-align:bottom;width:1em}.app-elements .c-header__icon+*{margin-left:8px}.app-elements .c-header__expander{background-color:#ebf1f5}.app-elements .c-header__search{padding:24px 0}@media only screen and (min-width:540px){.app-elements .c-header__search{max-width:70%}}.app-elements .c-header__search-container{position:relative}.app-elements .c-header__search-label{color:inherit;display:inline-block;font-weight:700;margin-bottom:8px}.app-elements .c-header__search-input{background-color:#fff;border:1px solid #000;padding:8px;width:100%}.app-elements .c-header__search-button{background-color:transparent;border:0;color:inherit;height:100%;padding:0 8px;position:absolute;right:0}.app-elements .has-tethered.c-header__expander{border-bottom:2px solid #01324b;left:0;margin-top:-2px;top:100%;width:100%;z-index:10}@media only screen and (min-width:540px){.app-elements .has-tethered.c-header__expander--menu{display:none;visibility:hidden}}.app-elements .has-tethered .c-header__heading{display:none;visibility:hidden}.app-elements .has-tethered .c-header__heading:first-child+*{margin-block-start:0}.app-elements .has-tethered .c-header__search{margin:auto}.app-elements .c-header__heading{margin:0 auto;max-width:1280px;padding:0 16px}.u-button{align-items:center;background-color:#f2f2f2;background-image:linear-gradient(#fff,#f2f2f2);border:1px solid #ccc;border-radius:2px;color:#004b83;cursor:pointer;display:inline-flex;font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1rem;justify-content:center;line-height:1.3;margin:0;padding:8px;position:relative;text-decoration:none;transition:all .25s ease 0s,color .25s ease 0s,border-color .25s ease 0s;width:auto}.u-button--primary{background-color:#33629d;background-image:linear-gradient(#4d76a9,#33629d);border:1px solid rgba(0,59,132,.5);color:#fff}.u-button--full-width{display:flex;width:100%}.u-clearfix:after,.u-clearfix:before{content:"";display:table}.u-clearfix:after{clear:both}.u-justify-content-space-between{justify-content:space-between}.u-display-none{display:none}.js .u-js-hide{display:none;visibility:hidden}@media print{.u-hide-print{display:none}}.u-icon{fill:currentcolor;display:inline-block;height:1em;transform:translate(0);vertical-align:text-top;width:1em}.u-list-reset{list-style:none;margin:0;padding:0}.u-container{margin:0 auto;max-width:1280px;padding:0 16px}.u-mt-0{margin-top:0}.u-mt-32{margin-top:32px}.u-mb-8{margin-bottom:8px}.u-mb-16{margin-bottom:16px}.u-mb-24{margin-bottom:24px}.u-mb-32{margin-bottom:32px}.u-float-left{float:left}.u-hide{display:none;visibility:hidden}.u-hide:first-child+*{margin-block-start:0}@media only screen and (min-width:1024px){.u-hide-at-lg{display:none;visibility:hidden}}.u-visually-hidden{clip:rect(0,0,0,0);border:0;height:1px;margin:-100%;overflow:hidden;padding:0;position:absolute!important;width:1px}.u-text-sm{font-size:1rem}.u-h4{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:min(max(1.125rem,2.5vw),1.25rem);font-style:normal;font-weight:700;line-height:1.4}.visually-hidden{clip:rect(1px,1px,1px,1px);height:1px;position:absolute!important;width:1px}.c-article-section__figure-description{font-family:Georgia,Palatino,serif}.c-article-section__content p{line-height:1.8}.c-pagination__input{border:1px solid #bfbfbf;border-radius:2px;box-shadow:inset 0 2px 6px 0 rgba(51,51,51,.2);box-sizing:initial;display:inline-block;height:28px;margin:0;max-width:64px;min-width:16px;padding:0 8px;text-align:center;transition:width .15s ease 0s}.c-pagination__input::-webkit-inner-spin-button,.c-pagination__input::-webkit-outer-spin-button{appearance:none;margin:0}@media only screen and (min-width:1024px){.c-article-collection__container{display:none}}.c-article-associated-content__container .c-article-associated-content__collection-label{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif;font-size:1.063rem}.c-article-associated-content__container .c-article-associated-content__collection-title{font-size:1.063rem;font-weight:400}.c-reading-companion__sections-list{font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Oxygen-Sans,Ubuntu,Cantarell,Helvetica Neue,sans-serif}.c-article-section__title,.c-article-title{font-weight:400}.c-header__cart-icon{margin-right:12px}.c-pdf-download__link{padding:13px 24px} }</style>



        <link rel="stylesheet" data-test="critical-css-handler" data-inline-css-source="critical-css" href="/oscar-static/app-springerlink/css/enhanced-article-682756ed77.css" media="print" onload="this.media='only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)';this.onload=null">
        
    



    
    <script>
        window.dataLayer = [{"GA Key":"UA-26408784-1","DOI":"10.1007/s10032-021-00367-3","Page":"article","springerJournal":true,"page":{"attributes":{"environment":"live"}},"Country":"IN","japan":false,"doi":"10.1007-s10032-021-00367-3","Journal Title":"International Journal on Document Analysis and Recognition (IJDAR)","Journal Id":10032,"Keywords":"Floor plan, Captioning, Evaluation, Language modeling","kwrd":["Floor_plan","Captioning","Evaluation","Language_modeling"],"Labs":"Y","ksg":"Krux.segments","kuid":"Krux.uid","Has Body":"Y","Features":[],"Open Access":"N","hasAccess":"N","bypassPaywall":"N","user":{"license":{"businessPartnerID":[],"businessPartnerIDString":""}},"Bpids":"","Bpnames":"","BPID":["1"],"VG Wort Identifier":"pw-vgzm.415900-10.1007-s10032-021-00367-3","Full HTML":"N","Subject Codes":["SCI","SCI22021","SCI2203X"],"pmc":["I","I22021","I2203X"],"session":{"authentication":{"loginStatus":"N"},"attributes":{"edition":"academic"}},"content":{"serial":{"eissn":"1433-2825","pissn":"1433-2833"},"type":"Article","category":{"pmc":{"primarySubject":"Computer Science","primarySubjectCode":"I","secondarySubjects":{"1":"Image Processing and Computer Vision","2":"Pattern Recognition"},"secondarySubjectCodes":{"1":"I22021","2":"I2203X"}},"sucode":"SC6"},"attributes":{"deliveryPlatform":"oscar"}},"Event Category":"Article"}];
    </script>

    <script>
    window.dataLayer.push({
        ga4MeasurementId: 'G-B3E4QL2TPR',
        ga360TrackingId: 'UA-26408784-1',
        twitterId: 'o47a7',
        ga4ServerUrl: 'https://collect.springer.com',
        imprint: 'springerlink',
        page: {
            attributes:{
                featureFlags: [{ name: 'darwin-orion', active: false }],
                darwinAvailable: false
            }
        }
        
    });
</script>

    <script data-test="gtm-head">
    window.initGTM = function() {
        if (window.config.mustardcut) {
            (function (w, d, s, l, i) {
                w[l] = w[l] || [];
                w[l].push({'gtm.start': new Date().getTime(), event: 'gtm.js'});
                var f = d.getElementsByTagName(s)[0],
                        j = d.createElement(s),
                        dl = l != 'dataLayer' ? '&l=' + l : '';
                j.async = true;
                j.src = 'https://www.googletagmanager.com/gtm.js?id=' + i + dl;
                f.parentNode.insertBefore(j, f);
            })(window, document, 'script', 'dataLayer', 'GTM-MRVXSHQ');
        }
    }
</script>
    <script>
    (function(w,d,t) {
        function cc() {
            var h = w.location.hostname;
            var e = d.createElement(t),
                    s = d.getElementsByTagName(t)[0];

            if (h.indexOf('springer.com') > -1) {
                e.src = 'https://cmp-static.springer.com/production_live/consent-bundle-17-35.js';
                e.setAttribute('onload', "initGTM(window,document,'script','dataLayer','GTM-MRVXSHQ')");
            } else {
                e.src = '/static/js/lib/cookie-consent.min.js';
                e.setAttribute('data-consent', h);
            }
            s.insertAdjacentElement('afterend', e);
        }

        cc();
    })(window,document,'script');
</script>

    <script>
    (function(w, d) {
        w.config = w.config || {};
        w.config.mustardcut = false;

        
        if (w.matchMedia && w.matchMedia('only print, only all and (prefers-color-scheme: no-preference), only all and (prefers-color-scheme: light), only all and (prefers-color-scheme: dark)').matches) {
            w.config.mustardcut = true;
            d.classList.add('js');
            d.classList.remove('grade-c');
            d.classList.remove('no-js');
        }
    })(window, document.documentElement);
</script>


    
<script>
    (function () {
        if ( typeof window.CustomEvent === "function" ) return false;
        function CustomEvent ( event, params ) {
            params = params || { bubbles: false, cancelable: false, detail: null };
            var evt = document.createEvent( 'CustomEvent' );
            evt.initCustomEvent( event, params.bubbles, params.cancelable, params.detail );
            return evt;
        }

        CustomEvent.prototype = window.Event.prototype;

        window.CustomEvent = CustomEvent;
    })();
</script>



    <script class="js-entry">
    if (window.config.mustardcut) {
        (function(w, d) {
            
            
            
                window.Component = {};
                window.suppressShareButton = false;
                window.onArticlePage = true;
            

            var currentScript = d.currentScript || d.head.querySelector('script.js-entry');

            
            function catchNoModuleSupport() {
                var scriptEl = d.createElement('script');
                return (!('noModule' in scriptEl) && 'onbeforeload' in scriptEl)
            }

            var headScripts = [
                {'src': '/oscar-static/js/polyfill-es5-bundle-51eb718839.js', 'async': false},
                {'src': '/oscar-static/js/airbrake-es5-bundle-57b9f2624a.js', 'async': false},
            ];

            var bodyScripts = [
                
                    {'src': '/oscar-static/js/app-es5-bundle-9e851d5bc0.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/app-es6-bundle-208aa30589.js', 'async': false, 'module': true}
                
                
                
                    , {'src': '/oscar-static/js/global-article-es5-bundle-e595d418d4.js', 'async': false, 'module': false},
                    {'src': '/oscar-static/js/global-article-es6-bundle-38f74de5cf.js', 'async': false, 'module': true}
                
            ];

            function createScript(script) {
                var scriptEl = d.createElement('script');
                scriptEl.src = script.src;
                scriptEl.async = script.async;
                if (script.module === true) {
                    scriptEl.type = "module";
                    if (catchNoModuleSupport()) {
                        scriptEl.src = '';
                    }
                } else if (script.module === false) {
                    scriptEl.setAttribute('nomodule', true)
                }
                if (script.charset) {
                    scriptEl.setAttribute('charset', script.charset);
                }

                return scriptEl;
            }

            for (var i = 0; i < headScripts.length; ++i) {
                var scriptEl = createScript(headScripts[i]);
                currentScript.parentNode.insertBefore(scriptEl, currentScript.nextSibling);
            }

            d.addEventListener('DOMContentLoaded', function() {
                for (var i = 0; i < bodyScripts.length; ++i) {
                    var scriptEl = createScript(bodyScripts[i]);
                    d.body.appendChild(scriptEl);
                }
            });

            // Webfont repeat view
            var config = w.config;
            if (config && config.publisherBrand && sessionStorage.fontsLoaded === 'true') {
                d.documentElement.className += ' webfonts-loaded';
            }
        })(window, document);
    }
</script>

    
    
    <link rel="canonical" href="https://link.springer.com/article/10.1007/s10032-021-00367-3"/>
    

    
    <script type="application/ld+json">{"mainEntity":{"headline":"Knowledge-driven description synthesis for floor plan interpretation","description":"Image captioning is a widely known problem in the area of AI. Caption generation from floor plan images has applications in indoor path planning, real estate, and providing architectural solutions. Several methods have been explored in the literature for generating captions or semi-structured descriptions from floor plan images. Since only the caption is insufficient to capture fine-grained details, researchers also proposed descriptive paragraphs from images. However, these descriptions have a rigid structure and lack flexibility, making it difficult to use them in real-time scenarios. This paper offers two models, description synthesis from image cue (DSIC) and transformer-based description generation (TBDG), for text generation from floor plan images. These two models take advantage of modern deep neural networks for visual feature extraction and text generation. The difference between both models is in the way they take input from the floor plan image. The DSIC model takes only visual features automatically extracted by a deep neural network, while the TBDG model learns textual captions extracted from input floor plan images with paragraphs. The specific keywords generated in TBDG and understanding them with paragraphs make it more robust in a general floor plan image. Experiments were carried out on a large-scale publicly available dataset and compared with state-of-the-art techniques to show the proposed model’s superiority.","datePublished":"2021-04-26","dateModified":"2021-04-26","pageStart":"19","pageEnd":"32","sameAs":"https://doi.org/10.1007/s10032-021-00367-3","keywords":"Image Processing and Computer Vision,Pattern Recognition","image":"https://static-content.springer.com/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig1_HTML.png","isPartOf":{"name":"International Journal on Document Analysis and Recognition (IJDAR)","issn":["1433-2825","1433-2833"],"volumeNumber":"24","@type":["Periodical","PublicationVolume"]},"publisher":{"name":"Springer Berlin Heidelberg","logo":{"url":"https://www.springernature.com/app-sn/public/images/logo-springernature.png","@type":"ImageObject"},"@type":"Organization"},"author":[{"name":"Shreya Goyal","affiliation":[{"name":"Indian Institute of Technology","address":{"name":"Indian Institute of Technology, Jodhpur, India","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"},{"name":"Chiranjoy Chattopadhyay","url":"http://orcid.org/0000-0002-3431-0483","affiliation":[{"name":"Indian Institute of Technology","address":{"name":"Indian Institute of Technology, Jodhpur, India","@type":"PostalAddress"},"@type":"Organization"}],"email":"chiranjoy@iitj.ac.in","@type":"Person"},{"name":"Gaurav Bhatnagar","affiliation":[{"name":"Indian Institute of Technology","address":{"name":"Indian Institute of Technology, Jodhpur, India","@type":"PostalAddress"},"@type":"Organization"}],"@type":"Person"}],"isAccessibleForFree":false,"hasPart":{"isAccessibleForFree":false,"cssSelector":".main-content","@type":"WebPageElement"},"@type":"ScholarlyArticle"},"@context":"https://schema.org","@type":"WebPage"}</script>

</head>
<body class="shared-article-renderer">
    
    
    
        
            <!-- Google Tag Manager (noscript) -->
            <noscript data-test="gtm-body">
                <iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MRVXSHQ"
                height="0" width="0" style="display:none;visibility:hidden"></iframe>
            </noscript>
            <!-- End Google Tag Manager (noscript) -->
        
    


    <div class="u-vh-full">
        <a class="c-skip-link" href="#main-content">Skip to main content</a>

        
            
        <div class="u-hide u-show-following-ad"></div>
        <aside class="c-ad c-ad--728x90" data-test="springer-doubleclick-ad">
            <div class="c-ad__inner">
                <p class="c-ad__label">Advertisement</p>
                <div id="div-gpt-ad-LB1" data-pa11y-ignore data-gpt data-test="LB1-ad"
                     data-gpt-unitpath="/270604982/springerlink/10032/article" data-gpt-sizes="728x90"
                     style="min-width:728px;min-height:90px" data-gpt-targeting="pos=LB1;articleid=s10032-021-00367-3;"></div>
            </div>
        </aside>

            
<div class="app-elements u-mb-24">
    <header class="c-header" data-header>
    <div class="c-header__container" data-header-expander-anchor>
        <div class="c-header__brand">
            <a href="https://link.springer.com"
               data-test="logo"
               data-track="click"
               data-track-action="click logo link"
               data-track-category="unified header"
               data-track-label="link">
                <img src="/oscar-static/images/darwin/header/img/logo-springerlink-39ee2a28d8.svg" alt="SpringerLink">
            </a>
        </div>
        
            <a href="https://link.springer.com/signup-login?previousUrl&#x3D;https%3A%2F%2Flink.springer.com%2Farticle%2F10.1007%2Fs10032-021-00367-3" class="c-header__link c-header__link--static"
               data-track="click"
               data-track-action="click log in link"
               data-track-category="unified header"
               data-track-label="link">
                <svg class="c-header__icon" width="24" height="24" aria-hidden="true" focusable="false">
                    <use xlink:href="#icon-eds-user-single"></use>
                </svg><span>Log in</span>
            </a>
        
    </div>

    
        <nav class="c-header__nav" aria-label="header navigation">
            <div class="c-header__nav-container">
                <div class="c-header__item c-header__item--menu">
                    <a href="#header-nav" class="c-header__link" data-header-expander>
                        <svg class="c-header__icon" width="24" height="24" aria-hidden="true" focusable="false">
                            <use xlink:href="#icon-eds-menu"></use>
                        </svg><span>Menu</span>
                    </a>
                </div>

                <div class="c-header__item c-header__item--inline-links">
                    
                        <a class="c-header__link" href="https://link.springer.com/journals/a/1" data-track="click" data-track-action="click find a journal" data-track-label="link">Find a journal</a>
                    
                        <a class="c-header__link" href="https://www.springernature.com/gp/authors" data-track="click" data-track-action="click publish with us link" data-track-label="link">Publish with us</a>
                    
                </div>

                <div class="c-header__link-container">
                    <div class="c-header__item c-header__item--divider">
                        <a href="#popup-search" class="c-header__link"
                           data-header-expander>
                            <svg class="c-header__icon" width="24" height="24" aria-hidden="true" focusable="false">
                                <use xlink:href="#icon-eds-search"></use>
                            </svg><span>Search</span>
                        </a>
                    </div>
                    
        <div class="c-header__item">
            <div class="c-header__cart-icon">
                <div id="ecommerce-header-cart-icon-link" class="c-header__item ecommerce-cart" style="display:inline-block">
 <a class="c-header__link" href="https://order.springer.com/public/cart" style="appearance:none;border:none;background:none;color:inherit;position:relative">
  <svg id="eds-i-cart" style="vertical-align:bottom" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewbox="0 0 24 24" aria-hidden="true" focusable="false">
   <path fill="currentColor" fill-rule="nonzero" d="M2 1a1 1 0 0 0 0 2l1.659.001 2.257 12.808a2.599 2.599 0 0 0 2.435 2.185l.167.004 9.976-.001a2.613 2.613 0 0 0 2.61-1.748l.03-.106 1.755-7.82.032-.107a2.546 2.546 0 0 0-.311-1.986l-.108-.157a2.604 2.604 0 0 0-2.197-1.076L6.042 5l-.56-3.17a1 1 0 0 0-.864-.82l-.12-.007L2.001 1ZM20.35 6.996a.63.63 0 0 1 .54.26.55.55 0 0 1 .082.505l-.028.1L19.2 15.63l-.022.05c-.094.177-.282.299-.526.317l-10.145.002a.61.61 0 0 1-.618-.515L6.394 6.999l13.955-.003ZM18 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4ZM8 19a2 2 0 1 0 0 4 2 2 0 0 0 0-4Z"></path>
  </svg><span style="padding-left:10px">Cart</span><span class="cart-info" style="display:none;position:absolute;top:10px;right:45px;background-color:#C65301;color:#fff;width:18px;height:18px;font-size:11px;border-radius:50%;line-height:17.5px;text-align:center"></span></a>
 <script>(function () { var exports = {}; if (window.fetch) {
            
            "use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.headerWidgetClientInit = void 0;
var headerWidgetClientInit = function (getCartInfo) {
    console.log("listen to updatedCart event");
    document.body.addEventListener("updatedCart", function () {
        console.log("updatedCart happened");
        updateCartIcon().then(function () { return console.log("Cart state update upon event"); });
    }, false);
    return updateCartIcon().then(function () { return console.log("Initial cart state update"); });
    function updateCartIcon() {
        return getCartInfo()
            .then(function (res) { return res.json(); })
            .then(refreshCartState)
            .catch(function () { return console.log("Could not fetch cart info"); });
    }
    function refreshCartState(json) {
        var indicator = document.querySelector("#ecommerce-header-cart-icon-link .cart-info");
        /* istanbul ignore else */
        if (indicator && json.itemCount) {
            indicator.style.display = 'block';
            indicator.textContent = json.itemCount > 9 ? '9+' : json.itemCount.toString();
            var moreThanOneItem = json.itemCount > 1;
            indicator.setAttribute('title', "there ".concat(moreThanOneItem ? "are" : "is", " ").concat(json.itemCount, " item").concat(moreThanOneItem ? "s" : "", " in your cart"));
        }
        return json;
    }
};
exports.headerWidgetClientInit = headerWidgetClientInit;

            
            headerWidgetClientInit(
              function () {
                return window.fetch("https://cart.springer.com/cart-info", {
                  credentials: "include",
                  headers: { Accept: "application/json" }
                })
              }
            )
        }})()</script>
</div>
            </div>
        </div>
    
                </div>
            </div>
        </nav>
    
</header>

</div>
        

        
                
    
        <nav class="u-container" aria-label="breadcrumbs" data-test="article-breadcrumbs">
            <ol class="c-breadcrumbs c-breadcrumbs--truncated" itemscope itemtype="https://schema.org/BreadcrumbList">
                
                    <li class="c-breadcrumbs__item" id="breadcrumb0" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <a href="/" class="c-breadcrumbs__link" itemprop="item" data-track="click" data-track-category="article" data-track-action="breadcrumbs" data-track-label="breadcrumb1"><span itemprop="name">Home</span></a><meta itemprop="position" content="1">
                            <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10" xmlns="http://www.w3.org/2000/svg">
                                <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                            </svg>
                    </li>
                
                    <li class="c-breadcrumbs__item" id="breadcrumb1" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <a href="/journal/10032" class="c-breadcrumbs__link" itemprop="item" data-track="click" data-track-category="article" data-track-action="breadcrumbs" data-track-label="breadcrumb2"><span itemprop="name">International Journal on Document Analysis and Recognition (IJDAR)</span></a><meta itemprop="position" content="2">
                            <svg class="c-breadcrumbs__chevron" role="img" aria-hidden="true" focusable="false" height="10" viewBox="0 0 10 10" width="10" xmlns="http://www.w3.org/2000/svg">
                                <path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill="#666" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/>
                            </svg>
                    </li>
                
                    <li class="c-breadcrumbs__item" id="breadcrumb2" itemprop="itemListElement" itemscope="" itemtype="https://schema.org/ListItem">
                        <span itemprop="name">Article</span><meta itemprop="position" content="3">
                    </li>
                
            </ol>
        </nav>
    

        
        
        <div class="u-container u-mt-32 u-mb-32 u-clearfix" id="main-content" data-component="article-container">
            <main class="c-article-main-column u-float-left js-main-column" data-track-component="article body">
                
                
                    <div class="c-context-bar u-hide"
                         data-test="context-bar"
                         data-context-bar data-context-bar-with-recommendations
                         aria-hidden="true">
                        <div class="c-context-bar__container u-container">
                            <div class="c-context-bar__title">
                                Knowledge-driven description synthesis for floor plan interpretation
                            </div>
                            
                                <div data-test="inCoD">
                                    
    <div class="c-pdf-container">
        <div class="c-pdf-download u-clear-both u-mb-16">
            <a href="/content/pdf/10.1007/s10032-021-00367-3.pdf?pdf=button" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button" data-track-external download>
                
                    <span class="c-pdf-download__text">Download PDF</span>
                    <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
                
            </a>
        </div>
    </div>


                                </div>
                            
                        </div>
                    </div>
                

                
                    <div class="c-pdf-button__container u-hide-at-lg js-context-bar-sticky-point-mobile">
                        
    <div class="c-pdf-container">
        <div class="c-pdf-download u-clear-both u-mb-16">
            <a href="/content/pdf/10.1007/s10032-021-00367-3.pdf?pdf=button" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button" data-track-external download>
                
                    <span class="c-pdf-download__text">Download PDF</span>
                    <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
                
            </a>
        </div>
    </div>


                    </div>
                

                <div class="c-article-collection__container">
                    
    

                </div>


                <article lang="en">
                    <div class="c-article-header">
                        <header>
                            <ul class="c-article-identifiers" data-test="article-identifier">
                                
    
        <li class="c-article-identifiers__item" data-test="article-category">Original Paper</li>
    
    
    

                                <li class="c-article-identifiers__item">
                                    <a href="#article-info" data-track="click" data-track-action="publication date" data-track-label="link">Published: <time datetime="2021-04-26">26 April 2021</time></a>
                                </li>
                            </ul>

                            
                            <h1 class="c-article-title" data-test="article-title" data-article-title="">Knowledge-driven description synthesis for floor plan interpretation</h1>
                            <ul class="c-article-author-list c-article-author-list--short" data-test="authors-list" data-component-authors-activator="authors-list"><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Shreya-Goyal" data-author-popup="auth-Shreya-Goyal">Shreya Goyal</a><sup class="u-js-hide"><a href="#Aff1">1</a></sup>, </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Chiranjoy-Chattopadhyay" data-author-popup="auth-Chiranjoy-Chattopadhyay" data-corresp-id="c1">Chiranjoy Chattopadhyay<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-email"></use></svg></a><span class="u-js-hide"> 
            <a class="js-orcid" href="http://orcid.org/0000-0002-3431-0483"><span class="u-visually-hidden">ORCID: </span>orcid.org/0000-0002-3431-0483</a></span><sup class="u-js-hide"><a href="#Aff1">1</a></sup> &amp; </li><li class="c-article-author-list__item"><a data-test="author-name" data-track="click" data-track-action="open author" data-track-label="link" href="#auth-Gaurav-Bhatnagar" data-author-popup="auth-Gaurav-Bhatnagar">Gaurav Bhatnagar</a><sup class="u-js-hide"><a href="#Aff1">1</a></sup> </li></ul>
                            <p class="c-article-info-details" data-container-section="info">
                                
    <a data-test="journal-link" href="/journal/10032" data-track="click" data-track-action="journal homepage" data-track-category="article body" data-track-label="link"><i data-test="journal-title">International Journal on Document Analysis and Recognition (IJDAR)</i></a>

                                <b data-test="journal-volume"><span class="u-visually-hidden">volume</span> 24</b>, <span class="u-visually-hidden">pages </span>19–32 (<span data-test="article-publication-year">2021</span>)<a href="#citeas" class="c-article-info-details__cite-as u-hide-print" data-track="click" data-track-action="cite this article" data-track-label="link">Cite this article</a>
                            </p>
                            
    

                            <div data-test="article-metrics">
                                
    <div id="altmetric-container">
        <div class="c-article-metrics-bar__wrapper u-clear-both">
            <ul class="c-article-metrics-bar u-list-reset">
                
                    <li class=" c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">461 <span class="c-article-metrics-bar__label">Accesses</span></p>
                    </li>
                
                
                    <li class="c-article-metrics-bar__item">
                        <p class="c-article-metrics-bar__count">4 <span class="c-article-metrics-bar__label">Citations</span></p>
                    </li>
                
                
                    
                
                <li class="c-article-metrics-bar__item">
                    <p class="c-article-metrics-bar__details"><a href="/article/10.1007/s10032-021-00367-3/metrics" data-track="click" data-track-action="view metrics" data-track-label="link" rel="nofollow">Metrics <span class="u-visually-hidden">details</span></a></p>
                </li>
            </ul>
        </div>
    </div>




                            </div>
                            
    

    

                            
                        </header>
                    </div>

                    <div data-article-body="true" data-track-component="article body" class="c-article-body">
                        <section aria-labelledby="Abs1" data-title="Abstract" lang="en"><div class="c-article-section" id="Abs1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Abs1">Abstract</h2><div class="c-article-section__content" id="Abs1-content"><p>Image captioning is a widely known problem in the area of AI. Caption generation from floor plan images has applications in indoor path planning, real estate, and providing architectural solutions. Several methods have been explored in the literature for generating captions or semi-structured descriptions from floor plan images. Since only the caption is insufficient to capture fine-grained details, researchers also proposed descriptive paragraphs from images. However, these descriptions have a rigid structure and lack flexibility, making it difficult to use them in real-time scenarios. This paper offers two models, description synthesis from image cue (DSIC) and transformer-based description generation (TBDG), for text generation from floor plan images. These two models take advantage of modern deep neural networks for visual feature extraction and text generation. The difference between both models is in the way they take input from the floor plan image. The DSIC model takes only visual features automatically extracted by a deep neural network, while the TBDG model learns textual captions extracted from input floor plan images with paragraphs. The specific keywords generated in TBDG and understanding them with paragraphs make it more robust in a general floor plan image. Experiments were carried out on a large-scale publicly available dataset and compared with state-of-the-art techniques to show the proposed model’s superiority.</p></div></div></section>
                        
    


                        

                        <div data-test="cobranding-download">
                            
                        </div>

                        <div class="app-checklist-banner--on-mobile">
                            
                                
    <div class="app-checklist-banner" data-test="article-checklist-banner">
        <div class="app-checklist-banner__body">
            <h3 class="app-checklist-banner__title">Working on a manuscript?</h3>
            <a class="app-checklist-banner__link" data-track="click" data-track-category="pre-submission-checklist" data-track-action="clicked article page checklist banner test 2 old version" data-track-label="link" href="https://beta.springernature.com/pre-submission?journalId=10032"
            data-test="article-checklist-banner-link">Avoid the common mistakes
            <svg class="app-checklist-banner__arrow-icon" aria-hidden="true" focusable="false">
                <use xlink:href="#icon-springer-arrow-right"></use>
            </svg>
            </a>
        </div>
        <div class="app-checklist-banner__icon-container">
        <svg class="app-checklist-banner__paper-icon" aria-hidden="true" focusable="false">
            <use xlink:href="#icon-checklist-banner"></use>
        </svg>
        </div>
    </div>

                            
                        </div>

                        
                            
                                <div
                                    class="main-content">
                                    <section data-title="Introduction"><div class="c-article-section" id="Sec1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec1"><span class="c-article-section__title-number">1 </span>Introduction</h2><div class="c-article-section__content" id="Sec1-content"><p>A floor plan is a graphical document that aids architects in showing the interior of a building. Floor plan image analysis involves semantic segmentation, symbol spotting, and identifying a relationship between them. Describing a floor plan in natural language has applications in robotics, real-estate business, and automation. However, there are several challenges regarding narrating a graphical document in natural language. A graphical document is not similar to a natural photograph that has an essential feature in every pixel. Hence, traditional approaches using image features with textual description fails in this context. The graphical document requires specific information for their description to make it more meaningful. Hence, cues taken directly from an image are not very efficient in this context. There are several approaches available for language modeling and text generation in which the encoder–decoder framework is the most popular choice. In image to text generation, CNN–RNN (CNN acting as an encoder, RNN as a decoder) is widely used in the literature. The variant of RNN is varied in the decoder as LSTM, Bi-LSTM, and GRU.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig1">1</a> depicts the proposed problem with the desired output. The generated captions in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig1">1</a> (second row from the top) are very structured and contain limited information. The bottom row provides more realistic descriptions. We take advantage of both image cues and word signals to generate specific and meaningful descriptions. The proposed work is an extension of the paper [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Goyal, S., Mistry, V., Chattopadhyay, C., Bhatnagar, G.: BRIDGE: building plan repository for image description generation, and evaluation. In: ICDAR (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR16" id="ref-link-section-d9079034e375">16</a>], where a large-scale dataset and annotations were proposed. This paper’s proposed work leverages those annotations by offering multisentence paragraph generation solutions from floor plan images.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-1" data-title="Fig. 1"><figure><figcaption><b id="Fig1" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 1</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/1" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig1_HTML.png?as=webp"><img aria-describedby="Fig1" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig1_HTML.png" alt="figure 1" loading="lazy" width="685" height="609"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-1-desc"><p>An illustration of the proposed problem domain with the desired output</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/1" data-track-dest="link:Figure1 Full size image" aria-label="Full size image figure 1" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-2" data-title="Fig. 2"><figure><figcaption><b id="Fig2" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 2</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/2" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig2_HTML.png?as=webp"><img aria-describedby="Fig2" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig2_HTML.png" alt="figure 2" loading="lazy" width="685" height="264"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-2-desc"><p>An illustration of the proposed work of generating textual description from floor plan images</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/2" data-track-dest="link:Figure2 Full size image" aria-label="Full size image figure 2" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-chevron-right"></use></svg></a></div></figure></div><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig2">2</a> depicts the overall flow of our proposed method. We extend the idea of extracting information from floor plan images in a multistaged pipeline using deep learning methods. This direction’s previous work is extended by offering models that learn textual features with visual features in an end-to-end framework. We propose two models, description synthesis from image cue (DSIC) and transformer-based description generation (TBDG), where TBDG is more robust than DSIC. These two proposed models differ in the way the decoder receives the input. In DSIC, region-wise visual features are learned with textual features, and a paragraph description is generated. In contrast, TBDG learns region-wise captions with region wise features, and those text features are given as input to the decoder model to create a paragraph. We further propose a deep learning-based multistaged pipeline for description generation in order to prove the superiority of end-to-end learning models on multistaged pipelines.
</p><h3 class="c-article__sub-heading" id="Sec2"><span class="c-article-section__title-number">1.1 </span>Uniqueness of the proposed work</h3><p>In the previous work, [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Goyal, S., Bhavsar, S., Patel, S., Chattopadhyay, C., Bhatnagar, G.: SUGAMAN: describing floor plans for visually impaired by annotation learning and proximity-based grammar. Image Process. 13(13), 2623–2635 (2019)" href="#ref-CR13" id="ref-link-section-d9079034e425">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Goyal, S., Chattopadhyay, C., Bhatnagar, G.: ASYSST: a framework for synopsis synthesis empowering visually impaired. In: MAHCI (2018)" href="#ref-CR14" id="ref-link-section-d9079034e425_1">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Goyal, S., Chattopadhyay, C., Bhatnagar, G.: Plan2Text: a framework for describing building floor plan images from first person perspective. In: CSPA (2018)" href="/article/10.1007/s10032-021-00367-3#ref-CR15" id="ref-link-section-d9079034e428">15</a>], only visual elements are learned and classified in a multistaged manner using classical machine learning approaches. Tasks such as semantic segmentation, room classification, and decor classification are performed in a sequential pipeline using classical machine learning methods. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Goyal, S., Mistry, V., Chattopadhyay, C., Bhatnagar, G.: BRIDGE: building plan repository for image description generation, and evaluation. In: ICDAR (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR16" id="ref-link-section-d9079034e431">16</a>], the similar visual elements are learned and classified in part-by-part manner using a deep neural network. In contrast with the existing approaches, in this paper, the visual information from floor plan images and textual features are learned together in an end to end deep learning framework, and a holistic description for the same is generated.
</p><h3 class="c-article__sub-heading" id="Sec3"><span class="c-article-section__title-number">1.2 </span>Organization of the paper</h3><p>The rest of the paper is organized in the following way. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10032-021-00367-3#Sec4">2</a> highlights the related works. Sections <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10032-021-00367-3#Sec9">3</a> and <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10032-021-00367-3#Sec14">4</a> describes the two proposed models. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10032-021-00367-3#Sec20">5</a> describes the experimental setup and the evaluation metrics followed for performance analysis. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10032-021-00367-3#Sec23">6</a> discusses the results generated using proposed models. Section <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10032-021-00367-3#Sec27">7</a> describes the comparative analysis of the various stages involved in description generation models and their qualitative and quantitative comparison, while the paper is concluded in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10032-021-00367-3#Sec33">8</a>.</p></div></div></section><section data-title="Related work"><div class="c-article-section" id="Sec4-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec4"><span class="c-article-section__title-number">2 </span>Related work</h2><div class="c-article-section__content" id="Sec4-content"><h3 class="c-article__sub-heading" id="Sec5"><span class="c-article-section__title-number">2.1 </span>Publicly available floor plan datasets</h3><p>In the literature, the publicly available datasets are ROBIN [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Sharma, D., Gupta, N., Chattopadhyay, C., Mehta, S.: DANIEL: A deep architecture for automatic analysis and retrieval of building floor plans. In: ICDAR (2017)" href="/article/10.1007/s10032-021-00367-3#ref-CR45" id="ref-link-section-d9079034e478">45</a>], CVC-FP [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="de las Heras, L.P., Terrades, O.R., Robles, S., Sánchez, G.: CVC-FP and SGT: a new database for structural floor plan analysis and its groundtruthing tool. IJDAR 18(1), 15–30 (2015)" href="/article/10.1007/s10032-021-00367-3#ref-CR7" id="ref-link-section-d9079034e481">7</a>], SESYD [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Delalandre, M., Valveny, E., Pridmore, T., Karatzas, D.: Generation of synthetic documents for performance evaluation of symbol recognition &amp; spotting systems. IJDAR 13(3), 187–207 (2010)" href="/article/10.1007/s10032-021-00367-3#ref-CR8" id="ref-link-section-d9079034e484">8</a>], BRIDGE [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Goyal, S., Mistry, V., Chattopadhyay, C., Bhatnagar, G.: BRIDGE: building plan repository for image description generation, and evaluation. In: ICDAR (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR16" id="ref-link-section-d9079034e487">16</a>], and FPLAN-POLY [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 3" title="Barducci, A., Marinai, S.: Object recognition in floor plans by graphs of white connected components. In: ICPR (2012)" href="/article/10.1007/s10032-021-00367-3#ref-CR3" id="ref-link-section-d9079034e490">3</a>]. However, apart from the BRIDGE, the others contain very few sample images and do not contain annotations for objects and their textual descriptions. These datasets were proposed for segmentation, retrieval, and layout analysis, which are not suitable for caption generation and description synthesis tasks. With the advent of deep neural networks, tasks such as symbol spotting, caption generation, retrieval, semantic segmentation are getting more accurate and robust. To meet the requirement of a large number of samples and corresponding annotations, the BRIDGE [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Goyal, S., Mistry, V., Chattopadhyay, C., Bhatnagar, G.: BRIDGE: building plan repository for image description generation, and evaluation. In: ICDAR (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR16" id="ref-link-section-d9079034e494">16</a>] dataset was proposed, having 13000-floor plan images along with annotations. There is high variability in the way decor symbols have been represented across these datasets. To overcome this limitation, samples from [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="de las Heras, L.P., Terrades, O.R., Robles, S., Sánchez, G.: CVC-FP and SGT: a new database for structural floor plan analysis and its groundtruthing tool. IJDAR 18(1), 15–30 (2015)" href="/article/10.1007/s10032-021-00367-3#ref-CR7" id="ref-link-section-d9079034e497">7</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Delalandre, M., Valveny, E., Pridmore, T., Karatzas, D.: Generation of synthetic documents for performance evaluation of symbol recognition &amp; spotting systems. IJDAR 13(3), 187–207 (2010)" href="/article/10.1007/s10032-021-00367-3#ref-CR8" id="ref-link-section-d9079034e500">8</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Sharma, D., Gupta, N., Chattopadhyay, C., Mehta, S.: DANIEL: A deep architecture for automatic analysis and retrieval of building floor plans. In: ICDAR (2017)" href="/article/10.1007/s10032-021-00367-3#ref-CR45" id="ref-link-section-d9079034e503">45</a>] has been included in the BRIDGE dataset, and decor symbol annotations are done. Hence, a large portion of variable decor symbols has been covered in BRIDGE dataset.</p><p>There are many large-scale datasets publicly available in the literature in the context of natural images [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., et al.: Visual genome: connecting language and vision using crowdsourced dense image annotations. IJCV 123(1), 32–73 (2017)" href="/article/10.1007/s10032-021-00367-3#ref-CR23" id="ref-link-section-d9079034e509">23</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 26" title="Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L.: Microsoft COCO: common objects in context. In: ECCV (2014)" href="/article/10.1007/s10032-021-00367-3#ref-CR26" id="ref-link-section-d9079034e512">26</a>], which have many realistic images along with their descriptions or captions annotations, region graphs, and other metadata. For example, [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 23" title="Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., et al.: Visual genome: connecting language and vision using crowdsourced dense image annotations. IJCV 123(1), 32–73 (2017)" href="/article/10.1007/s10032-021-00367-3#ref-CR23" id="ref-link-section-d9079034e515">23</a>] connects 108,077 images with textual annotation, 5.4 Million region descriptions, and other annotations for various tasks such as caption generation, visual question answering. MS-COCO and MS-COCO captions [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 5" title="Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollár, P., Zitnick, C.L.: Microsoft COCO captions: data collection and evaluation server. arXiv preprint &#xA;                  arXiv:1504.00325&#xA;                  &#xA;                 (2015)" href="/article/10.1007/s10032-021-00367-3#ref-CR5" id="ref-link-section-d9079034e518">5</a>] are examples of datasets that contain over 330,000 images and over one and a half million captions (five captions per image).</p><h3 class="c-article__sub-heading" id="Sec6"><span class="c-article-section__title-number">2.2 </span>Object detection and classification</h3><p>Researchers have explored handcrafted features and conventional machine learning models [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 1" title="Adam, S., Ogier, J.M., Cariou, C., Mullot, R., Labiche, J., Gardes, J.: Symbol and character recognition: application to engineering drawings. IJDAR 3(2), 89–101 (2000)" href="/article/10.1007/s10032-021-00367-3#ref-CR1" id="ref-link-section-d9079034e529">1</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 9" title="Dutta, A., Llados, J., Pal, U.: Symbol spotting in line drawings through graph paths hashing. In: ICDAR (2011)" href="/article/10.1007/s10032-021-00367-3#ref-CR9" id="ref-link-section-d9079034e532">9</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 10" title="Dutta, A., Lladós, J., Pal, U.: A symbol spotting approach in graphical documents by hashing serialized graphs. Pattern Recognit. 46(3), 752–768 (2013)" href="/article/10.1007/s10032-021-00367-3#ref-CR10" id="ref-link-section-d9079034e535">10</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 35" title="Qureshi, R.J., Ramel, J.Y., Barret, D., Cardot, H.: Spotting symbols in line drawing images using graph representations. In: GREC (2007)" href="/article/10.1007/s10032-021-00367-3#ref-CR35" id="ref-link-section-d9079034e538">35</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 49" title="Viola, P., Jones, M.: Rapid object detection using a boosted cascade of simple features. In: CVPR (2001)" href="/article/10.1007/s10032-021-00367-3#ref-CR49" id="ref-link-section-d9079034e541">49</a>] for the symbol spotting task in document images. As the deep neural network models are getting popular, methods like YOLO [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 36" title="Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: unified, real-time object detection. In: CVPR (2016)" href="/article/10.1007/s10032-021-00367-3#ref-CR36" id="ref-link-section-d9079034e545">36</a>], Fast-RCNN [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Girshick, R.: Fast R-CNN. In: ICCV (2015)" href="/article/10.1007/s10032-021-00367-3#ref-CR12" id="ref-link-section-d9079034e548">12</a>], Faster-RCNN [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time object detection with region proposal networks. In: NIPS (2015)" href="/article/10.1007/s10032-021-00367-3#ref-CR39" id="ref-link-section-d9079034e551">39</a>] in the context of natural images were proposed. The YOLO family-based algorithms [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: unified, real-time object detection. In: CVPR (2016)" href="#ref-CR36" id="ref-link-section-d9079034e554">36</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Redmon, J., Farhadi, A.: YOLO9000: better, faster, stronger. In: CVPR (2017)" href="#ref-CR37" id="ref-link-section-d9079034e554_1">37</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 38" title="Redmon, J., Farhadi, A.: YOLOv3: an incremental improvement. arXiv preprint &#xA;                  arXiv:1804.02767&#xA;                  &#xA;                 (2018)" href="/article/10.1007/s10032-021-00367-3#ref-CR38" id="ref-link-section-d9079034e557">38</a>] are region classification-based methods, which is a single neural network trained end-to-end and predicts bounding boxes and class labels directly. However, all the R-CNN family-based models [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 12" title="Girshick, R.: Fast R-CNN. In: ICCV (2015)" href="/article/10.1007/s10032-021-00367-3#ref-CR12" id="ref-link-section-d9079034e560">12</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 17" title="He, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask R-CNN. In: ICCV (2017)" href="/article/10.1007/s10032-021-00367-3#ref-CR17" id="ref-link-section-d9079034e564">17</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 18" title="He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional networks for visual recognition. T-PAMI 37(9), 1904–1916 (2015)" href="/article/10.1007/s10032-021-00367-3#ref-CR18" id="ref-link-section-d9079034e567">18</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 39" title="Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time object detection with region proposal networks. In: NIPS (2015)" href="/article/10.1007/s10032-021-00367-3#ref-CR39" id="ref-link-section-d9079034e570">39</a>], are region proposal-based methods, which extract several regions from the input image and extract features from those regions using CNN and classify them using a classifier. In one of the work [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 54" title="Ziran, Z., Marinai, S.: Object detection in floor plan images. In: IAPR Workshop on Artificial Neural Networks in Pattern Recognition (2018)" href="/article/10.1007/s10032-021-00367-3#ref-CR54" id="ref-link-section-d9079034e573">54</a>], symbol spotting in floor plans using YOLO and Fast-RCNN has also been explored. In the same line, [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Rezvanifar, A., Cote, M., Branzan Albu, A.: Symbol spotting on digital architectural floor plans using a deep learning-based framework. In: Proceedings of the IEEE/CVF CVPR Workshops, pp. 568–569 (2020)" href="/article/10.1007/s10032-021-00367-3#ref-CR40" id="ref-link-section-d9079034e576">40</a>] has performed symbol spotting in floor plans using the YOLO network. Apart from floor plans, symbol spotting or object detection in document images has been explored by several researchers. Examples include detecting tables, equations, figures [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 43" title="Saha, R., Mondal, A., Jawahar, C.: Graphical Object Detection in Document Images. In: ICDAR (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR43" id="ref-link-section-d9079034e579">43</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 44" title="Schreiber, S., Agne, S., Wolf, I., Dengel, A., Ahmed, S.: Deepdesrt: deep learning for detection and structure recognition of tables in document images. In: ICDAR (2017)" href="/article/10.1007/s10032-021-00367-3#ref-CR44" id="ref-link-section-d9079034e583">44</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 53" title="Yi, X., Gao, L., Liao, Y., Zhang, X., Liu, R., Jiang, Z.: CNN based page object detection in document images. In: ICDAR (2017)" href="/article/10.1007/s10032-021-00367-3#ref-CR53" id="ref-link-section-d9079034e586">53</a>], signatures, and logos [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 46" title="Sharma, N., Mandal, R., Sharma, R., Pal, U., Blumenstein, M.: Signature and Logo Detection using Deep CNN for Document Image Retrieval. In: ICFHR (2018)" href="/article/10.1007/s10032-021-00367-3#ref-CR46" id="ref-link-section-d9079034e589">46</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 47" title="Su, H., Gong, S., Zhu, X.: Scalable logo detection by self co-learning. Pattern Recognition 97, 107003 (2020)" href="/article/10.1007/s10032-021-00367-3#ref-CR47" id="ref-link-section-d9079034e592">47</a>]. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 21" title="Khan, I., Islam, N., Rehman, H.U., Khan, M.: A comparative study of graphic symbol recognition methods. Multimedia Tools Appl. 79(13), 8695–8725 (2020)" href="/article/10.1007/s10032-021-00367-3#ref-CR21" id="ref-link-section-d9079034e595">21</a>], a comparative study of the recognition of symbol spotting on graphical documents was presented.</p><h3 class="c-article__sub-heading" id="Sec7"><span class="c-article-section__title-number">2.3 </span>Image description generation</h3><p>Image description generation is a challenging task in AI. Template-based retrieval, n-grams, grammar rules, RNN, LSTM, GRU, are some example approaches to solve the problem. These methods work with image modality features by extracting information related to image using conventional architectures. Some of the initial works in this direction, [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 11" title="Farhadi, A., Hejrati, M., Sadeghi, M.A., Young, P., Rashtchian, C., Hockenmaier, J., Forsyth, D.: Every picture tells a story: generating sentences from images. In: ECCV (2010)" href="/article/10.1007/s10032-021-00367-3#ref-CR11" id="ref-link-section-d9079034e606">11</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 24" title="Kulkarni, G., Premraj, V., Ordonez, V., Dhar, S., Li, S., Choi, Y., Berg, A.C., Berg, T.L.: Babytalk: understanding and generating simple image descriptions. T-PAMI 35(12), 2891–2903 (2013)" href="/article/10.1007/s10032-021-00367-3#ref-CR24" id="ref-link-section-d9079034e609">24</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 25" title="Li, S., Kulkarni, G., Berg, T.L., Berg, A.C., Choi, Y.: Composing simple image descriptions using web-scale n-grams. In: CoNLL (2011)" href="/article/10.1007/s10032-021-00367-3#ref-CR25" id="ref-link-section-d9079034e612">25</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 33" title="Ordonez, V., Kulkarni, G., Berg, T.L.: Im2Text: describing images using 1 million captioned photographs. In: NIPS (2011)" href="/article/10.1007/s10032-021-00367-3#ref-CR33" id="ref-link-section-d9079034e615">33</a>] have used computer vision methods for extracting attributes from an image and generated sentences using retrieval and n-gram model.</p><p>Johnson et al. [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Johnson, J., Karpathy, A., Fei-Fei, L.: Densecap: fully convolutional localization networks for dense captioning. In: CVPR (2016)" href="/article/10.1007/s10032-021-00367-3#ref-CR20" id="ref-link-section-d9079034e621">20</a>] proposed an algorithm to generate region-wise captions. The Hierarchical recurrent network [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 22" title="Krause, J., Johnson, J., Krishna, R., Fei-Fei, L.: A hierarchical approach for generating descriptive image paragraphs. In: CVPR (2017)" href="/article/10.1007/s10032-021-00367-3#ref-CR22" id="ref-link-section-d9079034e624">22</a>] uses two RNNs to generate paragraphs from an image. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 50" title="Wang, Q., Chan, A.B.: CNN+CNN: convolutional decoders for image captioning. arXiv preprint &#xA;                  arXiv:1805.09019&#xA;                  &#xA;                 (2018)" href="/article/10.1007/s10032-021-00367-3#ref-CR50" id="ref-link-section-d9079034e627">50</a>], two CNN networks are used, where one of them is used as an encoder for image features, and the other is used as a decoder for language generation. Similarly, [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 4" title="Chatterjee, M., Schwing, A.G.: Diverse and coherent paragraph generation from images. In: ECCV (2018)" href="/article/10.1007/s10032-021-00367-3#ref-CR4" id="ref-link-section-d9079034e630">4</a>] has used a sentence topic generator network from visual features of images, and RNN is used for sentence generation. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 51" title="Wang, Z., Luo, Y., Li, Y., Huang, Z., Yin, H.: Look Deeper See Richer: Depth-aware Image Paragraph Captioning. In: ACM MM (2018)" href="/article/10.1007/s10032-021-00367-3#ref-CR51" id="ref-link-section-d9079034e633">51</a>], the depth aware attention model is used for generating a detailed paragraph from an image. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 30" title="Mao, Y., Zhou, C., Wang, X., Li, R.: Show and tell more: topic-oriented multi-sentence image captioning. In: IJCAI (2018)" href="/article/10.1007/s10032-021-00367-3#ref-CR30" id="ref-link-section-d9079034e637">30</a>], latent Dirichlet allocation (LDA) is used to mine topics of interest from textual descriptions and developed multiple topic-oriented sentences for image description. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 52" title="Yao, T., Pan, Y., Li, Y., Qiu, Z., Mei, T.: Boosting image captioning with attributes. In: ICCV (2017)" href="/article/10.1007/s10032-021-00367-3#ref-CR52" id="ref-link-section-d9079034e640">52</a>], CNN and the RNN framework were used to capture visual features and generate descriptions. Description is also generated from a stream of images in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 34" title="Park, C.C., Kim, G.: Expressing an image stream with a sequence of natural sentences. In: NIPS (2015)" href="/article/10.1007/s10032-021-00367-3#ref-CR34" id="ref-link-section-d9079034e643">34</a>] by using CNN and RNN for a visual feature and sequence encoding and retrieving sentences from an existing database. A similar line [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 27" title="Liu, Y., Fu, J., Mei, T., Chen, C.W.: Let your photos talk: Generating narrative paragraph for photo stream via bidirectional attention recurrent neural networks. In: AAAI (2017)" href="/article/10.1007/s10032-021-00367-3#ref-CR27" id="ref-link-section-d9079034e646">27</a>] description has been generated as storytelling from images by using the bi-directional attention-based RNN model.</p><h3 class="c-article__sub-heading" id="Sec8"><span class="c-article-section__title-number">2.4 </span>Language modeling</h3><p>Nowadays, since deep neural networks are very successful in natural language processing, learning text for generating descriptions using sequence-to-sequence models is a natural choice. In the work, [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 48" title="Sutskever, I., Vinyals, O., Le, Q.: Sequence to sequence learning with neural networks. NIPS (2014)" href="/article/10.1007/s10032-021-00367-3#ref-CR48" id="ref-link-section-d9079034e657">48</a>] has proposed seq2seq learning model for learning and modeling language by LSTM model. Moreover, [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 2" title="Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning to align and translate. arXiv preprint &#xA;                  arXiv:1409.0473&#xA;                  &#xA;                 (2014)" href="/article/10.1007/s10032-021-00367-3#ref-CR2" id="ref-link-section-d9079034e660">2</a>] and [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 28" title="Luong, M.T., Pham, H., Manning, C.D.: Effective approaches to attention-based neural machine translation. arXiv preprint &#xA;                  arXiv:1508.04025&#xA;                  &#xA;                 (2015)" href="/article/10.1007/s10032-021-00367-3#ref-CR28" id="ref-link-section-d9079034e663">28</a>] are the initial models which model the language by aligning the input sequence with a target sequence using attention-based models. The neural machine translation model has also been used in text summarization tasks such as [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 41" title="Rush, A.M., Chopra, S., Weston, J.: A neural attention model for abstractive sentence summarization. arXiv preprint &#xA;                  arXiv:1509.00685&#xA;                  &#xA;                 (2015)" href="/article/10.1007/s10032-021-00367-3#ref-CR41" id="ref-link-section-d9079034e666">41</a>] and [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 32" title="Nallapati, R., Zhou, B., Gulcehre, C., Xiang, B., et al.: Abstractive text summarization using sequence-to-sequence RNNs and beyond. arXiv preprint &#xA;                  arXiv:1602.06023&#xA;                  &#xA;                 (2016)" href="/article/10.1007/s10032-021-00367-3#ref-CR32" id="ref-link-section-d9079034e669">32</a>]. In the next section, the proposed models for description generation from floor plan images are described in detail.</p></div></div></section><section data-title="Description synthesis from image cue (DSIC)"><div class="c-article-section" id="Sec9-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec9"><span class="c-article-section__title-number">3 </span>Description synthesis from image cue (DSIC)</h2><div class="c-article-section__content" id="Sec9-content"><p>We have described the floor plan images in the proposed model by extracting region-wise visual features from images and learning paragraphs by providing them to a decoder network. The region proposal network (RPN) acts as the encoder, and a hierarchical RNN structure acts as the decoder. The system is trained in an end to end manner. We describe each step in detail next.</p><h3 class="c-article__sub-heading" id="Sec10"><span class="c-article-section__title-number">3.1 </span>Visual feature extraction</h3><p>We adopt a hierarchical RNN-based approach as a decoder framework. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig3">3</a> depicts a typical architecture of the proposed model. The dataset contains the image (<i>I</i>) and its corresponding paragraph description (<i>K</i>) in the proposed approach. The CNN is used along with a RPN to generate region proposals, <span class="mathjax-tex">\(R_1, R_2,\ldots ,R_n\)</span>. We extracted the top 5 region proposals for this approach and pooled them in a single pooling vector <i>P</i> using a projection matrix. In DSIC, two RNNs are used in a hierarchy, where one is used for learning sentence topic vectors from pooled features, and the other is used for learning words for the respective sentence topic vectors. In DSIC, the top 5 regions are extracted because there are average 5 sentences per paragraph in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Goyal, S., Mistry, V., Chattopadhyay, C., Bhatnagar, G.: BRIDGE: building plan repository for image description generation, and evaluation. In: ICDAR (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR16" id="ref-link-section-d9079034e749">16</a>].</p><h3 class="c-article__sub-heading" id="Sec11"><span class="c-article-section__title-number">3.2 </span>Region pooling</h3><p>All the extracted regions <span class="mathjax-tex">\(R_i\)</span> are pooled in a vector <i>P</i> by taking the projection of each region vector <span class="mathjax-tex">\(R_i\)</span> with a projection matrix <i>M</i> and taking an element wise maximum. Dimension of the pooled vector is same as the region vectors and defined as <span class="mathjax-tex">\(P= \max _{i=1}^n{(MR_i+bias)}.\)</span></p><p>The projection matrix is trained end-to-end on the sentence RNN and the word RNN. The pooled vector <i>P</i>, compactly represents all the regions <span class="mathjax-tex">\(R_i\)</span>s.</p><h3 class="c-article__sub-heading" id="Sec12"><span class="c-article-section__title-number">3.3 </span>Hierarchical RNN structure</h3><p>This network, as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig3">3</a>, contains two units of the RNN network. One is sentence level (S-RNN) and the other is word level (W-RNN). The S-RNN is single-layered, used for generating a sentence topic vector for each sentence, and decides the number of sentences to be generated. W-RNN is a two-layered and takes the sentence topic vectors as input and generates words in each sentence. Instead of using single RNN as a decoder, which would have to regress over a long sequence of words and make training the language model harder, two RNN networks are taken in a hierarchy. The choice of networks for both RNNs is kept as LSTM networks since they can learn long-term dependencies than a vanilla RNN. The S-RNN is followed by two-layered fully connected network, which generates a topic vector to be given as input to W-RNN after processing the hidden states from RNN. The W-RNN takes topic vector and word-level embeddings for the respective sentence as input. A probability distribution is generated for each word in the vocabulary, where is the threshold, <i>Th</i> is taken as 0.5, which generates further words for each sentence.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-3" data-title="Fig. 3"><figure><figcaption><b id="Fig3" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 3</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/3" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig3_HTML.png?as=webp"><img aria-describedby="Fig3" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig3_HTML.png" alt="figure 3" loading="lazy" width="685" height="194"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-3-desc"><p>Hierarchical RNN to yield paragraph from floor plans</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/3" data-track-dest="link:Figure3 Full size image" aria-label="Full size image figure 3" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec13"><span class="c-article-section__title-number">3.4 </span>Training</h3><p>At this stage, the pooled vectors <span class="mathjax-tex">\(P_i\)</span> generated from region proposals are taken as input to the sentence-level RNN for each image <i>I</i> and respective paragraph <i>K</i>. Each input maximum of 5 sentences and 60 words are generated (empirically identified based upon validation performance). Hence, at each stage, <span class="mathjax-tex">\(Sent_{max}=5\)</span> copies of word RNN and topic vector is generated by the sentence RNN for each word RNN for, <span class="mathjax-tex">\(Word_{max}=60\)</span> timestamps.</p><div id="Equ1" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} loss(I,K)= &amp; {} \beta _{sent}*\sum _{i=1}^{Sent_{max}}loss_{sent}(Prob_i, K_i)\nonumber \\&amp;+\beta _{word}*\sum _{i=1}^{Sent_{max}} \sum _{j=1}^{Word_{max}}loss_{word}(Prob_{ij}, K_{ij})\nonumber \\ \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (1)
                </div></div><p>Equation <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10032-021-00367-3#Equ1">1</a> is the loss function which is the weighted sum of cross-entropy losses, <span class="mathjax-tex">\(loss_{sent}\)</span> and <span class="mathjax-tex">\(loss_{word}\)</span>, where <span class="mathjax-tex">\(loss_{sent}\)</span> is the loss over probability over a sentence topic is generation <span class="mathjax-tex">\((Prob_{i})\)</span> and <span class="mathjax-tex">\(loss_{word}\)</span> is the loss over probability over words generation <span class="mathjax-tex">\((Prob_{ij})\)</span>, with each respective sentence topic where <i>K</i> is the paragraph description for each image <i>I</i>. The training parameters for DSIC model are such that: Sentence LSTM has 512 units, word LSTM has 512 units, Fully connected layer is size 1024. Next, we describe an alternative to DSIC model, TBDG model where the decoder unit takes text cues instead of image features/cues as input.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-4" data-title="Fig. 4"><figure><figcaption><b id="Fig4" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 4</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/4" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig4_HTML.png?as=webp"><img aria-describedby="Fig4" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig4_HTML.png" alt="figure 4" loading="lazy" width="685" height="232"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-4-desc"><p>Framework of the proposed TBDG model of generating paragraph description from input floor plan image</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/4" data-track-dest="link:Figure4 Full size image" aria-label="Full size image figure 4" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section data-title="Transformer-based description generation (TBDG)"><div class="c-article-section" id="Sec14-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec14"><span class="c-article-section__title-number">4 </span>Transformer-based description generation (TBDG)</h2><div class="c-article-section__content" id="Sec14-content"><p>The TBDG is a transformer-based model for generating descriptions from floor plan images. It takes input as text features by its decoder unit and generates a paragraph-based description. In TBDG, RPN learns region-wise captions available in BRIDGE dataset, instead of multisentenced paragraphs, which makes it different from DSIC model. In addition, a Bi-LSTM unit acts as an encoder to the LSTM unit acting as a decoder.</p><h3 class="c-article__sub-heading" id="Sec15"><span class="c-article-section__title-number">4.1 </span>Paragraph generation with extra knowledge</h3><p>Descriptions generated directly from image cues in DSIC lack the floor plan-specific information. There are chances to miss out on salient features in the graphical document. Additional knowledge is required to generate more flexible and exciting descriptions and accurate data specific to the input image. Hence, the data available is the tuple of (<i>I</i>,<span class="mathjax-tex">\(W_e\)</span>,<i>K</i>), where <i>I</i> is the input floor plan image, <span class="mathjax-tex">\(W_e\)</span> is the word cues extracted from the image, and <i>K</i> is the paragraph description about each floor plan. In language modeling and text generation networks, Seq2Seq models are widely used. However, with the advent of attention based seq2seq networks, popularly known as transformers, the performance of the text generation models has been increased to a great extent. In TBDG, the corpus <i>K</i> is preprocessed for training by removing extra lines, white spaces, unknown symbols and punctuation, and tokenized using PTB tokenizer [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 31" title="Marcus, M., Santorini, B., Marcinkiewicz, M.A.: Building a large annotated corpus of English: The Penn Treebank (1993)" href="/article/10.1007/s10032-021-00367-3#ref-CR31" id="ref-link-section-d9079034e1706">31</a>]. The words which are most frequently occurring are selected, and a vocabulary is generated for the words.</p><h3 class="c-article__sub-heading" id="Sec16"><span class="c-article-section__title-number">4.2 </span>Region-wise captions</h3><p>Floor plan images are distinctly different from natural images, and conventional deep models are inefficient to create features depicting a unique floor plan. Hence, learning region-wise visual features is advantageous in this context. We have extracted the region using the region proposal model described in DSIC. The annotations for regions in floor plans, available in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Goyal, S., Mistry, V., Chattopadhyay, C., Bhatnagar, G.: BRIDGE: building plan repository for image description generation, and evaluation. In: ICDAR (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR16" id="ref-link-section-d9079034e1717">16</a>], are used along with these region proposals to train an LSTM model. The model generates region-wise descriptions/captions, <span class="mathjax-tex">\(C_1, C_2,\ldots ,C_n\)</span> as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig4">4</a>. The generated captions are taken as input to the encoder–decoder unit, which is the next stage of the pipeline, where these captions serve as extra knowledge to the decoder network.</p><h3 class="c-article__sub-heading" id="Sec17"><span class="c-article-section__title-number">4.3 </span>Caption fusion and word embedding generation</h3><p>At this stage, we have <i>n</i> captions generated for each floor plan image. We select the top 5 captions with the highest probability and fuse them as a paragraph. <span class="mathjax-tex">\(C_1 \circ C_2 \circ C_3 \circ C_4 \circ C_5 = W_i\)</span>, where <span class="mathjax-tex">\(W_i\)</span> is the fused one-dimensional vector of the extracted captions and <i>i</i> is the number of training samples. <span class="mathjax-tex">\(W_i\)</span> is the concatenation of word embeddings created by word2vec and <span class="mathjax-tex">\(|W_e|= \min {(|W_1|, |W_2|, |W_3|,\ldots ,|W_i|)}\)</span>. Word2vec generates the embeddings for words, which is a representation of each word as a vector. The dimension of the concatenated vector was taken as the minimum of the vectors to avoid the vanishing gradient problem during the back propagation of the network.</p><h3 class="c-article__sub-heading" id="Sec18"><span class="c-article-section__title-number">4.4 </span>Paragraph encoding</h3><p>In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Goyal, S., Mistry, V., Chattopadhyay, C., Bhatnagar, G.: BRIDGE: building plan repository for image description generation, and evaluation. In: ICDAR (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR16" id="ref-link-section-d9079034e2026">16</a>], for each floor plan, a detailed paragraph description is available. However, some of the paragraphs are too long for encoding and contain additional information. Training the model with too long sequences leads to the vanishing gradient problem. Considering the dataset’s size, manually selecting useful information from each set of sentences is impossible. Hence, we heuristically selected a few keywords from the corpus. Examples of such keywords are common categories of regions like bedroom, bathroom, kitchen, porch, garage, and other keywords describing objects, like stairs, bathtubs, kitchen bars. From the available paragraphs, we extracted only those sentences which consist of these keywords to shorten the length of each paragraph. Each target sequence <span class="mathjax-tex">\((T_i)\)</span> is a one-dimensional vector and concatenation of the word embeddings generated by word2vec, and, <span class="mathjax-tex">\(|T_e|=\min {(|T_1|, |T_2|, |T_3|,\ldots ,|T_i|)} \)</span> as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig4">4</a>.</p><h3 class="c-article__sub-heading" id="Sec19"><span class="c-article-section__title-number">4.5 </span>Encoder–decoder architecture</h3><p>In TBDG model, we have proposed a transformer architecture that can handle dependencies between input and output sequence tokens by giving the decoder the entire input sequence. It focuses on a certain part of the input sequence when it predicts the output sequence. As shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig4">4</a>, the encoded captions <span class="mathjax-tex">\(W_e\)</span> are given as input to the Bi-LSTM unit which acts as an encoder. The Bi-LSTM unit generates hidden states <span class="mathjax-tex">\((h_1, h_2, h_3,\ldots ,h_n)\)</span> and given to an attention mechanism which first generates alignment scores <span class="mathjax-tex">\(e_{ij}\)</span> between the current target hidden state <span class="mathjax-tex">\(h_t\)</span> and source hidden state <span class="mathjax-tex">\(h_s\)</span>. The alignment scores are further given to SoftMax layer, which generates normalized output probabilities for each word as <span class="mathjax-tex">\(\alpha _{ij}\)</span>, (See Eq. <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10032-021-00367-3#Equ2">2</a>). Here, <span class="mathjax-tex">\(e_{ij}\)</span> are the outputs generated by the alignment model, where <i>i</i> is the number of time step. Attention weight <span class="mathjax-tex">\(\alpha _{ij}\)</span> is the normalized attention score at each time stamp <i>i</i> for <i>j</i>th hidden state, where <i>n</i> is the number of encoded words in the sentence or hidden states. Further context vector <span class="mathjax-tex">\(cv_i\)</span> is generated at every time step <i>i</i>, which is a weighted sum of encoded feature vectors. Context vector is defined as <span class="mathjax-tex">\(cv_{i}= \sum _{j=1}^n(\alpha _{ij}h_i)\)</span>. Attention scores learn how relevant is the input vector to the output vector. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig4">4</a>, the word embeddings, <span class="mathjax-tex">\(W_e\)</span> and <span class="mathjax-tex">\(T_e\)</span>, are given as input and target output vectors to the encoder unit. Equation <a data-track="click" data-track-label="link" data-track-action="equation anchor" href="/article/10.1007/s10032-021-00367-3#Equ2">2</a> describes the calculation of attention scores in the proposed model.</p><div id="Equ2" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} e_{ij}= align(h_t, h_s)\nonumber \\ \alpha _{ij}= \frac{exp(e_{ij})}{\sum {exp(e_{in})}} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (2)
                </div></div><p>Hence, this way, the decoder learns correspondence between input and output sequences in a global context and generates output sentences <span class="mathjax-tex">\(S_e\)</span>. Here, the decoder is a LSTM network with 256 units, connected to a time-distributed dense layer with SoftMax activation function. Time-distributed dense layer applies a fully connected (dense) operation on every time step. The network parameters used for training TBDS model are such that: Optimizer used is Adam, loss function is categorical cross-entropy, input sequence length is 80, output sequence length is kept 80, and embedding dimensions is 150 (empirically determined).</p></div></div></section><section data-title="Experimental setup"><div class="c-article-section" id="Sec20-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec20"><span class="c-article-section__title-number">5 </span>Experimental setup</h2><div class="c-article-section__content" id="Sec20-content"><p>Here, we discuss the details of how the experiments were conducted. All the experiments were performed on the dataset BRIDGE on a system with NVIDIA GPU Quadro <i>P</i>6000, with 24 GB GPU memory, 256 GB RAM. All implementation has been done in Keras with Python.</p><h3 class="c-article__sub-heading" id="Sec21"><span class="c-article-section__title-number">5.1 </span>Dataset</h3><p>In this paper, we have conducted our experiments on BRIDGE [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Goyal, S., Mistry, V., Chattopadhyay, C., Bhatnagar, G.: BRIDGE: building plan repository for image description generation, and evaluation. In: ICDAR (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR16" id="ref-link-section-d9079034e2801">16</a>]. This dataset has a large number of floor plan samples and their corresponding metadata. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig5">5</a> shows the components of [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Goyal, S., Mistry, V., Chattopadhyay, C., Bhatnagar, G.: BRIDGE: building plan repository for image description generation, and evaluation. In: ICDAR (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR16" id="ref-link-section-d9079034e2807">16</a>] which has (a) floor plan image, (b) decor symbol annotation in an XML format, (c) region-wise caption annotations in JSON format, (d) paragraph-based descriptions. Each paragraph’s average length in word count is 116, with the average length of each sentence being 5. The count of diversity is 121, a measure of the richness of words used in sentences. There are 134942 nouns, 5027 verbs, 46379 adjectives, and 5476 proper nouns available in the dataset.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-5" data-title="Fig. 5"><figure><figcaption><b id="Fig5" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 5</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/5" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig5_HTML.png?as=webp"><img aria-describedby="Fig5" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig5_HTML.png" alt="figure 5" loading="lazy" width="685" height="332"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-5-desc"><p>An illustration of a floor plan image and its corresponding annotations in the BRIDGE dataset [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Goyal, S., Mistry, V., Chattopadhyay, C., Bhatnagar, G.: BRIDGE: building plan repository for image description generation, and evaluation. In: ICDAR (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR16" id="ref-link-section-d9079034e2820">16</a>]</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/5" data-track-dest="link:Figure5 Full size image" aria-label="Full size image figure 5" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec22"><span class="c-article-section__title-number">5.2 </span>Quantitative evaluation metrics</h3><p>We have quantitatively evaluated the symbol spotting accuracy and text synthesis quality. The performance metrics are defined next. </p><ol class="u-list-style-none">
                  <li>
                    <span class="u-custom-list-number">1.</span>
                    
                      <p>ROUGE: It is a set of metrics designed to evaluate the text summaries with a collection of reference summaries. We have compared the generated descriptions with available human-written descriptions using n-gram ROUGE based on the formula </p><div id="Equ3" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} \frac{\sum _{S\in \{RS\}}{\sum _{gram-n\in S}{Count_{m}(gram-n)}}}{\sum _{S\in \{RS\}}{\sum _{gram-n\in S}{Count(gram-n)}}} \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (3)
                </div></div><p> where <i>RS</i> stands for reference summaries, <i>n</i> stands for length of the n-gram, <span class="mathjax-tex">\(gram-n\)</span>, and <span class="mathjax-tex">\(Count_{m}(gram-n)\)</span> is the maximum number of n-grams co-occurring in the candidate summary and the set of reference summaries.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">2.</span>
                    
                      <p>BLEU: It analyzes the co-occurrences of n-grams between a machine translation and a human-written sentence. The more the matches, the better is the candidate translation is. The score ranges from 0 to 1, where 0 is the worst score, and 1 is the perfect match. The n-gram modified precision score <span class="mathjax-tex">\((p_n)\)</span> is computed as: </p><div id="Equ9" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} p_n=\frac{\sum _{C\in \{Cand\}}{\sum _{gram-n\in C}{Count_{clip}(gram-n)}}}{\sum _{C'\in \{Cand\}}{\sum _{gram-n'\in C'}{Count(gram-n')}}} \end{aligned}$$</span></div></div><p><span class="mathjax-tex">\(Count_{clip}\)</span> limits the number of times an n-gram to be considered in a candidate (<i>Cand</i>) string. Then, they computer the geometric mean of the modified precision <span class="mathjax-tex">\((p_n)\)</span> using n-gram up to length <i>N</i> and weight <span class="mathjax-tex">\(W_n\)</span>, which sums up to 1. A brevity penalty (BP) is used for longer candidate summaries and for spurious words in it, which is defined by the following equation: </p><div id="Equ4" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} BP= {\left\{ \begin{array}{ll} 1, &amp;{} \text {if}\ c&gt;r \\ e^{\frac{1-r}{c}}, &amp;{} c\le r \end{array}\right. } \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (4)
                </div></div><p><i>c</i> is the length of the candidate summary, and <i>r</i> is the length of the reference summary. Then, BLEU score for corpus level given equal weights to all n-grams is evaluated by the following equation: </p><div id="Equ5" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} BLEU=BP.exp^{\sum _{i=1}^{N}{W_n}log(p_n) } \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (5)
                </div></div><p> Here, <span class="mathjax-tex">\(W_n\)</span> is the equally distributed weight in n-grams. For example, in case of BLEU-4, the weights used are <span class="mathjax-tex">\(\{(0.25),(0.25),(0.25),(0.25)\}\)</span>.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">3.</span>
                    
                      <p>METEOR: It is a metric used for evaluating machine-generated summaries against human-written summaries by checking the goodness of the order of words in both. METEOR score combines precision, recall, and fragmentation (alignment) in the sentences. It is a harmonic mean of the uni-gram precision and uni-gram recall given alignment and calculated as: </p><div id="Equ6" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} PN= &amp; {} \frac{1}{2}\left( \frac{no \;of\; chunks}{matched\; uni-grams}\right) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (6)
                </div></div><div id="Equ7" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} METEOR= &amp; {} \frac{10PR}{R+9P}(1-PN) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (7)
                </div></div><p><i>PN</i> is the penalty imposed based on a larger number of chunks, <i>P</i> are the uni-gram precision, <i>R</i> is the uni-gram recall. METEOR is the final score obtained by multiplying the harmonic mean of unigram precision and uni-gram recall with the penalty imposed.</p>
                    
                  </li>
                  <li>
                    <span class="u-custom-list-number">4.</span>
                    
                      <p>Average Precision (AP): The metric average precision used for evaluating the performance of decor symbol detection method is defined by the following equation: </p><div id="Equ8" class="c-article-equation"><div class="c-article-equation__content"><span class="mathjax-tex">$$\begin{aligned} AP= \frac{1}{N_s}* \sum P_r(rec) \end{aligned}$$</span></div><div class="c-article-equation__number">
                    (8)
                </div></div><p> where <span class="mathjax-tex">\(N_s\)</span> is the total detection for each class of symbol, <span class="mathjax-tex">\(P_r\)</span> is the precision value as a function of recall(<i>rec</i>). Mean average precision (mAP) is the average of <i>AP</i> calculated over all the classes.</p>
                    
                  </li>
                </ol><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-6" data-title="Fig. 6"><figure><figcaption><b id="Fig6" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 6</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/6" rel="nofollow"><picture><img aria-describedby="Fig6" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig6_HTML.png" alt="figure 6" loading="lazy" width="685" height="312"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-6-desc"><p>Object classes available in BRIDGE dataset [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Goyal, S., Mistry, V., Chattopadhyay, C., Bhatnagar, G.: BRIDGE: building plan repository for image description generation, and evaluation. In: ICDAR (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR16" id="ref-link-section-d9079034e4271">16</a>]</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/6" data-track-dest="link:Figure6 Full size image" aria-label="Full size image figure 6" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-7" data-title="Fig. 7"><figure><figcaption><b id="Fig7" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 7</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/7" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig7_HTML.png?as=webp"><img aria-describedby="Fig7" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig7_HTML.png" alt="figure 7" loading="lazy" width="685" height="370"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-7-desc"><p>Qualitative result of the spotted decor symbols in two floor plan images</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/7" data-track-dest="link:Figure7 Full size image" aria-label="Full size image figure 7" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-8" data-title="Fig. 8"><figure><figcaption><b id="Fig8" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 8</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/8" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig8_HTML.png?as=webp"><img aria-describedby="Fig8" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig8_HTML.png" alt="figure 8" loading="lazy" width="685" height="241"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-8-desc"><p>A visualization of the image classification network with top-6 activation maps from each layers</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/8" data-track-dest="link:Figure8 Full size image" aria-label="Full size image figure 8" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section data-title="Results of the proposed models"><div class="c-article-section" id="Sec23-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec23"><span class="c-article-section__title-number">6 </span>Results of the proposed models</h2><div class="c-article-section__content" id="Sec23-content"><p>In the next sections, results generated with the proposed models are described in detail. To validate the superiority of the proposed models DSIC and TBDG, the description generation by a multistaged pipeline with deep learning is also proposed and a comparative analysis is done to validate the choice of the networks used. In the next sections, steps involved in visual element detection are described in detail. It also discusses the resultant detection and classification of visual elements in the proposed pipeline.</p><h3 class="c-article__sub-heading" id="Sec24"><span class="c-article-section__title-number">6.1 </span> Decor symbol detection and classification</h3><p>Symbol spotting is a widespread problem in document image interpretation. In [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Goyal, S., Mistry, V., Chattopadhyay, C., Bhatnagar, G.: BRIDGE: building plan repository for image description generation, and evaluation. In: ICDAR (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR16" id="ref-link-section-d9079034e4333">16</a>], there are annotations for the decor symbols. In this work, we have adapted the YOLO model [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 37" title="Redmon, J., Farhadi, A.: YOLO9000: better, faster, stronger. In: CVPR (2017)" href="/article/10.1007/s10032-021-00367-3#ref-CR37" id="ref-link-section-d9079034e4336">37</a>] for detecting and classifying the decors by fine-tuning it using the decor symbol annotations present in the BRIDGE dataset. The symbol spotting network has <i>nine</i> convolutional layers with max-pool layers in between and is fine-tuned for 16 object categories (as shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig6">6</a>). The trained network has 105 filters (for BRIDGE dataset) and a linear activation function. The predicted class confidence score is calculated as <span class="mathjax-tex">\(Prob(object) \times IoU\)</span> . Here, <i>IoU</i> is the intersection of union between the predicted bounding box and the ground truth bounding box. It is calculated as <span class="mathjax-tex">\(IoU= \frac{Area\ of\ Intersection}{Area\ of\ Union}\)</span>. At the same time, <i>Prob</i>(<i>object</i>) is the probability of detecting the object in that bounding box. The decor symbols detected here <span class="mathjax-tex">\((o_i)\)</span>, are used in generating semi-structured descriptions in the later stage. The decor symbols in floor plans can vary widely because of the representation across different datasets. Also, in the real-world floor plans made by architects, the model might differ. We introduced variability by including samples of floor plans from different datasets such as [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="de las Heras, L.P., Terrades, O.R., Robles, S., Sánchez, G.: CVC-FP and SGT: a new database for structural floor plan analysis and its groundtruthing tool. IJDAR 18(1), 15–30 (2015)" href="/article/10.1007/s10032-021-00367-3#ref-CR7" id="ref-link-section-d9079034e4546">7</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 8" title="Delalandre, M., Valveny, E., Pridmore, T., Karatzas, D.: Generation of synthetic documents for performance evaluation of symbol recognition &amp; spotting systems. IJDAR 13(3), 187–207 (2010)" href="/article/10.1007/s10032-021-00367-3#ref-CR8" id="ref-link-section-d9079034e4550">8</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Sharma, D., Gupta, N., Chattopadhyay, C., Mehta, S.: DANIEL: A deep architecture for automatic analysis and retrieval of building floor plans. In: ICDAR (2017)" href="/article/10.1007/s10032-021-00367-3#ref-CR45" id="ref-link-section-d9079034e4553">45</a>] for decor symbol annotations. The training dataset covers a wide range of decor symbols, making the network detect and recognize the symbols’ variability. The detected decor symbols in floor plan images are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig7">7</a>. The two images are taken from BRIDGE datasets and show the variability in decor symbols for two different floor plan images. Wide variability in decor symbols is included in the training dataset to make the detection model more general. The symbols which are not detected, for example, “billiard” and “cooking range,” are not included in symbol annotations.</p><h3 class="c-article__sub-heading" id="Sec25"><span class="c-article-section__title-number">6.2 </span>Room characterization</h3><p>Room characterization is a step to recognize and classify individual rooms in a floor plan to their respective class. In this regard, rooms in each floor plan are classified into <i>five</i> classes, <i>Bedroom, Bathroom, Kitchen, Hall, Living room</i>. Annotations for each room class are taken from BRIDGE dataset, where region bounding boxes are available and class names are taken from the region-wise captions for the respective bounding box. A deep learning image classification model using VGG19 as a backbone network is used as a classification framework. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig8">8</a> depicts the framework diagram of the model and visualization of the network output for a class image “Bedroom.” In this network, only the last 5 layers are kept trainable in VGG19 and appended with two dense and dropout (0.5) layers. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig8">8</a> depicts that the activations for the initial layers retain almost the entire information from the image, focusing on specific parts such as edges and the image’s background. However, in the deeper layers, activations are less visually interpretable. The characterized rooms(<i>r</i>) from an input floor plan image are stored as <span class="mathjax-tex">\((r_1,r_2,\ldots ,r_n)\)</span>. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig9">9</a> shows the resultant classification for floor plan image room classification framework into <i>five</i> defined classes. The empty spaces in the floor plan are not marked as any room class in the BRIDGE dataset, hence they are not classified by the model. VGG19 pretrained on ImageNet dataset is fine-tuned with a 1920 training sample of 5 room classes, and validation is done over 460 samples. The training data contains a mixed sample of room images from [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 7" title="de las Heras, L.P., Terrades, O.R., Robles, S., Sánchez, G.: CVC-FP and SGT: a new database for structural floor plan analysis and its groundtruthing tool. IJDAR 18(1), 15–30 (2015)" href="/article/10.1007/s10032-021-00367-3#ref-CR7" id="ref-link-section-d9079034e4643">7</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Goyal, S., Mistry, V., Chattopadhyay, C., Bhatnagar, G.: BRIDGE: building plan repository for image description generation, and evaluation. In: ICDAR (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR16" id="ref-link-section-d9079034e4646">16</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 45" title="Sharma, D., Gupta, N., Chattopadhyay, C., Mehta, S.: DANIEL: A deep architecture for automatic analysis and retrieval of building floor plans. In: ICDAR (2017)" href="/article/10.1007/s10032-021-00367-3#ref-CR45" id="ref-link-section-d9079034e4649">45</a>]. The rooms <span class="mathjax-tex">\(r_i\)</span>, generated here are used in generating multistaged descriptions in the later stage. The number of samples for each class in the training and validation dataset are: <i>Bedroom:</i> 440 and 86, <i>Bathroom:</i> 887 and 223, <i>Kitchen:</i> 287 and 72, <i>Hall:</i> 75 and 21, <i>Living Room:</i> 231 and 58 in the respective order.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-9" data-title="Fig. 9"><figure><figcaption><b id="Fig9" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 9</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/9" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig9_HTML.png?as=webp"><img aria-describedby="Fig9" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig9_HTML.png" alt="figure 9" loading="lazy" width="685" height="415"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-9-desc"><p>Qualitative result of room classification for <i>five</i> classes on two different floor plan images</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/9" data-track-dest="link:Figure9 Full size image" aria-label="Full size image figure 9" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-10" data-title="Fig. 10"><figure><figcaption><b id="Fig10" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 10</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/10" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig10_HTML.png?as=webp"><img aria-describedby="Fig10" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig10_HTML.png" alt="figure 10" loading="lazy" width="685" height="479"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-10-desc"><p>An illustration of semi-structured description generation from floor plan images</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/10" data-track-dest="link:Figure10 Full size image" aria-label="Full size image figure 10" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-11" data-title="Fig. 11"><figure><figcaption><b id="Fig11" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 11</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/11" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig11_HTML.png?as=webp"><img aria-describedby="Fig11" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig11_HTML.png" alt="figure 11" loading="lazy" width="685" height="229"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-11-desc"><p>Descriptions generated with proposed models and various baseline models</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/11" data-track-dest="link:Figure11 Full size image" aria-label="Full size image figure 11" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-chevron-right"></use></svg></a></div></figure></div><h3 class="c-article__sub-heading" id="Sec26"><span class="c-article-section__title-number">6.3 </span>Description generation</h3><p>In the previous sections, different visual elements from the floor plan are detected and classified using various deep learning models. In the multistaged pipeline, these visual elements are used for a semi-structured sentence model proposed in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Goyal, S., Chattopadhyay, C., Bhatnagar, G.: ASYSST: a framework for synopsis synthesis empowering visually impaired. In: MAHCI (2018)" href="/article/10.1007/s10032-021-00367-3#ref-CR14" id="ref-link-section-d9079034e4754">14</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Goyal, S., Bhavsar, S., Patel, S., Chattopadhyay, C., Bhatnagar, G.: SUGAMAN: describing floor plans for visually impaired by annotation learning and proximity-based grammar. Image Process. 13(13), 2623–2635 (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR13" id="ref-link-section-d9079034e4757">13</a>], and a description for the given floor plan is generated. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig10">10</a> depicts an example where the synthesized descriptions for a given floor plan image with the visual elements described in the previous steps. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig11">11</a> shows the results generated by the proposed models, TBDG, DSIC, semi-structured sentence-based model, and other baseline models. In this paper, a comparison of semi-structured sentences with learned sentences is presented to demonstrate the superiority of end-to-end learning models with multistaged pipelines. Multistaged pipeline for floor plan recognition and description generation is presented here as a comparison with the end-to-end models, DSIC and TBDG. Multistaged pipelines have been used in the literature for floor plan recognition and description generation in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Goyal, S., Bhavsar, S., Patel, S., Chattopadhyay, C., Bhatnagar, G.: SUGAMAN: describing floor plans for visually impaired by annotation learning and proximity-based grammar. Image Process. 13(13), 2623–2635 (2019)" href="#ref-CR13" id="ref-link-section-d9079034e4766">13</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" title="Goyal, S., Chattopadhyay, C., Bhatnagar, G.: ASYSST: a framework for synopsis synthesis empowering visually impaired. In: MAHCI (2018)" href="#ref-CR14" id="ref-link-section-d9079034e4766_1">14</a>,<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 15" title="Goyal, S., Chattopadhyay, C., Bhatnagar, G.: Plan2Text: a framework for describing building floor plan images from first person perspective. In: CSPA (2018)" href="/article/10.1007/s10032-021-00367-3#ref-CR15" id="ref-link-section-d9079034e4770">15</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 29" title="Madugalla, A., Marriott, K., Marinai, S., Capobianco, S., Goncu, C.: Creating accessible online floor plans for visually impaired readers. ACM T-ACCESS 13(4), 1–37 (2020)" href="/article/10.1007/s10032-021-00367-3#ref-CR29" id="ref-link-section-d9079034e4773">29</a>]. In multistage pipelines, the accuracy of the generated descriptions depends upon the accuracy of the intermediate stages. Hence, mis-classification of one component will lead to error in the output sentence. This rationale is the driving factor to come up with an end-to-end learning model with advanced deep neural networks. In the next sections, comparative analysis for various modules and sub-modules are discussed in detail, along with the qualitative and quantitative evaluation of generated descriptions.</p></div></div></section><section data-title="Comparative analysis with state of the art"><div class="c-article-section" id="Sec27-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec27"><span class="c-article-section__title-number">7 </span>Comparative analysis with state of the art</h2><div class="c-article-section__content" id="Sec27-content"><p>In this section, a qualitative and quantitative comparative analysis with various state-of-the-art schemes are presented based on the metrics discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10032-021-00367-3#Sec22">5.2</a>.</p><h3 class="c-article__sub-heading" id="Sec28"><span class="c-article-section__title-number">7.1 </span>Comparative analysis of multistaged pipeline</h3><p>In this sub-section, we present how the various stages of the proposed multistaged pipeline have performed.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec29"><span class="c-article-section__title-number">7.1.1 </span>Decor identification</h4>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-12" data-title="Fig. 12"><figure><figcaption><b id="Fig12" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 12</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/12" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig12_HTML.png?as=webp"><img aria-describedby="Fig12" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig12_HTML.png" alt="figure 12" loading="lazy" width="685" height="409"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-12-desc"><p>A comparison of YOLO and Faster-RCNN models for decor identification</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/12" data-track-dest="link:Figure12 Full size image" aria-label="Full size image figure 12" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-13" data-title="Fig. 13"><figure><figcaption><b id="Fig13" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 13</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/13" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig13_HTML.png?as=webp"><img aria-describedby="Fig13" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig13_HTML.png" alt="figure 13" loading="lazy" width="685" height="267"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-13-desc"><p>Performance evaluation of room classification</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/13" data-track-dest="link:Figure13 Full size image" aria-label="Full size image figure 13" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-chevron-right"></use></svg></a></div></figure></div>
                    <div class="c-article-table" data-test="inline-table" data-container-section="table" id="table-1"><figure><figcaption class="c-article-table__figcaption"><b id="Tab1" data-test="table-caption">Table 1 Evaluation of generated paragraphs with different metrices (METEOR, BLEU, ROUGE)</b></figcaption><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="table-link" data-track="click" data-track-action="view table" data-track-label="button" rel="nofollow" href="/article/10.1007/s10032-021-00367-3/tables/1" aria-label="Full size table 1"><span>Full size table</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-chevron-right"></use></svg></a></div></figure></div>
                  <p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig12">12</a> depicts a comparative analysis of YOLO and F-RCNN models trained on BRIDGE dataset. The mAP obtained for decor symbol spotting network using YOLO is <span class="mathjax-tex">\(82.06\%\)</span> and for F-RCNN is <span class="mathjax-tex">\(75.25 \%\)</span>. For a few categories of symbols, F-RCNN is performing better, but the overall mAP is <span class="mathjax-tex">\(\sim 7\%\)</span> higher for YOLO. Hence, YOLO is used in the model instead of faster-RCNN given the better performance. Furthermore in the work [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 40" title="Rezvanifar, A., Cote, M., Branzan Albu, A.: Symbol spotting on digital architectural floor plans using a deep learning-based framework. In: Proceedings of the IEEE/CVF CVPR Workshops, pp. 568–569 (2020)" href="/article/10.1007/s10032-021-00367-3#ref-CR40" id="ref-link-section-d9079034e5598">40</a>], symbol spotting from architectural images is done for occluded and cluttered plans using YOLO, concluding the fact that YOLO as a single shot detector performs better than two-stage classification networks such as faster-RCNN for architectural drawings.</p><h4 class="c-article__sub-heading c-article__sub-heading--small" id="Sec30"><span class="c-article-section__title-number">7.1.2 </span>Room characterization</h4><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig13">13</a>a, b depicts the performance of image cues/ visual elements extraction from floor plan images for room classification. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig13">13</a>a is the training and validation accuracy and loss curves for the room image classification using VGG19 backbone network. After training for 50 epochs, an accuracy of <span class="mathjax-tex">\(82.98\%\)</span> could be achieved in-room image classification. The fluctuation of validation loss is due to the uneven distribution of the number of images in all <i>five</i> classes. A fivefold cross-validation over the data samples was performed on training data to validate the model.</p><p>The room image classification model discussed in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10032-021-00367-3#Sec25">6.2</a>, was also implemented using much recent Capsule network [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 42" title="Sabour, S., Frosst, N., Hinton, G.E.: Dynamic routing between capsules. In: Advances in neural information processing systems, pp. 3856–3866 (2017)" href="/article/10.1007/s10032-021-00367-3#ref-CR42" id="ref-link-section-d9079034e5646">42</a>] as a backbone network, which gave a classification accuracy of <span class="mathjax-tex">\(56.01 \%\)</span>, making VGG19 the obvious choice for the backbone network. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig13">13</a>b is the training and validation accuracy for the room classification model using Capsule network. The performance of the room characterization on BRIDGE dataset was also tested with classical machine learning methods proposed in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Goyal, S., Bhavsar, S., Patel, S., Chattopadhyay, C., Bhatnagar, G.: SUGAMAN: describing floor plans for visually impaired by annotation learning and proximity-based grammar. Image Process. 13(13), 2623–2635 (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR13" id="ref-link-section-d9079034e5674">13</a>, <a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Goyal, S., Chattopadhyay, C., Bhatnagar, G.: ASYSST: a framework for synopsis synthesis empowering visually impaired. In: MAHCI (2018)" href="/article/10.1007/s10032-021-00367-3#ref-CR14" id="ref-link-section-d9079034e5678">14</a>]. BoD classifier with multilayered perceptron, proposed in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 14" title="Goyal, S., Chattopadhyay, C., Bhatnagar, G.: ASYSST: a framework for synopsis synthesis empowering visually impaired. In: MAHCI (2018)" href="/article/10.1007/s10032-021-00367-3#ref-CR14" id="ref-link-section-d9079034e5681">14</a>], gave a validation accuracy of <span class="mathjax-tex">\(61.30 \%\)</span> and LOFD proposed in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Goyal, S., Bhavsar, S., Patel, S., Chattopadhyay, C., Bhatnagar, G.: SUGAMAN: describing floor plans for visually impaired by annotation learning and proximity-based grammar. Image Process. 13(13), 2623–2635 (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR13" id="ref-link-section-d9079034e5706">13</a>] with multilayered perceptron gave <span class="mathjax-tex">\(63.75 \%\)</span> of accuracy, while the validation accuracy of the proposed model is <span class="mathjax-tex">\(82.98\%\)</span>, making VGG19 a suitable choice for room classification model. In Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig9">9</a>, the two images have variability in the representation of each room class; however, the features leared using CNNs are much robust in the case of variable representation of images of the same class compared to handcrafted features, leading to higher validation accuracy.</p><h3 class="c-article__sub-heading" id="Sec31"><span class="c-article-section__title-number">7.2 </span>Quantitative evaluation of description generation</h3><p>In this sub-section, we present the quantitative evaluation of our proposed model with other state-of-the-art. The baseline descriptions are generated using language modeling where models such as LSTM [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8), 1735–1780 (1997)" href="/article/10.1007/s10032-021-00367-3#ref-CR19" id="ref-link-section-d9079034e5766">19</a>], Bi-LSTM [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 19" title="Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. 9(8), 1735–1780 (1997)" href="/article/10.1007/s10032-021-00367-3#ref-CR19" id="ref-link-section-d9079034e5769">19</a>] and GRU [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 6" title="Cho, K., Van Merriënboer, B., Bahdanau, D., Bengio, Y.: On the properties of neural machine translation: encoder-decoder approaches. arXiv preprint &#xA;                  arXiv:1409.1259&#xA;                  &#xA;                 (2014)" href="/article/10.1007/s10032-021-00367-3#ref-CR6" id="ref-link-section-d9079034e5772">6</a>] are experimented with. Language modeling is done by learning an entire corpus. These paragraph corpus are the textual descriptions for floor plans [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Goyal, S., Mistry, V., Chattopadhyay, C., Bhatnagar, G.: BRIDGE: building plan repository for image description generation, and evaluation. In: ICDAR (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR16" id="ref-link-section-d9079034e5775">16</a>]. The generated descriptions from the proposed models and presented baselines are compared on various matrices defined in Sect. <a data-track="click" data-track-label="link" data-track-action="section anchor" href="/article/10.1007/s10032-021-00367-3#Sec22">5.2</a>, and the quantitative results are presented in Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10032-021-00367-3#Tab1">1</a>.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig14">14</a>a shows the loss curve for TBDG for the part of language learning (Sequence2Sequence training, LSTM as encoder, Bi-LSTM as decoder), Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig14">14</a>b shows the loss curve for DSIC for the language learning part (CNN as encoder, hierarchical RNN as decoder). In TBDG, since LSTM and Bi-LSTM layers are used for training on word features, in the network, the loss converged below 1 and became stable in 51 epochs. In DSIC, training LSTM based hierarchical RNN with image features took a longer epoch time to converge than TBDG because of the larger number of trainable parameters. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig15">15</a>a–c shows the loss curves for the baseline language models for language modeling, i.e., LSTM, Bi-LSTM and GRU models, respectively. As it can be seen that the loss value reached below 1 but did not become 0 while training for 20 epochs. Additionally, GRU has similar benefits, but they are more efficient than LSTMs when training with more data is required. In this case, LSTM and Bi-LSTM took <span class="mathjax-tex">\(\sim 550\)</span> ms/epoch, while GRU took <span class="mathjax-tex">\(\sim 300\)</span> ms/epoch. The loss value in GRU got stabilized earlier than LSTMs while trained for 50 epochs.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-14" data-title="Fig. 14"><figure><figcaption><b id="Fig14" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 14</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/14" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig14_HTML.png?as=webp"><img aria-describedby="Fig14" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig14_HTML.png" alt="figure 14" loading="lazy" width="685" height="320"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-14-desc"><p>Performance evaluation for TBDG &amp; DSIC models</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/14" data-track-dest="link:Figure14 Full size image" aria-label="Full size image figure 14" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-15" data-title="Fig. 15"><figure><figcaption><b id="Fig15" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 15</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/15" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig15_HTML.png?as=webp"><img aria-describedby="Fig15" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig15_HTML.png" alt="figure 15" loading="lazy" width="685" height="229"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-15-desc"><p>A comparison of loss curves of language models</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/15" data-track-dest="link:Figure15 Full size image" aria-label="Full size image figure 15" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-16" data-title="Fig. 16"><figure><figcaption><b id="Fig16" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 16</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/16" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig16_HTML.png?as=webp"><img aria-describedby="Fig16" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig16_HTML.png" alt="figure 16" loading="lazy" width="685" height="476"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-16-desc"><p>Descriptions generated with proposed models and various baseline models</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/16" data-track-dest="link:Figure16 Full size image" aria-label="Full size image figure 16" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-chevron-right"></use></svg></a></div></figure></div><p>Table <a data-track="click" data-track-label="link" data-track-action="table anchor" href="/article/10.1007/s10032-021-00367-3#Tab1">1</a> shows the quantitative comparison of the description synthesis with the proposed models and the presented baseline models for various metrics with the ground truth paragraphs available in [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 16" title="Goyal, S., Mistry, V., Chattopadhyay, C., Bhatnagar, G.: BRIDGE: building plan repository for image description generation, and evaluation. In: ICDAR (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR16" id="ref-link-section-d9079034e5899">16</a>] where the values in bold represents the highest value of a particular metric for a given model. The evaluation is done on BLEU-<span class="mathjax-tex">\(\{1,2,3,4\}\)</span>, <span class="mathjax-tex">\( ROUGE_L \)</span>, and METEOR, where the BLEU score variant depends upon the n-gram. It can be seen that the performance of TBDG is better than all other description generation schemes on all the metrics except for BLEU-3, 4. It is least in semi-structured template-based method, and Densecap-concatenated paragraphs (taking top 5 sentences from Densecap model trained on floor plans) since the sentences have a fixed structure given different input images. However, the performance increases for the language models, LSTM, Bi-LSTM, and GRU even if they do not generate image-specific sentences. These language models generate phrases and context used in the training corpus while generating sentences when we use a seed sentence to create a paragraph, which increases the BLEU scores for different n-grams. <span class="mathjax-tex">\(ROUGE_L\)</span> also gives the highest precision, recall and f-score values for the TBDG model. Hence, it can be concluded that the knowledge-driven description generation (TBDG) performs better than generating descriptions directly from image cues (visual features). Other language models generate sentences using corpus phrases but not specific to the input image, which is not very useful in the current scenario. The qualitative evaluation and comparison of the proposed models with the baseline models are discussed next.</p><h3 class="c-article__sub-heading" id="Sec32"><span class="c-article-section__title-number">7.3 </span>Qualitative evaluation of description generation</h3><p>All the paragraph descriptions generated by various techniques are shown in Fig. <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig16">16</a>, which are corresponding to the images shown in the respective sets of descriptions.</p><p>Results show that paragraphs generated by [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 13" title="Goyal, S., Bhavsar, S., Patel, S., Chattopadhyay, C., Bhatnagar, G.: SUGAMAN: describing floor plans for visually impaired by annotation learning and proximity-based grammar. Image Process. 13(13), 2623–2635 (2019)" href="/article/10.1007/s10032-021-00367-3#ref-CR13" id="ref-link-section-d9079034e6021">13</a>] and [<a data-track="click" data-track-action="reference anchor" data-track-label="link" data-test="citation-ref" aria-label="Reference 20" title="Johnson, J., Karpathy, A., Fei-Fei, L.: Densecap: fully convolutional localization networks for dense captioning. In: CVPR (2016)" href="/article/10.1007/s10032-021-00367-3#ref-CR20" id="ref-link-section-d9079034e6024">20</a>] are simple and have a fixed structure and they do not have flexibility. They do not describe the connection of a room with another in a global context. However, paragraphs generated from DSIC and TBDG are very descriptive and close to human written sentences. They also include specific details of the images, for example, details about the contents of a bedroom, such as closets and bathrooms, details about the staircase in a hall. They also include details about other areas in the floor plan, for example, porches and garage, which multistage-based methods fail to describe because they do not have these room classes included in their training data. These models themselves capture intricate details in the descriptions, in which multistage-based methods fail, since they require explicit annotation for every component. Moreover, paragraphs generated from other baselines, LSTM, Bi-LSTM, GRU language models, are generating phrases and words related to the language structure but possess very less relevance to the input image. Hence, these kinds of models are suitable for poetry, story, and abstract generation but not for image to paragraph generation.</p><p>Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig17">17</a> shows the failed prediction of paragraph for the proposed model TBDG.</p><p>Sometimes the model fails to generate longer sequences or the words which are less frequent in the vocabulary, and then it starts repeating the sentences. Figure <a data-track="click" data-track-label="link" data-track-action="figure anchor" href="/article/10.1007/s10032-021-00367-3#Fig18">18</a> shows the failure case specific to DSIC and the requirement of the TBDG model for the input floor plan image. However, DSIC yielded descriptions with details related to the plan but not relevant to the current image. Hence, with TBDG, the generated sentences describe the details of bedrooms and bathrooms, taking cues from the words. Hence, it validates the robustness of the TBDG model over DSIC for a general floor plan image.</p><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-17" data-title="Fig. 17"><figure><figcaption><b id="Fig17" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 17</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/17" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig17_HTML.png?as=webp"><img aria-describedby="Fig17" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig17_HTML.png" alt="figure 17" loading="lazy" width="685" height="376"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-17-desc"><p>An illustration of a failure case with the TBDG model</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/17" data-track-dest="link:Figure17 Full size image" aria-label="Full size image figure 17" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-chevron-right"></use></svg></a></div></figure></div><div class="c-article-section__figure js-c-reading-companion-figures-item" data-test="figure" data-container-section="figure" id="figure-18" data-title="Fig. 18"><figure><figcaption><b id="Fig18" class="c-article-section__figure-caption" data-test="figure-caption-text">Fig. 18</b></figcaption><div class="c-article-section__figure-content"><div class="c-article-section__figure-item"><a class="c-article-section__figure-link" data-test="img-link" data-track="click" data-track-label="image" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/18" rel="nofollow"><picture><source type="image/webp" srcset="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig18_HTML.png?as=webp"><img aria-describedby="Fig18" src="//media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs10032-021-00367-3/MediaObjects/10032_2021_367_Fig18_HTML.png" alt="figure 18" loading="lazy" width="685" height="388"></picture></a></div><div class="c-article-section__figure-description" data-test="bottom-caption" id="figure-18-desc"><p>An illustration depicting the robustness of the TBDG model over DSIC for an unknown sample</p></div></div><div class="u-text-right u-hide-print"><a class="c-article__pill-button" data-test="article-link" data-track="click" data-track-label="button" data-track-action="view figure" href="/article/10.1007/s10032-021-00367-3/figures/18" data-track-dest="link:Figure18 Full size image" aria-label="Full size image figure 18" rel="nofollow"><span>Full size image</span><svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-chevron-right"></use></svg></a></div></figure></div></div></div></section><section data-title="Conclusion"><div class="c-article-section" id="Sec33-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Sec33"><span class="c-article-section__title-number">8 </span>Conclusion</h2><div class="c-article-section__content" id="Sec33-content"><p>In this work, we proposed two models, DSIC and TBDG for generating textual descriptions for floor plan images, which are graphical documents depicting a blueprint of a building. However, being a 2D line drawing images with binary pixel values makes them different from natural images. Hence, due to the lack of information at every pixel, various state-of-the-art description generation methods for natural images do not perform well for floor plan images. Therefore, we proposed a transformer-based image to paragraph generation scheme (TBDG), which takes both image and word cues to create a paragraph. We also proposed a hierarchical recurrent neural network-based model (DSIC) to generate descriptions by directly learning features from the image, which lacks robustness in the case of a general floor plan image. We evaluated the proposed model on different metrics by presenting several baselines language models for description generation and proposing a deep learning based multistaged pipeline to generate descriptions from floor plan images. We trained and tested the proposed models and baselines on the BRIDGE dataset, which contains large-scale floor plan images and annotations for various tasks. In future work, these models will be made more generalized to generate descriptions for widely variable floor plan images by improving the network architecture and redesigning the method of taking word cues.
</p></div></div></section>
                                </div>
                            
                        

                        <div id="MagazineFulltextArticleBodySuffix"><section aria-labelledby="Bib1" data-title="References"><div class="c-article-section" id="Bib1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Bib1">References</h2><div class="c-article-section__content" id="Bib1-content"><div data-container-section="references"><ol class="c-article-references" data-track-component="outbound reference"><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="1."><p class="c-article-references__text" id="ref-CR1">Adam, S., Ogier, J.M., Cariou, C., Mullot, R., Labiche, J., Gardes, J.: Symbol and character recognition: application to engineering drawings. IJDAR <b>3</b>(2), 89–101 (2000)</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1007/s100320000033" data-track-action="article reference" href="https://doi.org/10.1007%2Fs100320000033" aria-label="Article reference 1" data-doi="10.1007/s100320000033">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 1" href="http://scholar.google.com/scholar_lookup?&amp;title=Symbol%20and%20character%20recognition%3A%20application%20to%20engineering%20drawings&amp;journal=IJDAR&amp;doi=10.1007%2Fs100320000033&amp;volume=3&amp;issue=2&amp;pages=89-101&amp;publication_year=2000&amp;author=Adam%2CS&amp;author=Ogier%2CJM&amp;author=Cariou%2CC&amp;author=Mullot%2CR&amp;author=Labiche%2CJ&amp;author=Gardes%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="2."><p class="c-article-references__text" id="ref-CR2">Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning to align and translate. arXiv preprint <a href="http://arxiv.org/abs/1409.0473">arXiv:1409.0473</a> (2014)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="3."><p class="c-article-references__text" id="ref-CR3">Barducci, A., Marinai, S.: Object recognition in floor plans by graphs of white connected components. In: ICPR (2012)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="4."><p class="c-article-references__text" id="ref-CR4">Chatterjee, M., Schwing, A.G.: Diverse and coherent paragraph generation from images. In: ECCV (2018)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="5."><p class="c-article-references__text" id="ref-CR5">Chen, X., Fang, H., Lin, T.Y., Vedantam, R., Gupta, S., Dollár, P., Zitnick, C.L.: Microsoft COCO captions: data collection and evaluation server. arXiv preprint <a href="http://arxiv.org/abs/1504.00325">arXiv:1504.00325</a> (2015)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="6."><p class="c-article-references__text" id="ref-CR6">Cho, K., Van Merriënboer, B., Bahdanau, D., Bengio, Y.: On the properties of neural machine translation: encoder-decoder approaches. arXiv preprint <a href="http://arxiv.org/abs/1409.1259">arXiv:1409.1259</a> (2014)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="7."><p class="c-article-references__text" id="ref-CR7">de las Heras, L.P., Terrades, O.R., Robles, S., Sánchez, G.: CVC-FP and SGT: a new database for structural floor plan analysis and its groundtruthing tool. IJDAR <b>18</b>(1), 15–30 (2015)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="8."><p class="c-article-references__text" id="ref-CR8">Delalandre, M., Valveny, E., Pridmore, T., Karatzas, D.: Generation of synthetic documents for performance evaluation of symbol recognition &amp; spotting systems. IJDAR <b>13</b>(3), 187–207 (2010)</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1007/s10032-010-0120-x" data-track-action="article reference" href="https://doi.org/10.1007%2Fs10032-010-0120-x" aria-label="Article reference 8" data-doi="10.1007/s10032-010-0120-x">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 8" href="http://scholar.google.com/scholar_lookup?&amp;title=Generation%20of%20synthetic%20documents%20for%20performance%20evaluation%20of%20symbol%20recognition%20%26%20spotting%20systems&amp;journal=IJDAR&amp;doi=10.1007%2Fs10032-010-0120-x&amp;volume=13&amp;issue=3&amp;pages=187-207&amp;publication_year=2010&amp;author=Delalandre%2CM&amp;author=Valveny%2CE&amp;author=Pridmore%2CT&amp;author=Karatzas%2CD">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="9."><p class="c-article-references__text" id="ref-CR9">Dutta, A., Llados, J., Pal, U.: Symbol spotting in line drawings through graph paths hashing. In: ICDAR (2011)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="10."><p class="c-article-references__text" id="ref-CR10">Dutta, A., Lladós, J., Pal, U.: A symbol spotting approach in graphical documents by hashing serialized graphs. Pattern Recognit. <b>46</b>(3), 752–768 (2013)</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.patcog.2012.10.003" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.patcog.2012.10.003" aria-label="Article reference 10" data-doi="10.1016/j.patcog.2012.10.003">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 10" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20symbol%20spotting%20approach%20in%20graphical%20documents%20by%20hashing%20serialized%20graphs&amp;journal=Pattern%20Recognit.&amp;doi=10.1016%2Fj.patcog.2012.10.003&amp;volume=46&amp;issue=3&amp;pages=752-768&amp;publication_year=2013&amp;author=Dutta%2CA&amp;author=Llad%C3%B3s%2CJ&amp;author=Pal%2CU">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="11."><p class="c-article-references__text" id="ref-CR11">Farhadi, A., Hejrati, M., Sadeghi, M.A., Young, P., Rashtchian, C., Hockenmaier, J., Forsyth, D.: Every picture tells a story: generating sentences from images. In: ECCV (2010)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="12."><p class="c-article-references__text" id="ref-CR12">Girshick, R.: Fast R-CNN. In: ICCV (2015)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="13."><p class="c-article-references__text" id="ref-CR13">Goyal, S., Bhavsar, S., Patel, S., Chattopadhyay, C., Bhatnagar, G.: SUGAMAN: describing floor plans for visually impaired by annotation learning and proximity-based grammar. Image Process. <b>13</b>(13), 2623–2635 (2019)</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1049/iet-ipr.2018.5627" data-track-action="article reference" href="https://doi.org/10.1049%2Fiet-ipr.2018.5627" aria-label="Article reference 13" data-doi="10.1049/iet-ipr.2018.5627">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 13" href="http://scholar.google.com/scholar_lookup?&amp;title=SUGAMAN%3A%20describing%20floor%20plans%20for%20visually%20impaired%20by%20annotation%20learning%20and%20proximity-based%20grammar&amp;journal=Image%20Process.&amp;doi=10.1049%2Fiet-ipr.2018.5627&amp;volume=13&amp;issue=13&amp;pages=2623-2635&amp;publication_year=2019&amp;author=Goyal%2CS&amp;author=Bhavsar%2CS&amp;author=Patel%2CS&amp;author=Chattopadhyay%2CC&amp;author=Bhatnagar%2CG">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="14."><p class="c-article-references__text" id="ref-CR14">Goyal, S., Chattopadhyay, C., Bhatnagar, G.: ASYSST: a framework for synopsis synthesis empowering visually impaired. In: MAHCI (2018)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="15."><p class="c-article-references__text" id="ref-CR15">Goyal, S., Chattopadhyay, C., Bhatnagar, G.: Plan2Text: a framework for describing building floor plan images from first person perspective. In: CSPA (2018)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="16."><p class="c-article-references__text" id="ref-CR16">Goyal, S., Mistry, V., Chattopadhyay, C., Bhatnagar, G.: BRIDGE: building plan repository for image description generation, and evaluation. In: ICDAR (2019)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="17."><p class="c-article-references__text" id="ref-CR17">He, K., Gkioxari, G., Dollár, P., Girshick, R.: Mask R-CNN. In: ICCV (2017)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="18."><p class="c-article-references__text" id="ref-CR18">He, K., Zhang, X., Ren, S., Sun, J.: Spatial pyramid pooling in deep convolutional networks for visual recognition. T-PAMI <b>37</b>(9), 1904–1916 (2015)</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1109/TPAMI.2015.2389824" data-track-action="article reference" href="https://doi.org/10.1109%2FTPAMI.2015.2389824" aria-label="Article reference 18" data-doi="10.1109/TPAMI.2015.2389824">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 18" href="http://scholar.google.com/scholar_lookup?&amp;title=Spatial%20pyramid%20pooling%20in%20deep%20convolutional%20networks%20for%20visual%20recognition&amp;journal=T-PAMI&amp;doi=10.1109%2FTPAMI.2015.2389824&amp;volume=37&amp;issue=9&amp;pages=1904-1916&amp;publication_year=2015&amp;author=He%2CK&amp;author=Zhang%2CX&amp;author=Ren%2CS&amp;author=Sun%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="19."><p class="c-article-references__text" id="ref-CR19">Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural Comput. <b>9</b>(8), 1735–1780 (1997)</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1162/neco.1997.9.8.1735" data-track-action="article reference" href="https://doi.org/10.1162%2Fneco.1997.9.8.1735" aria-label="Article reference 19" data-doi="10.1162/neco.1997.9.8.1735">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 19" href="http://scholar.google.com/scholar_lookup?&amp;title=Long%20short-term%20memory&amp;journal=Neural%20Comput.&amp;doi=10.1162%2Fneco.1997.9.8.1735&amp;volume=9&amp;issue=8&amp;pages=1735-1780&amp;publication_year=1997&amp;author=Hochreiter%2CS&amp;author=Schmidhuber%2CJ">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="20."><p class="c-article-references__text" id="ref-CR20">Johnson, J., Karpathy, A., Fei-Fei, L.: Densecap: fully convolutional localization networks for dense captioning. In: CVPR (2016)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="21."><p class="c-article-references__text" id="ref-CR21">Khan, I., Islam, N., Rehman, H.U., Khan, M.: A comparative study of graphic symbol recognition methods. Multimedia Tools Appl. <b>79</b>(13), 8695–8725 (2020)</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1007/s11042-018-6289-6" data-track-action="article reference" href="https://doi.org/10.1007%2Fs11042-018-6289-6" aria-label="Article reference 21" data-doi="10.1007/s11042-018-6289-6">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 21" href="http://scholar.google.com/scholar_lookup?&amp;title=A%20comparative%20study%20of%20graphic%20symbol%20recognition%20methods&amp;journal=Multimedia%20Tools%20Appl.&amp;doi=10.1007%2Fs11042-018-6289-6&amp;volume=79&amp;issue=13&amp;pages=8695-8725&amp;publication_year=2020&amp;author=Khan%2CI&amp;author=Islam%2CN&amp;author=Rehman%2CHU&amp;author=Khan%2CM">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="22."><p class="c-article-references__text" id="ref-CR22">Krause, J., Johnson, J., Krishna, R., Fei-Fei, L.: A hierarchical approach for generating descriptive image paragraphs. In: CVPR (2017)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="23."><p class="c-article-references__text" id="ref-CR23">Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.J., Shamma, D.A., et al.: Visual genome: connecting language and vision using crowdsourced dense image annotations. IJCV <b>123</b>(1), 32–73 (2017)</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1007/s11263-016-0981-7" data-track-action="article reference" href="https://doi.org/10.1007%2Fs11263-016-0981-7" aria-label="Article reference 23" data-doi="10.1007/s11263-016-0981-7">Article</a> 
    <a data-track="click" rel="nofollow noopener" data-track-label="link" data-track-action="mathscinet reference" href="http://www.ams.org/mathscinet-getitem?mr=3640738" aria-label="MathSciNet reference 23">MathSciNet</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 23" href="http://scholar.google.com/scholar_lookup?&amp;title=Visual%20genome%3A%20connecting%20language%20and%20vision%20using%20crowdsourced%20dense%20image%20annotations&amp;journal=IJCV&amp;doi=10.1007%2Fs11263-016-0981-7&amp;volume=123&amp;issue=1&amp;pages=32-73&amp;publication_year=2017&amp;author=Krishna%2CR&amp;author=Zhu%2CY&amp;author=Groth%2CO&amp;author=Johnson%2CJ&amp;author=Hata%2CK&amp;author=Kravitz%2CJ&amp;author=Chen%2CS&amp;author=Kalantidis%2CY&amp;author=Li%2CLJ&amp;author=Shamma%2CDA">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="24."><p class="c-article-references__text" id="ref-CR24">Kulkarni, G., Premraj, V., Ordonez, V., Dhar, S., Li, S., Choi, Y., Berg, A.C., Berg, T.L.: Babytalk: understanding and generating simple image descriptions. T-PAMI <b>35</b>(12), 2891–2903 (2013)</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1109/TPAMI.2012.162" data-track-action="article reference" href="https://doi.org/10.1109%2FTPAMI.2012.162" aria-label="Article reference 24" data-doi="10.1109/TPAMI.2012.162">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 24" href="http://scholar.google.com/scholar_lookup?&amp;title=Babytalk%3A%20understanding%20and%20generating%20simple%20image%20descriptions&amp;journal=T-PAMI&amp;doi=10.1109%2FTPAMI.2012.162&amp;volume=35&amp;issue=12&amp;pages=2891-2903&amp;publication_year=2013&amp;author=Kulkarni%2CG&amp;author=Premraj%2CV&amp;author=Ordonez%2CV&amp;author=Dhar%2CS&amp;author=Li%2CS&amp;author=Choi%2CY&amp;author=Berg%2CAC&amp;author=Berg%2CTL">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="25."><p class="c-article-references__text" id="ref-CR25">Li, S., Kulkarni, G., Berg, T.L., Berg, A.C., Choi, Y.: Composing simple image descriptions using web-scale n-grams. In: CoNLL (2011)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="26."><p class="c-article-references__text" id="ref-CR26">Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L.: Microsoft COCO: common objects in context. In: ECCV (2014)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="27."><p class="c-article-references__text" id="ref-CR27">Liu, Y., Fu, J., Mei, T., Chen, C.W.: Let your photos talk: Generating narrative paragraph for photo stream via bidirectional attention recurrent neural networks. In: AAAI (2017)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="28."><p class="c-article-references__text" id="ref-CR28">Luong, M.T., Pham, H., Manning, C.D.: Effective approaches to attention-based neural machine translation. arXiv preprint <a href="http://arxiv.org/abs/1508.04025">arXiv:1508.04025</a> (2015)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="29."><p class="c-article-references__text" id="ref-CR29">Madugalla, A., Marriott, K., Marinai, S., Capobianco, S., Goncu, C.: Creating accessible online floor plans for visually impaired readers. ACM T-ACCESS <b>13</b>(4), 1–37 (2020)</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1145/3410446" data-track-action="article reference" href="https://doi.org/10.1145%2F3410446" aria-label="Article reference 29" data-doi="10.1145/3410446">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 29" href="http://scholar.google.com/scholar_lookup?&amp;title=Creating%20accessible%20online%20floor%20plans%20for%20visually%20impaired%20readers&amp;journal=ACM%20T-ACCESS&amp;doi=10.1145%2F3410446&amp;volume=13&amp;issue=4&amp;pages=1-37&amp;publication_year=2020&amp;author=Madugalla%2CA&amp;author=Marriott%2CK&amp;author=Marinai%2CS&amp;author=Capobianco%2CS&amp;author=Goncu%2CC">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="30."><p class="c-article-references__text" id="ref-CR30">Mao, Y., Zhou, C., Wang, X., Li, R.: Show and tell more: topic-oriented multi-sentence image captioning. In: IJCAI (2018)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="31."><p class="c-article-references__text" id="ref-CR31">Marcus, M., Santorini, B., Marcinkiewicz, M.A.: Building a large annotated corpus of English: The Penn Treebank (1993)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="32."><p class="c-article-references__text" id="ref-CR32">Nallapati, R., Zhou, B., Gulcehre, C., Xiang, B., et al.: Abstractive text summarization using sequence-to-sequence RNNs and beyond. arXiv preprint <a href="http://arxiv.org/abs/1602.06023">arXiv:1602.06023</a> (2016)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="33."><p class="c-article-references__text" id="ref-CR33">Ordonez, V., Kulkarni, G., Berg, T.L.: Im2Text: describing images using 1 million captioned photographs. In: NIPS (2011)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="34."><p class="c-article-references__text" id="ref-CR34">Park, C.C., Kim, G.: Expressing an image stream with a sequence of natural sentences. In: NIPS (2015)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="35."><p class="c-article-references__text" id="ref-CR35">Qureshi, R.J., Ramel, J.Y., Barret, D., Cardot, H.: Spotting symbols in line drawing images using graph representations. In: GREC (2007)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="36."><p class="c-article-references__text" id="ref-CR36">Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: unified, real-time object detection. In: CVPR (2016)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="37."><p class="c-article-references__text" id="ref-CR37">Redmon, J., Farhadi, A.: YOLO9000: better, faster, stronger. In: CVPR (2017)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="38."><p class="c-article-references__text" id="ref-CR38">Redmon, J., Farhadi, A.: YOLOv3: an incremental improvement. arXiv preprint <a href="http://arxiv.org/abs/1804.02767">arXiv:1804.02767</a> (2018)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="39."><p class="c-article-references__text" id="ref-CR39">Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: towards real-time object detection with region proposal networks. In: NIPS (2015)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="40."><p class="c-article-references__text" id="ref-CR40">Rezvanifar, A., Cote, M., Branzan Albu, A.: Symbol spotting on digital architectural floor plans using a deep learning-based framework. In: Proceedings of the IEEE/CVF CVPR Workshops, pp. 568–569 (2020)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="41."><p class="c-article-references__text" id="ref-CR41">Rush, A.M., Chopra, S., Weston, J.: A neural attention model for abstractive sentence summarization. arXiv preprint <a href="http://arxiv.org/abs/1509.00685">arXiv:1509.00685</a> (2015)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="42."><p class="c-article-references__text" id="ref-CR42">Sabour, S., Frosst, N., Hinton, G.E.: Dynamic routing between capsules. In: Advances in neural information processing systems, pp. 3856–3866 (2017)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="43."><p class="c-article-references__text" id="ref-CR43">Saha, R., Mondal, A., Jawahar, C.: Graphical Object Detection in Document Images. In: ICDAR (2019)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="44."><p class="c-article-references__text" id="ref-CR44">Schreiber, S., Agne, S., Wolf, I., Dengel, A., Ahmed, S.: Deepdesrt: deep learning for detection and structure recognition of tables in document images. In: ICDAR (2017)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="45."><p class="c-article-references__text" id="ref-CR45">Sharma, D., Gupta, N., Chattopadhyay, C., Mehta, S.: DANIEL: A deep architecture for automatic analysis and retrieval of building floor plans. In: ICDAR (2017)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="46."><p class="c-article-references__text" id="ref-CR46">Sharma, N., Mandal, R., Sharma, R., Pal, U., Blumenstein, M.: Signature and Logo Detection using Deep CNN for Document Image Retrieval. In: ICFHR (2018)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="47."><p class="c-article-references__text" id="ref-CR47">Su, H., Gong, S., Zhu, X.: Scalable logo detection by self co-learning. Pattern Recognition <b>97</b>, 107003 (2020)</p><p class="c-article-references__links u-hide-print"><a data-track="click" rel="nofollow noopener" data-track-label="10.1016/j.patcog.2019.107003" data-track-action="article reference" href="https://doi.org/10.1016%2Fj.patcog.2019.107003" aria-label="Article reference 47" data-doi="10.1016/j.patcog.2019.107003">Article</a> 
    <a data-track="click" data-track-action="google scholar reference" data-track-label="link" rel="nofollow noopener" aria-label="Google Scholar reference 47" href="http://scholar.google.com/scholar_lookup?&amp;title=Scalable%20logo%20detection%20by%20self%20co-learning&amp;journal=Pattern%20Recognition&amp;doi=10.1016%2Fj.patcog.2019.107003&amp;volume=97&amp;publication_year=2020&amp;author=Su%2CH&amp;author=Gong%2CS&amp;author=Zhu%2CX">
                    Google Scholar</a> 
                </p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="48."><p class="c-article-references__text" id="ref-CR48">Sutskever, I., Vinyals, O., Le, Q.: Sequence to sequence learning with neural networks. NIPS (2014)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="49."><p class="c-article-references__text" id="ref-CR49">Viola, P., Jones, M.: Rapid object detection using a boosted cascade of simple features. In: CVPR (2001)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="50."><p class="c-article-references__text" id="ref-CR50">Wang, Q., Chan, A.B.: CNN+CNN: convolutional decoders for image captioning. arXiv preprint <a href="http://arxiv.org/abs/1805.09019">arXiv:1805.09019</a> (2018)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="51."><p class="c-article-references__text" id="ref-CR51">Wang, Z., Luo, Y., Li, Y., Huang, Z., Yin, H.: Look Deeper See Richer: Depth-aware Image Paragraph Captioning. In: ACM MM (2018)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="52."><p class="c-article-references__text" id="ref-CR52">Yao, T., Pan, Y., Li, Y., Qiu, Z., Mei, T.: Boosting image captioning with attributes. In: ICCV (2017)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="53."><p class="c-article-references__text" id="ref-CR53">Yi, X., Gao, L., Liao, Y., Zhang, X., Liu, R., Jiang, Z.: CNN based page object detection in document images. In: ICDAR (2017)</p></li><li class="c-article-references__item js-c-reading-companion-references-item" data-counter="54."><p class="c-article-references__text" id="ref-CR54">Ziran, Z., Marinai, S.: Object detection in floor plan images. In: IAPR Workshop on Artificial Neural Networks in Pattern Recognition (2018)</p></li></ol><p class="c-article-references__download u-hide-print"><a data-track="click" data-track-action="download citation references" data-track-label="link" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1007/s10032-021-00367-3?format=refman&amp;flavour=references">Download references<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-download"></use></svg></a></p></div></div></div></section></div><section data-title="Acknowledgements"><div class="c-article-section" id="Ack1-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="Ack1">Acknowledgements</h2><div class="c-article-section__content" id="Ack1-content"><p>Funding was provided by Science and Engineering Research Board (ECR/2016/000953).</p></div></div></section><section aria-labelledby="author-information" data-title="Author information"><div class="c-article-section" id="author-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="author-information">Author information</h2><div class="c-article-section__content" id="author-information-content"><h3 class="c-article__sub-heading" id="affiliations">Authors and Affiliations</h3><ol class="c-article-author-affiliation__list"><li id="Aff1"><p class="c-article-author-affiliation__address">Indian Institute of Technology, Jodhpur, 342037, India</p><p class="c-article-author-affiliation__authors-list">Shreya Goyal, Chiranjoy Chattopadhyay &amp; Gaurav Bhatnagar</p></li></ol><div class="u-js-hide u-hide-print" data-test="author-info"><span class="c-article__sub-heading">Authors</span><ol class="c-article-authors-search u-list-reset"><li id="auth-Shreya-Goyal"><span class="c-article-authors-search__title u-h3 js-search-name">Shreya Goyal</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=Shreya%20Goyal" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Shreya%20Goyal" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Shreya%20Goyal%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Chiranjoy-Chattopadhyay"><span class="c-article-authors-search__title u-h3 js-search-name">Chiranjoy Chattopadhyay</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=Chiranjoy%20Chattopadhyay" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Chiranjoy%20Chattopadhyay" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Chiranjoy%20Chattopadhyay%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li><li id="auth-Gaurav-Bhatnagar"><span class="c-article-authors-search__title u-h3 js-search-name">Gaurav Bhatnagar</span><div class="c-article-authors-search__list"><div class="c-article-authors-search__item c-article-authors-search__list-item--left"><a href="/search?dc.creator=Gaurav%20Bhatnagar" class="c-article-button" data-track="click" data-track-action="author link - publication" data-track-label="link" rel="nofollow">View author publications</a></div><div class="c-article-authors-search__item c-article-authors-search__list-item--right"><p class="search-in-title-js c-article-authors-search__text">You can also search for this author in
                        <span class="c-article-identifiers"><a class="c-article-identifiers__item" href="http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=search&amp;term=Gaurav%20Bhatnagar" data-track="click" data-track-action="author link - pubmed" data-track-label="link" rel="nofollow">PubMed</a><span class="u-hide"> </span><a class="c-article-identifiers__item" href="http://scholar.google.co.uk/scholar?as_q=&amp;num=10&amp;btnG=Search+Scholar&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=%22Gaurav%20Bhatnagar%22&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;as_allsubj=all&amp;hl=en" data-track="click" data-track-action="author link - scholar" data-track-label="link" rel="nofollow">Google Scholar</a></span></p></div></div></li></ol></div><h3 class="c-article__sub-heading" id="corresponding-author">Corresponding author</h3><p id="corresponding-author-list">Correspondence to
                <a id="corresp-c1" href="mailto:chiranjoy@iitj.ac.in">Chiranjoy Chattopadhyay</a>.</p></div></div></section><section data-title="Additional information"><div class="c-article-section" id="additional-information-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="additional-information">Additional information</h2><div class="c-article-section__content" id="additional-information-content"><h3 class="c-article__sub-heading">Publisher's Note</h3><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div></div></section><section data-title="Rights and permissions"><div class="c-article-section" id="rightslink-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="rightslink">Rights and permissions</h2><div class="c-article-section__content" id="rightslink-content"><p class="c-article-rights"><a data-track="click" data-track-action="view rights and permissions" data-track-label="link" href="https://s100.copyright.com/AppDispatchServlet?title=Knowledge-driven%20description%20synthesis%20for%20floor%20plan%20interpretation&amp;author=Shreya%20Goyal%20et%20al&amp;contentID=10.1007%2Fs10032-021-00367-3&amp;copyright=The%20Author%28s%29%2C%20under%20exclusive%20licence%20to%20Springer-Verlag%20GmbH%20Germany%2C%20part%20of%20Springer%20Nature&amp;publication=1433-2833&amp;publicationDate=2021-04-26&amp;publisherName=SpringerNature&amp;orderBeanReset=true">Reprints and Permissions</a></p></div></div></section><section aria-labelledby="article-info" data-title="About this article"><div class="c-article-section" id="article-info-section"><h2 class="c-article-section__title js-section-title js-c-reading-companion-sections-item" id="article-info">About this article</h2><div class="c-article-section__content" id="article-info-content"><div class="c-bibliographic-information"><div class="u-hide-print c-bibliographic-information__column c-bibliographic-information__column--border"><a data-crossmark="10.1007/s10032-021-00367-3" target="_blank" rel="noopener" href="https://crossmark.crossref.org/dialog/?doi=10.1007/s10032-021-00367-3" data-track="click" data-track-action="Click Crossmark" data-track-label="link" data-test="crossmark"><img width="57" height="81" alt="Check for updates. Verify currency and authenticity via CrossMark" src="data:image/svg+xml;base64,<svg height="81" width="57" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m17.35 35.45 21.3-14.2v-17.03h-21.3" fill="#989898"/><path d="m38.65 35.45-21.3-14.2v-17.03h21.3" fill="#747474"/><path d="m28 .5c-12.98 0-23.5 10.52-23.5 23.5s10.52 23.5 23.5 23.5 23.5-10.52 23.5-23.5c0-6.23-2.48-12.21-6.88-16.62-4.41-4.4-10.39-6.88-16.62-6.88zm0 41.25c-9.8 0-17.75-7.95-17.75-17.75s7.95-17.75 17.75-17.75 17.75 7.95 17.75 17.75c0 4.71-1.87 9.22-5.2 12.55s-7.84 5.2-12.55 5.2z" fill="#535353"/><path d="m41 36c-5.81 6.23-15.23 7.45-22.43 2.9-7.21-4.55-10.16-13.57-7.03-21.5l-4.92-3.11c-4.95 10.7-1.19 23.42 8.78 29.71 9.97 6.3 23.07 4.22 30.6-4.86z" fill="#9c9c9c"/><path d="m.2 58.45c0-.75.11-1.42.33-2.01s.52-1.09.91-1.5c.38-.41.83-.73 1.34-.94.51-.22 1.06-.32 1.65-.32.56 0 1.06.11 1.51.35.44.23.81.5 1.1.81l-.91 1.01c-.24-.24-.49-.42-.75-.56-.27-.13-.58-.2-.93-.2-.39 0-.73.08-1.05.23-.31.16-.58.37-.81.66-.23.28-.41.63-.53 1.04-.13.41-.19.88-.19 1.39 0 1.04.23 1.86.68 2.46.45.59 1.06.88 1.84.88.41 0 .77-.07 1.07-.23s.59-.39.85-.68l.91 1c-.38.43-.8.76-1.28.99-.47.22-1 .34-1.58.34-.59 0-1.13-.1-1.64-.31-.5-.2-.94-.51-1.31-.91-.38-.4-.67-.9-.88-1.48-.22-.59-.33-1.26-.33-2.02zm8.4-5.33h1.61v2.54l-.05 1.33c.29-.27.61-.51.96-.72s.76-.31 1.24-.31c.73 0 1.27.23 1.61.71.33.47.5 1.14.5 2.02v4.31h-1.61v-4.1c0-.57-.08-.97-.25-1.21-.17-.23-.45-.35-.83-.35-.3 0-.56.08-.79.22-.23.15-.49.36-.78.64v4.8h-1.61zm7.37 6.45c0-.56.09-1.06.26-1.51.18-.45.42-.83.71-1.14.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.36c.07.62.29 1.1.65 1.44.36.33.82.5 1.38.5.29 0 .57-.04.83-.13s.51-.21.76-.37l.55 1.01c-.33.21-.69.39-1.09.53-.41.14-.83.21-1.26.21-.48 0-.92-.08-1.34-.25-.41-.16-.76-.4-1.07-.7-.31-.31-.55-.69-.72-1.13-.18-.44-.26-.95-.26-1.52zm4.6-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.07.45-.31.29-.5.73-.58 1.3zm2.5.62c0-.57.09-1.08.28-1.53.18-.44.43-.82.75-1.13s.69-.54 1.1-.71c.42-.16.85-.24 1.31-.24.45 0 .84.08 1.17.23s.61.34.85.57l-.77 1.02c-.19-.16-.38-.28-.56-.37-.19-.09-.39-.14-.61-.14-.56 0-1.01.21-1.35.63-.35.41-.52.97-.52 1.67 0 .69.17 1.24.51 1.66.34.41.78.62 1.32.62.28 0 .54-.06.78-.17.24-.12.45-.26.64-.42l.67 1.03c-.33.29-.69.51-1.08.65-.39.15-.78.23-1.18.23-.46 0-.9-.08-1.31-.24-.4-.16-.75-.39-1.05-.7s-.53-.69-.7-1.13c-.17-.45-.25-.96-.25-1.53zm6.91-6.45h1.58v6.17h.05l2.54-3.16h1.77l-2.35 2.8 2.59 4.07h-1.75l-1.77-2.98-1.08 1.23v1.75h-1.58zm13.69 1.27c-.25-.11-.5-.17-.75-.17-.58 0-.87.39-.87 1.16v.75h1.34v1.27h-1.34v5.6h-1.61v-5.6h-.92v-1.2l.92-.07v-.72c0-.35.04-.68.13-.98.08-.31.21-.57.4-.79s.42-.39.71-.51c.28-.12.63-.18 1.04-.18.24 0 .48.02.69.07.22.05.41.1.57.17zm.48 5.18c0-.57.09-1.08.27-1.53.17-.44.41-.82.72-1.13.3-.31.65-.54 1.04-.71.39-.16.8-.24 1.23-.24s.84.08 1.24.24c.4.17.74.4 1.04.71s.54.69.72 1.13c.19.45.28.96.28 1.53s-.09 1.08-.28 1.53c-.18.44-.42.82-.72 1.13s-.64.54-1.04.7-.81.24-1.24.24-.84-.08-1.23-.24-.74-.39-1.04-.7c-.31-.31-.55-.69-.72-1.13-.18-.45-.27-.96-.27-1.53zm1.65 0c0 .69.14 1.24.43 1.66.28.41.68.62 1.18.62.51 0 .9-.21 1.19-.62.29-.42.44-.97.44-1.66 0-.7-.15-1.26-.44-1.67-.29-.42-.68-.63-1.19-.63-.5 0-.9.21-1.18.63-.29.41-.43.97-.43 1.67zm6.48-3.44h1.33l.12 1.21h.05c.24-.44.54-.79.88-1.02.35-.24.7-.36 1.07-.36.32 0 .59.05.78.14l-.28 1.4-.33-.09c-.11-.01-.23-.02-.38-.02-.27 0-.56.1-.86.31s-.55.58-.77 1.1v4.2h-1.61zm-47.87 15h1.61v4.1c0 .57.08.97.25 1.2.17.24.44.35.81.35.3 0 .57-.07.8-.22.22-.15.47-.39.73-.73v-4.7h1.61v6.87h-1.32l-.12-1.01h-.04c-.3.36-.63.64-.98.86-.35.21-.76.32-1.24.32-.73 0-1.27-.24-1.61-.71-.33-.47-.5-1.14-.5-2.02zm9.46 7.43v2.16h-1.61v-9.59h1.33l.12.72h.05c.29-.24.61-.45.97-.63.35-.17.72-.26 1.1-.26.43 0 .81.08 1.15.24.33.17.61.4.84.71.24.31.41.68.53 1.11.13.42.19.91.19 1.44 0 .59-.09 1.11-.25 1.57-.16.47-.38.85-.65 1.16-.27.32-.58.56-.94.73-.35.16-.72.25-1.1.25-.3 0-.6-.07-.9-.2s-.59-.31-.87-.56zm0-2.3c.26.22.5.37.73.45.24.09.46.13.66.13.46 0 .84-.2 1.15-.6.31-.39.46-.98.46-1.77 0-.69-.12-1.22-.35-1.61-.23-.38-.61-.57-1.13-.57-.49 0-.99.26-1.52.77zm5.87-1.69c0-.56.08-1.06.25-1.51.16-.45.37-.83.65-1.14.27-.3.58-.54.93-.71s.71-.25 1.08-.25c.39 0 .73.07 1 .2.27.14.54.32.81.55l-.06-1.1v-2.49h1.61v9.88h-1.33l-.11-.74h-.06c-.25.25-.54.46-.88.64-.33.18-.69.27-1.06.27-.87 0-1.56-.32-2.07-.95s-.76-1.51-.76-2.65zm1.67-.01c0 .74.13 1.31.4 1.7.26.38.65.58 1.15.58.51 0 .99-.26 1.44-.77v-3.21c-.24-.21-.48-.36-.7-.45-.23-.08-.46-.12-.7-.12-.45 0-.82.19-1.13.59-.31.39-.46.95-.46 1.68zm6.35 1.59c0-.73.32-1.3.97-1.71.64-.4 1.67-.68 3.08-.84 0-.17-.02-.34-.07-.51-.05-.16-.12-.3-.22-.43s-.22-.22-.38-.3c-.15-.06-.34-.1-.58-.1-.34 0-.68.07-1 .2s-.63.29-.93.47l-.59-1.08c.39-.24.81-.45 1.28-.63.47-.17.99-.26 1.54-.26.86 0 1.51.25 1.93.76s.63 1.25.63 2.21v4.07h-1.32l-.12-.76h-.05c-.3.27-.63.48-.98.66s-.73.27-1.14.27c-.61 0-1.1-.19-1.48-.56-.38-.36-.57-.85-.57-1.46zm1.57-.12c0 .3.09.53.27.67.19.14.42.21.71.21.28 0 .54-.07.77-.2s.48-.31.73-.56v-1.54c-.47.06-.86.13-1.18.23-.31.09-.57.19-.76.31s-.33.25-.41.4c-.09.15-.13.31-.13.48zm6.29-3.63h-.98v-1.2l1.06-.07.2-1.88h1.34v1.88h1.75v1.27h-1.75v3.28c0 .8.32 1.2.97 1.2.12 0 .24-.01.37-.04.12-.03.24-.07.34-.11l.28 1.19c-.19.06-.4.12-.64.17-.23.05-.49.08-.76.08-.4 0-.74-.06-1.02-.18-.27-.13-.49-.3-.67-.52-.17-.21-.3-.48-.37-.78-.08-.3-.12-.64-.12-1.01zm4.36 2.17c0-.56.09-1.06.27-1.51s.41-.83.71-1.14c.29-.3.63-.54 1.01-.71.39-.17.78-.25 1.18-.25.47 0 .88.08 1.23.24.36.16.65.38.89.67s.42.63.54 1.03c.12.41.18.84.18 1.32 0 .32-.02.57-.07.76h-4.37c.08.62.29 1.1.65 1.44.36.33.82.5 1.38.5.3 0 .58-.04.84-.13.25-.09.51-.21.76-.37l.54 1.01c-.32.21-.69.39-1.09.53s-.82.21-1.26.21c-.47 0-.92-.08-1.33-.25-.41-.16-.77-.4-1.08-.7-.3-.31-.54-.69-.72-1.13-.17-.44-.26-.95-.26-1.52zm4.61-.62c0-.55-.11-.98-.34-1.28-.23-.31-.58-.47-1.06-.47-.41 0-.77.15-1.08.45-.31.29-.5.73-.57 1.3zm3.01 2.23c.31.24.61.43.92.57.3.13.63.2.98.2.38 0 .65-.08.83-.23s.27-.35.27-.6c0-.14-.05-.26-.13-.37-.08-.1-.2-.2-.34-.28-.14-.09-.29-.16-.47-.23l-.53-.22c-.23-.09-.46-.18-.69-.3-.23-.11-.44-.24-.62-.4s-.33-.35-.45-.55c-.12-.21-.18-.46-.18-.75 0-.61.23-1.1.68-1.49.44-.38 1.06-.57 1.83-.57.48 0 .91.08 1.29.25s.71.36.99.57l-.74.98c-.24-.17-.49-.32-.73-.42-.25-.11-.51-.16-.78-.16-.35 0-.6.07-.76.21-.17.15-.25.33-.25.54 0 .14.04.26.12.36s.18.18.31.26c.14.07.29.14.46.21l.54.19c.23.09.47.18.7.29s.44.24.64.4c.19.16.34.35.46.58.11.23.17.5.17.82 0 .3-.06.58-.17.83-.12.26-.29.48-.51.68-.23.19-.51.34-.84.45-.34.11-.72.17-1.15.17-.48 0-.95-.09-1.41-.27-.46-.19-.86-.41-1.2-.68z" fill="#535353"/></g></svg>"></a></div><div class="c-bibliographic-information__column"><h3 class="c-article__sub-heading" id="citeas">Cite this article</h3><p class="c-bibliographic-information__citation">Goyal, S., Chattopadhyay, C. &amp; Bhatnagar, G. Knowledge-driven description synthesis for floor plan interpretation.
                    <i>IJDAR</i> <b>24</b>, 19–32 (2021). https://doi.org/10.1007/s10032-021-00367-3</p><p class="c-bibliographic-information__download-citation u-hide-print"><a data-test="citation-link" data-track="click" data-track-action="download article citation" data-track-label="link" data-track-external="" rel="nofollow" href="https://citation-needed.springer.com/v2/references/10.1007/s10032-021-00367-3?format=refman&amp;flavour=citation">Download citation<svg width="16" height="16" focusable="false" role="img" aria-hidden="true" class="u-icon"><use xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="#icon-download"></use></svg></a></p><ul class="c-bibliographic-information__list" data-test="publication-history"><li class="c-bibliographic-information__list-item"><p>Received<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2020-01-10">10 January 2020</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Revised<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2021-03-31">31 March 2021</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Accepted<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2021-04-11">11 April 2021</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Published<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2021-04-26">26 April 2021</time></span></p></li><li class="c-bibliographic-information__list-item"><p>Issue Date<span class="u-hide">: </span><span class="c-bibliographic-information__value"><time datetime="2021-06">June 2021</time></span></p></li><li class="c-bibliographic-information__list-item c-bibliographic-information__list-item--doi"><p><abbr title="Digital Object Identifier">DOI</abbr><span class="u-hide">: </span><span class="c-bibliographic-information__value">https://doi.org/10.1007/s10032-021-00367-3</span></p></li></ul><div data-component="share-box"><div class="c-article-share-box u-display-none" hidden=""><h3 class="c-article__sub-heading">Share this article</h3><p class="c-article-share-box__description">Anyone you share the following link with will be able to read this content:</p><button class="js-get-share-url c-article-share-box__button" type="button" id="get-share-url" data-track="click" data-track-label="button" data-track-external="" data-track-action="get shareable link">Get shareable link</button><div class="js-no-share-url-container u-display-none" hidden=""><p class="js-c-article-share-box__no-sharelink-info c-article-share-box__no-sharelink-info">Sorry, a shareable link is not currently available for this article.</p></div><div class="js-share-url-container u-display-none" hidden=""><p class="js-share-url c-article-share-box__only-read-input" id="share-url" data-track="click" data-track-label="button" data-track-action="select share url"></p><button class="js-copy-share-url c-article-share-box__button--link-like" type="button" id="copy-share-url" data-track="click" data-track-label="button" data-track-action="copy share url" data-track-external="">Copy to clipboard</button></div><p class="js-c-article-share-box__additional-info c-article-share-box__additional-info">
                            Provided by the Springer Nature SharedIt content-sharing initiative
                        </p></div></div><h3 class="c-article__sub-heading">Keywords</h3><ul class="c-article-subject-list"><li class="c-article-subject-list__subject"><span>Floor plan</span></li><li class="c-article-subject-list__subject"><span>Captioning</span></li><li class="c-article-subject-list__subject"><span>Evaluation</span></li><li class="c-article-subject-list__subject"><span>Language modeling</span></li></ul><div data-component="article-info-list"></div></div></div></div></div></section>
                    </div>
                </article>
            </main>

            <div class="c-article-extras u-text-sm u-hide-print" id="sidebar" data-container-type="reading-companion"
                 data-track-component="reading companion">
                <aside>
                    

                    <div data-test="download-article-link-wrapper" class="js-context-bar-sticky-point-desktop">
                        
    <div class="c-pdf-container">
        <div class="c-pdf-download u-clear-both u-mb-16">
            <a href="/content/pdf/10.1007/s10032-021-00367-3.pdf?pdf=button" class="u-button u-button--full-width u-button--primary u-justify-content-space-between c-pdf-download__link" data-article-pdf="true" data-readcube-pdf-url="true" data-test="pdf-link" data-draft-ignore="true" data-track="click" data-track-action="download pdf" data-track-label="button" data-track-external download>
                
                    <span class="c-pdf-download__text">Download PDF</span>
                    <svg aria-hidden="true" focusable="false" width="16" height="16" class="u-icon"><use xlink:href="#icon-download"/></svg>
                
            </a>
        </div>
    </div>


                    </div>

                    
                        
    <div class="app-checklist-banner" data-test="article-checklist-banner">
        <div class="app-checklist-banner__body">
            <h3 class="app-checklist-banner__title">Working on a manuscript?</h3>
            <a class="app-checklist-banner__link" data-track="click" data-track-category="pre-submission-checklist" data-track-action="clicked article page checklist banner test 2 old version" data-track-label="link" href="https://beta.springernature.com/pre-submission?journalId=10032"
            data-test="article-checklist-banner-link">Avoid the common mistakes
            <svg class="app-checklist-banner__arrow-icon" aria-hidden="true" focusable="false">
                <use xlink:href="#icon-springer-arrow-right"></use>
            </svg>
            </a>
        </div>
        <div class="app-checklist-banner__icon-container">
        <svg class="app-checklist-banner__paper-icon" aria-hidden="true" focusable="false">
            <use xlink:href="#icon-checklist-banner"></use>
        </svg>
        </div>
    </div>

                    

                    <div data-test="collections">
                        
    

                    </div>

                    <div data-test="editorial-summary">
                        
                    </div>

                    <div class="c-reading-companion">
                        <div class="c-reading-companion__sticky" data-component="reading-companion-sticky"
                             data-test="reading-companion-sticky">
                            

                            <div
                                class="c-reading-companion__panel c-reading-companion__sections c-reading-companion__panel--active"
                                id="tabpanel-sections">
                                <div class="u-lazy-ad-wrapper u-mt-16 u-hide"
                                     data-component-mpu><div class="c-ad c-ad--300x250">
    <div class="c-ad__inner">
        <p class="c-ad__label">Advertisement</p>
        <div id="div-gpt-ad-MPU1"
             class="div-gpt-ad grade-c-hide"
             data-pa11y-ignore
             data-gpt
             data-gpt-unitpath="/270604982/springerlink/10032/article"
             data-gpt-sizes="300x250" data-test="MPU1-ad"
             data-gpt-targeting="pos=MPU1;articleid=s10032-021-00367-3;">
        </div>
    </div>
</div>

</div>
                            </div>
                            <div
                                class="c-reading-companion__panel c-reading-companion__figures c-reading-companion__panel--full-width"
                                id="tabpanel-figures"></div>
                            <div
                                class="c-reading-companion__panel c-reading-companion__references c-reading-companion__panel--full-width"
                                id="tabpanel-references"></div>
                        </div>
                    </div>
                </aside>
            </div>
        </div>

        
        <script>
                
        </script>
    

        
    
        <div class="app-elements">
            

<div class="c-header__expander" id="popup-search">
    <h2 class="c-header__heading">Search</h2>
    <div class="u-container">
        <div class="c-header__search">
            <form role="search" method="GET" action="//link.springer.com/search"
                  data-track="submit"
                  data-track-action="submit search form"
                  data-track-category="unified header"
                  data-track-label="form">
                <label for="header-search" class="c-header__search-label">Search by keyword or author</label>
                <div class="c-header__search-container">
                    <input id="header-search" class="c-header__search-input" autocomplete="off" name="query" type="text" value="" required>
                    <button class="c-header__search-button" type="submit">
                        <svg class="c-header__icon" aria-hidden="true" focusable="false">
                            <use xlink:href="#icon-eds-search"></use>
                        </svg>
                        <span class="u-visually-hidden">Search</span>
                    </button>
                </div>
            </form>
        </div>
    </div>
</div>

<div class="c-header__expander c-header__expander--menu" id="header-nav">
    
        <h2 class="c-header__heading">Navigation</h2>
        <ul class="c-header__list">
            
                <li class="c-header__list-item"><a class="c-header__link" href="https://link.springer.com/journals/a/1" data-track="click" data-track-action="click find a journal" data-track-label="link">Find a journal</a></li>
            
                <li class="c-header__list-item"><a class="c-header__link" href="https://www.springernature.com/gp/authors" data-track="click" data-track-action="click publish with us link" data-track-label="link">Publish with us</a></li>
            
        </ul>
    
</div>
            <footer>
	<div class="c-footer" data-track-component="unified-footer">
		
			
			
	<div class="c-footer__container">
		<div class="c-footer__grid c-footer__group--separator">
			
			<div class="c-footer__group">
				<h3 class="c-footer__heading">Discover content</h3>
				<ul class="c-footer__list">
					
					<li class="c-footer__item"><a class="c-footer__link" href="https://link.springer.com/journals/a/1" data-track="click" data-track-action="journals a-z" data-track-label="link">Journals A-Z</a></li>
					
					<li class="c-footer__item"><a class="c-footer__link" href="https://link.springer.com/books/a/1" data-track="click" data-track-action="books a-z" data-track-label="link">Books A-Z</a></li>
					
				</ul>
			</div>
			
			<div class="c-footer__group">
				<h3 class="c-footer__heading">Publish with us</h3>
				<ul class="c-footer__list">
					
					<li class="c-footer__item"><a class="c-footer__link" href="https://www.springernature.com/gp/authors" data-track="click" data-track-action="publish your research" data-track-label="link">Publish your research</a></li>
					
					<li class="c-footer__item"><a class="c-footer__link" href="https://www.springernature.com/gp/open-research/about/the-fundamentals-of-open-access-and-open-research" data-track="click" data-track-action="open access publishing" data-track-label="link">Open access publishing</a></li>
					
				</ul>
			</div>
			
			<div class="c-footer__group">
				<h3 class="c-footer__heading">Products and services</h3>
				<ul class="c-footer__list">
					
					<li class="c-footer__item"><a class="c-footer__link" href="https://www.springernature.com/gp/products" data-track="click" data-track-action="our products" data-track-label="link">Our products</a></li>
					
					<li class="c-footer__item"><a class="c-footer__link" href="https://www.springernature.com/gp/librarians" data-track="click" data-track-action="librarians" data-track-label="link">Librarians</a></li>
					
					<li class="c-footer__item"><a class="c-footer__link" href="https://www.springernature.com/gp/societies" data-track="click" data-track-action="societies" data-track-label="link">Societies</a></li>
					
					<li class="c-footer__item"><a class="c-footer__link" href="https://www.springernature.com/gp/partners" data-track="click" data-track-action="partners and advertisers" data-track-label="link">Partners and advertisers</a></li>
					
				</ul>
			</div>
			
			<div class="c-footer__group">
				<h3 class="c-footer__heading">Our imprints</h3>
				<ul class="c-footer__list">
					
					<li class="c-footer__item"><a class="c-footer__link" href="https://www.springer.com/" data-track="click" data-track-action="Springer" data-track-label="link">Springer</a></li>
					
					<li class="c-footer__item"><a class="c-footer__link" href="https://www.nature.com/" data-track="click" data-track-action="Nature Portfolio" data-track-label="link">Nature Portfolio</a></li>
					
					<li class="c-footer__item"><a class="c-footer__link" href="https://www.biomedcentral.com/" data-track="click" data-track-action="BMC" data-track-label="link">BMC</a></li>
					
					<li class="c-footer__item"><a class="c-footer__link" href="https://www.palgrave.com/" data-track="click" data-track-action="Palgrave Macmillan" data-track-label="link">Palgrave Macmillan</a></li>
					
					<li class="c-footer__item"><a class="c-footer__link" href="https://www.apress.com/" data-track="click" data-track-action="Apress" data-track-label="link">Apress</a></li>
					
				</ul>
			</div>
			
		</div>
	</div>


		
		
		<div class="c-footer__container">
    
        <nav aria-label="footer navigation">
            <ul class="c-footer__links">
                
                    <li class="c-footer__item">
                        
                        
                            <button class="c-footer__link" data-cc-action="preferences" data-track="click" data-track-action="Manage cookies" data-track-label="link"><span class="c-footer__button-text">Your privacy choices/Manage cookies</span></button>
                        
                    </li>
                
                    <li class="c-footer__item">
                        
                            <a class="c-footer__link" href="https://www.springernature.com/gp/legal/ccpa" data-track="click" data-track-action="california privacy statement" data-track-label="link">Your US state privacy rights</a>
                        
                        
                    </li>
                
                    <li class="c-footer__item">
                        
                            <a class="c-footer__link" href="https://www.springernature.com/gp/info/accessibility" data-track="click" data-track-action="accessibility statement" data-track-label="link">Accessibility statement</a>
                        
                        
                    </li>
                
                    <li class="c-footer__item">
                        
                            <a class="c-footer__link" href="https://link.springer.com/termsandconditions" data-track="click" data-track-action="terms and conditions" data-track-label="link">Terms and conditions</a>
                        
                        
                    </li>
                
                    <li class="c-footer__item">
                        
                            <a class="c-footer__link" href="https://link.springer.com/privacystatement" data-track="click" data-track-action="privacy policy" data-track-label="link">Privacy policy</a>
                        
                        
                    </li>
                
                    <li class="c-footer__item">
                        
                            <a class="c-footer__link" href="https://support.springernature.com/en/support/home" data-track="click" data-track-action="help and support" data-track-label="link">Help and support</a>
                        
                        
                    </li>
                
            </ul>
        </nav>
    
    
        <div class="c-footer__user">
            <p class="c-footer__user-info">
                
                <span data-test="footer-user-ip">14.139.196.12</span>
            </p>
            <p class="c-footer__user-info" data-test="footer-business-partners">INDEST AICTE Consortium Indian Institute of Technology (3000185589)  - INDEST AICTE Consortium C/o Indian Institute of Technology (3000188743)  - Information and Library Network (INFLIBNET) Centre (3994475188)  - Indian Institute of Technology, Guwahati (2000341243) </p>
        </div>
    
    
        <a href="https://www.springernature.com/" class="c-footer__link">
            <img src="/oscar-static/images/darwin/footer/img/logo-springernature_white-64dbfad7d8.svg" alt="Springer Nature" loading="lazy" width="200" height="20"/>
        </a>
    
    <p class="c-footer__legal" data-test="copyright">&copy; 2023 Springer Nature</p>
</div>

	</div>
</footer>
        </div>
    


    </div>
    
    

    <div class="u-visually-hidden" aria-hidden="true">
    
    <?xml version="1.0" encoding="UTF-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><path id="a" d="M0 .74h56.72v55.24H0z"/></defs><symbol id="icon-access" viewBox="0 0 18 18"><path d="m14 8c.5522847 0 1 .44771525 1 1v7h2.5c.2761424 0 .5.2238576.5.5v1.5h-18v-1.5c0-.2761424.22385763-.5.5-.5h2.5v-7c0-.55228475.44771525-1 1-1s1 .44771525 1 1v6.9996556h8v-6.9996556c0-.55228475.4477153-1 1-1zm-8 0 2 1v5l-2 1zm6 0v7l-2-1v-5zm-2.42653766-7.59857636 7.03554716 4.92488299c.4162533.29137735.5174853.86502537.226108 1.28127873-.1721584.24594054-.4534847.39241464-.7536934.39241464h-14.16284822c-.50810197 0-.92-.41189803-.92-.92 0-.30020869.1464741-.58153499.39241464-.75369337l7.03554714-4.92488299c.34432015-.2410241.80260453-.2410241 1.14692468 0zm-.57346234 2.03988748-3.65526982 2.55868888h7.31053962z" fill-rule="evenodd"/></symbol><symbol id="icon-account" viewBox="0 0 18 18"><path d="m10.2379028 16.9048051c1.3083556-.2032362 2.5118471-.7235183 3.5294683-1.4798399-.8731327-2.5141501-2.0638925-3.935978-3.7673711-4.3188248v-1.27684611c1.1651924-.41183641 2-1.52307546 2-2.82929429 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.30621883.83480763 2.41745788 2 2.82929429v1.27684611c-1.70347856.3828468-2.89423845 1.8046747-3.76737114 4.3188248 1.01762123.7563216 2.22111275 1.2766037 3.52946833 1.4798399.40563808.0629726.81921174.0951949 1.23790281.0951949s.83226473-.0322223 1.2379028-.0951949zm4.3421782-2.1721994c1.4927655-1.4532925 2.419919-3.484675 2.419919-5.7326057 0-4.418278-3.581722-8-8-8s-8 3.581722-8 8c0 2.2479307.92715352 4.2793132 2.41991895 5.7326057.75688473-2.0164459 1.83949951-3.6071894 3.48926591-4.3218837-1.14534283-.70360829-1.90918486-1.96796271-1.90918486-3.410722 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.44275929-.763842 2.70711371-1.9091849 3.410722 1.6497664.7146943 2.7323812 2.3054378 3.4892659 4.3218837zm-5.580081 3.2673943c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-alert" viewBox="0 0 18 18"><path d="m4 10h2.5c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-3.08578644l-1.12132034 1.1213203c-.18753638.1875364-.29289322.4418903-.29289322.7071068v.1715729h14v-.1715729c0-.2652165-.1053568-.5195704-.2928932-.7071068l-1.7071068-1.7071067v-3.4142136c0-2.76142375-2.2385763-5-5-5-2.76142375 0-5 2.23857625-5 5zm3 4c0 1.1045695.8954305 2 2 2s2-.8954305 2-2zm-5 0c-.55228475 0-1-.4477153-1-1v-.1715729c0-.530433.21071368-1.0391408.58578644-1.4142135l1.41421356-1.4142136v-3c0-3.3137085 2.6862915-6 6-6s6 2.6862915 6 6v3l1.4142136 1.4142136c.3750727.3750727.5857864.8837805.5857864 1.4142135v.1715729c0 .5522847-.4477153 1-1 1h-4c0 1.6568542-1.3431458 3-3 3-1.65685425 0-3-1.3431458-3-3z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-broad" viewBox="0 0 16 16"><path d="m6.10307866 2.97190702v7.69043288l2.44965196-2.44676915c.38776071-.38730439 1.0088052-.39493524 1.38498697-.01919617.38609051.38563612.38643641 1.01053024-.00013864 1.39665039l-4.12239817 4.11754683c-.38616704.3857126-1.01187344.3861062-1.39846576-.0000311l-4.12258206-4.11773056c-.38618426-.38572979-.39254614-1.00476697-.01636437-1.38050605.38609047-.38563611 1.01018509-.38751562 1.4012233.00306241l2.44985644 2.4469734v-8.67638639c0-.54139983.43698413-.98042709.98493125-.98159081l7.89910522-.0043627c.5451687 0 .9871152.44142642.9871152.98595351s-.4419465.98595351-.9871152.98595351z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 14 15)"/></symbol><symbol id="icon-arrow-down" viewBox="0 0 16 16"><path d="m3.28337502 11.5302405 4.03074001 4.176208c.37758093.3912076.98937525.3916069 1.367372-.0000316l4.03091977-4.1763942c.3775978-.3912252.3838182-1.0190815.0160006-1.4001736-.3775061-.39113013-.9877245-.39303641-1.3700683.003106l-2.39538585 2.4818345v-11.6147896l-.00649339-.11662112c-.055753-.49733869-.46370161-.88337888-.95867408-.88337888-.49497246 0-.90292107.38604019-.95867408.88337888l-.00649338.11662112v11.6147896l-2.39518594-2.4816273c-.37913917-.39282218-.98637524-.40056175-1.35419292-.0194697-.37750607.3911302-.37784433 1.0249269.00013556 1.4165479z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-left" viewBox="0 0 16 16"><path d="m4.46975946 3.28337502-4.17620792 4.03074001c-.39120768.37758093-.39160691.98937525.0000316 1.367372l4.1763942 4.03091977c.39122514.3775978 1.01908149.3838182 1.40017357.0160006.39113012-.3775061.3930364-.9877245-.00310603-1.3700683l-2.48183446-2.39538585h11.61478958l.1166211-.00649339c.4973387-.055753.8833789-.46370161.8833789-.95867408 0-.49497246-.3860402-.90292107-.8833789-.95867408l-.1166211-.00649338h-11.61478958l2.4816273-2.39518594c.39282216-.37913917.40056173-.98637524.01946965-1.35419292-.39113012-.37750607-1.02492687-.37784433-1.41654791.00013556z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-right" viewBox="0 0 16 16"><path d="m11.5302405 12.716625 4.176208-4.03074003c.3912076-.37758093.3916069-.98937525-.0000316-1.367372l-4.1763942-4.03091981c-.3912252-.37759778-1.0190815-.38381821-1.4001736-.01600053-.39113013.37750607-.39303641.98772445.003106 1.37006824l2.4818345 2.39538588h-11.6147896l-.11662112.00649339c-.49733869.055753-.88337888.46370161-.88337888.95867408 0 .49497246.38604019.90292107.88337888.95867408l.11662112.00649338h11.6147896l-2.4816273 2.39518592c-.39282218.3791392-.40056175.9863753-.0194697 1.3541929.3911302.3775061 1.0249269.3778444 1.4165479-.0001355z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-sub" viewBox="0 0 16 16"><path d="m7.89692134 4.97190702v7.69043288l-2.44965196-2.4467692c-.38776071-.38730434-1.0088052-.39493519-1.38498697-.0191961-.38609047.3856361-.38643643 1.0105302.00013864 1.3966504l4.12239817 4.1175468c.38616704.3857126 1.01187344.3861062 1.39846576-.0000311l4.12258202-4.1177306c.3861843-.3857298.3925462-1.0047669.0163644-1.380506-.3860905-.38563612-1.0101851-.38751563-1.4012233.0030624l-2.44985643 2.4469734v-8.67638639c0-.54139983-.43698413-.98042709-.98493125-.98159081l-7.89910525-.0043627c-.54516866 0-.98711517.44142642-.98711517.98595351s.44194651.98595351.98711517.98595351z" fill-rule="evenodd"/></symbol><symbol id="icon-arrow-up" viewBox="0 0 16 16"><path d="m12.716625 4.46975946-4.03074003-4.17620792c-.37758093-.39120768-.98937525-.39160691-1.367372.0000316l-4.03091981 4.1763942c-.37759778.39122514-.38381821 1.01908149-.01600053 1.40017357.37750607.39113012.98772445.3930364 1.37006824-.00310603l2.39538588-2.48183446v11.61478958l.00649339.1166211c.055753.4973387.46370161.8833789.95867408.8833789.49497246 0 .90292107-.3860402.95867408-.8833789l.00649338-.1166211v-11.61478958l2.39518592 2.4816273c.3791392.39282216.9863753.40056173 1.3541929.01946965.3775061-.39113012.3778444-1.02492687-.0001355-1.41654791z" fill-rule="evenodd"/></symbol><symbol id="icon-article" viewBox="0 0 18 18"><path d="m13 15v-12.9906311c0-.0073595-.0019884-.0093689.0014977-.0093689l-11.00158888.00087166v13.00506804c0 .5482678.44615281.9940603.99415146.9940603h10.27350412c-.1701701-.2941734-.2675644-.6357129-.2675644-1zm-12 .0059397v-13.00506804c0-.5562408.44704472-1.00087166.99850233-1.00087166h11.00299537c.5510129 0 .9985023.45190985.9985023 1.0093689v2.9906311h3v9.9914698c0 1.1065798-.8927712 2.0085302-1.9940603 2.0085302h-12.01187942c-1.09954652 0-1.99406028-.8927712-1.99406028-1.9940603zm13-9.0059397v9c0 .5522847.4477153 1 1 1s1-.4477153 1-1v-9zm-10-2h7v4h-7zm1 1v2h5v-2zm-1 4h7v1h-7zm0 2h7v1h-7zm0 2h7v1h-7z" fill-rule="evenodd"/></symbol><symbol id="icon-audio" viewBox="0 0 18 18"><path d="m13.0957477 13.5588459c-.195279.1937043-.5119137.193729-.7072234.0000551-.1953098-.193674-.1953346-.5077061-.0000556-.7014104 1.0251004-1.0168342 1.6108711-2.3905226 1.6108711-3.85745208 0-1.46604976-.5850634-2.83898246-1.6090736-3.85566829-.1951894-.19379323-.1950192-.50782531.0003802-.70141028.1953993-.19358497.512034-.19341614.7072234.00037709 1.2094886 1.20083761 1.901635 2.8250555 1.901635 4.55670148 0 1.73268608-.6929822 3.35779608-1.9037571 4.55880738zm2.1233994 2.1025159c-.195234.193749-.5118687.1938462-.7072235.0002171-.1953548-.1936292-.1954528-.5076613-.0002189-.7014104 1.5832215-1.5711805 2.4881302-3.6939808 2.4881302-5.96012998 0-2.26581266-.9046382-4.3883241-2.487443-5.95944795-.1952117-.19377107-.1950777-.50780316.0002993-.70141031s.5120117-.19347426.7072234.00029682c1.7683321 1.75528196 2.7800854 4.12911258 2.7800854 6.66056144 0 2.53182498-1.0120556 4.90597838-2.7808529 6.66132328zm-14.21898205-3.6854911c-.5523759 0-1.00016505-.4441085-1.00016505-.991944v-3.96777631c0-.54783558.44778915-.99194407 1.00016505-.99194407h2.0003301l5.41965617-3.8393633c.44948677-.31842296 1.07413994-.21516983 1.39520191.23062232.12116339.16823446.18629727.36981184.18629727.57655577v12.01603479c0 .5478356-.44778914.9919441-1.00016505.9919441-.20845738 0-.41170538-.0645985-.58133413-.184766l-5.41965617-3.8393633zm0-.991944h2.32084805l5.68047235 4.0241292v-12.01603479l-5.68047235 4.02412928h-2.32084805z" fill-rule="evenodd"/></symbol><symbol id="icon-block" viewBox="0 0 24 24"><path d="m0 0h24v24h-24z" fill-rule="evenodd"/></symbol><symbol id="icon-book" viewBox="0 0 18 18"><path d="m4 13v-11h1v11h11v-11h-13c-.55228475 0-1 .44771525-1 1v10.2675644c.29417337-.1701701.63571286-.2675644 1-.2675644zm12 1h-13c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1h13zm0 3h-13c-1.1045695 0-2-.8954305-2-2v-12c0-1.1045695.8954305-2 2-2h13c.5522847 0 1 .44771525 1 1v14c0 .5522847-.4477153 1-1 1zm-8.5-13h6c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-6c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1 2h4c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-4c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-broad" viewBox="0 0 24 24"><path d="m9.18274226 7.81v7.7999954l2.48162734-2.4816273c.3928221-.3928221 1.0219731-.4005617 1.4030652-.0194696.3911301.3911301.3914806 1.0249268-.0001404 1.4165479l-4.17620796 4.1762079c-.39120769.3912077-1.02508144.3916069-1.41671995-.0000316l-4.1763942-4.1763942c-.39122514-.3912251-.39767006-1.0190815-.01657798-1.4001736.39113012-.3911301 1.02337106-.3930364 1.41951349.0031061l2.48183446 2.4818344v-8.7999954c0-.54911294.4426881-.99439484.99778758-.99557515l8.00221246-.00442485c.5522847 0 1 .44771525 1 1s-.4477153 1-1 1z" fill-rule="evenodd" transform="matrix(-1 0 0 -1 20.182742 24.805206)"/></symbol><symbol id="icon-calendar" viewBox="0 0 18 18"><path d="m12.5 0c.2761424 0 .5.21505737.5.49047852v.50952148h2c1.1072288 0 2 .89451376 2 2v12c0 1.1072288-.8945138 2-2 2h-12c-1.1072288 0-2-.8945138-2-2v-12c0-1.1072288.89451376-2 2-2h1v1h-1c-.55393837 0-1 .44579254-1 1v3h14v-3c0-.55393837-.4457925-1-1-1h-2v1.50952148c0 .27088381-.2319336.49047852-.5.49047852-.2761424 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.2319336-.49047852.5-.49047852zm3.5 7h-14v8c0 .5539384.44579254 1 1 1h12c.5539384 0 1-.4457925 1-1zm-11 6v1h-1v-1zm3 0v1h-1v-1zm3 0v1h-1v-1zm-6-2v1h-1v-1zm3 0v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-3-2v1h-1v-1zm6 0v1h-1v-1zm-3 0v1h-1v-1zm-5.5-9c.27614237 0 .5.21505737.5.49047852v.50952148h5v1h-5v1.50952148c0 .27088381-.23193359.49047852-.5.49047852-.27614237 0-.5-.21505737-.5-.49047852v-3.01904296c0-.27088381.23193359-.49047852.5-.49047852z" fill-rule="evenodd"/></symbol><symbol id="icon-cart" viewBox="0 0 18 18"><path d="m5 14c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm10 0c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm-10 1c-.55228475 0-1 .4477153-1 1s.44771525 1 1 1 1-.4477153 1-1-.44771525-1-1-1zm10 0c-.5522847 0-1 .4477153-1 1s.4477153 1 1 1 1-.4477153 1-1-.4477153-1-1-1zm-12.82032249-15c.47691417 0 .88746157.33678127.98070211.80449199l.23823144 1.19501025 13.36277974.00045554c.5522847.00001882.9999659.44774934.9999659 1.00004222 0 .07084994-.0075361.14150708-.022474.2107727l-1.2908094 5.98534344c-.1007861.46742419-.5432548.80388386-1.0571651.80388386h-10.24805106c-.59173366 0-1.07142857.4477153-1.07142857 1 0 .5128358.41361449.9355072.94647737.9932723l.1249512.0067277h10.35933776c.2749512 0 .4979349.2228539.4979349.4978051 0 .2749417-.2227336.4978951-.4976753.4980063l-10.35959736.0041886c-1.18346732 0-2.14285714-.8954305-2.14285714-2 0-.6625717.34520317-1.24989198.87690425-1.61383592l-1.63768102-8.19004794c-.01312273-.06561364-.01950005-.131011-.0196107-.19547395l-1.71961253-.00064219c-.27614237 0-.5-.22385762-.5-.5 0-.27614237.22385763-.5.5-.5zm14.53193359 2.99950224h-13.11300004l1.20580469 6.02530174c.11024034-.0163252.22327998-.02480398.33844139-.02480398h10.27064786z"/></symbol><symbol id="icon-chevron-less" viewBox="0 0 10 10"><path d="m5.58578644 4-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 -1 -1 0 9 9)"/></symbol><symbol id="icon-chevron-more" viewBox="0 0 10 10"><path d="m5.58578644 6-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4.00000002c-.39052429.3905243-1.02368927.3905243-1.41421356 0s-.39052429-1.02368929 0-1.41421358z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-chevron-right" viewBox="0 0 10 10"><path d="m5.96738168 4.70639573 2.39518594-2.41447274c.37913917-.38219212.98637524-.38972225 1.35419292-.01894278.37750606.38054586.37784436.99719163-.00013556 1.37821513l-4.03074001 4.06319683c-.37758093.38062133-.98937525.38100976-1.367372-.00003075l-4.03091981-4.06337806c-.37759778-.38063832-.38381821-.99150444-.01600053-1.3622839.37750607-.38054587.98772445-.38240057 1.37006824.00302197l2.39538588 2.4146743.96295325.98624457z" fill-rule="evenodd" transform="matrix(0 -1 1 0 0 10)"/></symbol><symbol id="icon-circle-fill" viewBox="0 0 16 16"><path d="m8 14c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-circle" viewBox="0 0 16 16"><path d="m8 12c2.209139 0 4-1.790861 4-4s-1.790861-4-4-4-4 1.790861-4 4 1.790861 4 4 4zm0 2c-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6 6 2.6862915 6 6-2.6862915 6-6 6z" fill-rule="evenodd"/></symbol><symbol id="icon-citation" viewBox="0 0 18 18"><path d="m8.63593473 5.99995183c2.20913897 0 3.99999997 1.79084375 3.99999997 3.99996146 0 1.40730761-.7267788 2.64486871-1.8254829 3.35783281 1.6240224.6764218 2.8754442 2.0093871 3.4610603 3.6412466l-1.0763845.000006c-.5310008-1.2078237-1.5108121-2.1940153-2.7691712-2.7181346l-.79002167-.329052v-1.023992l.63016577-.4089232c.8482885-.5504661 1.3698342-1.4895187 1.3698342-2.51898361 0-1.65683828-1.3431457-2.99996146-2.99999997-2.99996146-1.65685425 0-3 1.34312318-3 2.99996146 0 1.02946491.52154569 1.96851751 1.36983419 2.51898361l.63016581.4089232v1.023992l-.79002171.329052c-1.25835905.5241193-2.23817037 1.5103109-2.76917113 2.7181346l-1.07638453-.000006c.58561612-1.6318595 1.8370379-2.9648248 3.46106024-3.6412466-1.09870405-.7129641-1.82548287-1.9505252-1.82548287-3.35783281 0-2.20911771 1.790861-3.99996146 4-3.99996146zm7.36897597-4.99995183c1.1018574 0 1.9950893.89353404 1.9950893 2.00274083v5.994422c0 1.10608317-.8926228 2.00274087-1.9950893 2.00274087l-3.0049107-.0009037v-1l3.0049107.00091329c.5490631 0 .9950893-.44783123.9950893-1.00275046v-5.994422c0-.55646537-.4450595-1.00275046-.9950893-1.00275046h-14.00982141c-.54906309 0-.99508929.44783123-.99508929 1.00275046v5.9971821c0 .66666024.33333333.99999036 1 .99999036l2-.00091329v1l-2 .0009037c-1 0-2-.99999041-2-1.99998077v-5.9971821c0-1.10608322.8926228-2.00274083 1.99508929-2.00274083zm-8.5049107 2.9999711c.27614237 0 .5.22385547.5.5 0 .2761349-.22385763.5-.5.5h-4c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm3 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-1c-.27614237 0-.5-.2238651-.5-.5 0-.27614453.22385763-.5.5-.5zm4 0c.2761424 0 .5.22385547.5.5 0 .2761349-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238651-.5-.5 0-.27614453.2238576-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-close" viewBox="0 0 16 16"><path d="m2.29679575 12.2772478c-.39658757.3965876-.39438847 1.0328109-.00062148 1.4265779.39651227.3965123 1.03246768.3934888 1.42657791-.0006214l4.27724782-4.27724787 4.2772478 4.27724787c.3965876.3965875 1.0328109.3943884 1.4265779.0006214.3965123-.3965122.3934888-1.0324677-.0006214-1.4265779l-4.27724787-4.2772478 4.27724787-4.27724782c.3965875-.39658757.3943884-1.03281091.0006214-1.42657791-.3965122-.39651226-1.0324677-.39348875-1.4265779.00062148l-4.2772478 4.27724782-4.27724782-4.27724782c-.39658757-.39658757-1.03281091-.39438847-1.42657791-.00062148-.39651226.39651227-.39348875 1.03246768.00062148 1.42657791l4.27724782 4.27724782z" fill-rule="evenodd"/></symbol><symbol id="icon-collections" viewBox="0 0 18 18"><path d="m15 4c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2h1c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227l-.1166211-.00672773h-1v-1zm-4-3c1.1045695 0 2 .8954305 2 2v9c0 1.1045695-.8954305 2-2 2h-8c-1.1045695 0-2-.8954305-2-2v-9c0-1.1045695.8954305-2 2-2zm0 1h-8c-.51283584 0-.93550716.38604019-.99327227.88337887l-.00672773.11662113v9c0 .5128358.38604019.9355072.88337887.9932723l.11662113.0067277h8c.5128358 0 .9355072-.3860402.9932723-.8833789l.0067277-.1166211v-9c0-.51283584-.3860402-.93550716-.8833789-.99327227zm-1.5 7c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm0-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-5c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-compare" viewBox="0 0 18 18"><path d="m12 3c3.3137085 0 6 2.6862915 6 6s-2.6862915 6-6 6c-1.0928452 0-2.11744941-.2921742-2.99996061-.8026704-.88181407.5102749-1.90678042.8026704-3.00003939.8026704-3.3137085 0-6-2.6862915-6-6s2.6862915-6 6-6c1.09325897 0 2.11822532.29239547 3.00096303.80325037.88158756-.51107621 1.90619177-.80325037 2.99903697-.80325037zm-6 1c-2.76142375 0-5 2.23857625-5 5 0 2.7614237 2.23857625 5 5 5 .74397391 0 1.44999672-.162488 2.08451611-.4539116-1.27652344-1.1000812-2.08451611-2.7287264-2.08451611-4.5460884s.80799267-3.44600721 2.08434391-4.5463015c-.63434719-.29121054-1.34037-.4536985-2.08434391-.4536985zm6 0c-.7439739 0-1.4499967.16248796-2.08451611.45391156 1.27652341 1.10008123 2.08451611 2.72872644 2.08451611 4.54608844s-.8079927 3.4460072-2.08434391 4.5463015c.63434721.2912105 1.34037001.4536985 2.08434391.4536985 2.7614237 0 5-2.2385763 5-5 0-2.76142375-2.2385763-5-5-5zm-1.4162763 7.0005324h-3.16744736c.15614659.3572676.35283837.6927622.58425872 1.0006671h1.99892988c.23142036-.3079049.42811216-.6433995.58425876-1.0006671zm.4162763-2.0005324h-4c0 .34288501.0345146.67770871.10025909 1.0011864h3.79948181c.0657445-.32347769.1002591-.65830139.1002591-1.0011864zm-.4158423-1.99953894h-3.16831543c-.13859957.31730812-.24521946.651783-.31578599.99935097h3.79988742c-.0705665-.34756797-.1771864-.68204285-.315786-.99935097zm-1.58295822-1.999926-.08316107.06199199c-.34550042.27081213-.65446126.58611297-.91825862.93727862h2.00044041c-.28418626-.37830727-.6207872-.71499149-.99902072-.99927061z" fill-rule="evenodd"/></symbol><symbol id="icon-download-file" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.5046024 4c.27614237 0 .5.21637201.5.49209595v6.14827645l1.7462789-1.77990922c.1933927-.1971171.5125222-.19455839.7001689-.0069117.1932998.19329992.1910058.50899492-.0027774.70277812l-2.59089271 2.5908927c-.19483374.1948337-.51177825.1937771-.70556873-.0000133l-2.59099079-2.5909908c-.19484111-.1948411-.19043735-.5151448-.00279066-.70279146.19329987-.19329987.50465175-.19237083.70018565.00692852l1.74638684 1.78001764v-6.14827695c0-.27177709.23193359-.49209595.5-.49209595z" fill-rule="evenodd"/></symbol><symbol id="icon-download" viewBox="0 0 16 16"><path d="m12.9975267 12.999368c.5467123 0 1.0024733.4478567 1.0024733 1.000316 0 .5563109-.4488226 1.000316-1.0024733 1.000316h-9.99505341c-.54671233 0-1.00247329-.4478567-1.00247329-1.000316 0-.5563109.44882258-1.000316 1.00247329-1.000316zm-4.9975267-11.999368c.55228475 0 1 .44497754 1 .99589209v6.80214418l2.4816273-2.48241149c.3928222-.39294628 1.0219732-.4006883 1.4030652-.01947579.3911302.39125371.3914806 1.02525073-.0001404 1.41699553l-4.17620792 4.17752758c-.39120769.3913313-1.02508144.3917306-1.41671995-.0000316l-4.17639421-4.17771394c-.39122513-.39134876-.39767006-1.01940351-.01657797-1.40061601.39113012-.39125372 1.02337105-.3931606 1.41951349.00310701l2.48183446 2.48261871v-6.80214418c0-.55001601.44386482-.99589209 1-.99589209z" fill-rule="evenodd"/></symbol><symbol id="icon-editors" viewBox="0 0 18 18"><path d="m8.72592184 2.54588137c-.48811714-.34391207-1.08343326-.54588137-1.72592184-.54588137-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400182l-.79002171.32905522c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274v.9009805h-1v-.9009805c0-2.5479714 1.54557359-4.79153984 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4 1.09079823 0 2.07961816.43662103 2.80122451 1.1446278-.37707584.09278571-.7373238.22835063-1.07530267.40125357zm-2.72592184 14.45411863h-1v-.9009805c0-2.5479714 1.54557359-4.7915398 3.82548288-5.7411543-1.09870406-.71297106-1.82548288-1.95054399-1.82548288-3.3578652 0-2.209139 1.790861-4 4-4s4 1.790861 4 4c0 1.40732121-.7267788 2.64489414-1.8254829 3.3578652 2.2799093.9496145 3.8254829 3.1931829 3.8254829 5.7411543v.9009805h-1v-.9009805c0-2.1155483-1.2760206-4.0125067-3.2099783-4.8180274l-.7900217-.3290552v-1.02400184l.6301658-.40892721c.8482885-.55047139 1.3698342-1.489533 1.3698342-2.51900785 0-1.65685425-1.3431458-3-3-3-1.65685425 0-3 1.34314575-3 3 0 1.02947485.5215457 1.96853646 1.3698342 2.51900785l.6301658.40892721v1.02400184l-.79002171.3290552c-1.93395773.8055207-3.20997829 2.7024791-3.20997829 4.8180274z" fill-rule="evenodd"/></symbol><symbol id="icon-email" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-.0049107 2.55749512v1.44250488l-7 4-7-4v-1.44250488l7 4z" fill-rule="evenodd"/></symbol><symbol id="icon-error" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm2.8630343 4.71100931-2.8630343 2.86303426-2.86303426-2.86303426c-.39658757-.39658757-1.03281091-.39438847-1.4265779-.00062147-.39651227.39651226-.39348876 1.03246767.00062147 1.4265779l2.86303426 2.86303426-2.86303426 2.8630343c-.39658757.3965875-.39438847 1.0328109-.00062147 1.4265779.39651226.3965122 1.03246767.3934887 1.4265779-.0006215l2.86303426-2.8630343 2.8630343 2.8630343c.3965875.3965876 1.0328109.3943885 1.4265779.0006215.3965122-.3965123.3934887-1.0324677-.0006215-1.4265779l-2.8630343-2.8630343 2.8630343-2.86303426c.3965876-.39658757.3943885-1.03281091.0006215-1.4265779-.3965123-.39651227-1.0324677-.39348876-1.4265779.00062147z" fill-rule="evenodd"/></symbol><symbol id="icon-ethics" viewBox="0 0 18 18"><path d="m6.76384967 1.41421356.83301651-.8330165c.77492941-.77492941 2.03133823-.77492941 2.80626762 0l.8330165.8330165c.3750728.37507276.8837806.58578644 1.4142136.58578644h1.3496361c1.1045695 0 2 .8954305 2 2v1.34963611c0 .53043298.2107137 1.03914081.5857864 1.41421356l.8330165.83301651c.7749295.77492941.7749295 2.03133823 0 2.80626762l-.8330165.8330165c-.3750727.3750728-.5857864.8837806-.5857864 1.4142136v1.3496361c0 1.1045695-.8954305 2-2 2h-1.3496361c-.530433 0-1.0391408.2107137-1.4142136.5857864l-.8330165.8330165c-.77492939.7749295-2.03133821.7749295-2.80626762 0l-.83301651-.8330165c-.37507275-.3750727-.88378058-.5857864-1.41421356-.5857864h-1.34963611c-1.1045695 0-2-.8954305-2-2v-1.3496361c0-.530433-.21071368-1.0391408-.58578644-1.4142136l-.8330165-.8330165c-.77492941-.77492939-.77492941-2.03133821 0-2.80626762l.8330165-.83301651c.37507276-.37507275.58578644-.88378058.58578644-1.41421356v-1.34963611c0-1.1045695.8954305-2 2-2h1.34963611c.53043298 0 1.03914081-.21071368 1.41421356-.58578644zm-1.41421356 1.58578644h-1.34963611c-.55228475 0-1 .44771525-1 1v1.34963611c0 .79564947-.31607052 1.55871121-.87867966 2.12132034l-.8330165.83301651c-.38440512.38440512-.38440512 1.00764896 0 1.39205408l.8330165.83301646c.56260914.5626092.87867966 1.3256709.87867966 2.1213204v1.3496361c0 .5522847.44771525 1 1 1h1.34963611c.79564947 0 1.55871121.3160705 2.12132034.8786797l.83301651.8330165c.38440512.3844051 1.00764896.3844051 1.39205408 0l.83301646-.8330165c.5626092-.5626092 1.3256709-.8786797 2.1213204-.8786797h1.3496361c.5522847 0 1-.4477153 1-1v-1.3496361c0-.7956495.3160705-1.5587112.8786797-2.1213204l.8330165-.83301646c.3844051-.38440512.3844051-1.00764896 0-1.39205408l-.8330165-.83301651c-.5626092-.56260913-.8786797-1.32567087-.8786797-2.12132034v-1.34963611c0-.55228475-.4477153-1-1-1h-1.3496361c-.7956495 0-1.5587112-.31607052-2.1213204-.87867966l-.83301646-.8330165c-.38440512-.38440512-1.00764896-.38440512-1.39205408 0l-.83301651.8330165c-.56260913.56260914-1.32567087.87867966-2.12132034.87867966zm3.58698944 11.4960218c-.02081224.002155-.04199226.0030286-.06345763.002542-.98766446-.0223875-1.93408568-.3063547-2.75885125-.8155622-.23496767-.1450683-.30784554-.4531483-.16277726-.688116.14506827-.2349677.45314827-.3078455.68811595-.1627773.67447084.4164161 1.44758575.6483839 2.25617384.6667123.01759529.0003988.03495764.0017019.05204365.0038639.01713363-.0017748.03452416-.0026845.05212715-.0026845 2.4852814 0 4.5-2.0147186 4.5-4.5 0-1.04888973-.3593547-2.04134635-1.0074477-2.83787157-.1742817-.21419731-.1419238-.5291218.0722736-.70340353.2141973-.17428173.5291218-.14192375.7034035.07227357.7919032.97327203 1.2317706 2.18808682 1.2317706 3.46900153 0 3.0375661-2.4624339 5.5-5.5 5.5-.02146768 0-.04261937-.0013529-.06337445-.0039782zm1.57975095-10.78419583c.2654788.07599731.419084.35281842.3430867.61829728-.0759973.26547885-.3528185.419084-.6182973.3430867-.37560116-.10752146-.76586237-.16587951-1.15568824-.17249193-2.5587807-.00064534-4.58547766 2.00216524-4.58547766 4.49928198 0 .62691557.12797645 1.23496.37274865 1.7964426.11035133.2531347-.0053975.5477984-.25853224.6581497-.25313473.1103514-.54779841-.0053975-.65814974-.2585322-.29947131-.6869568-.45606667-1.43097603-.45606667-2.1960601 0-3.05211432 2.47714695-5.50006595 5.59399617-5.49921198.48576182.00815502.96289603.0795037 1.42238033.21103795zm-1.9766658 6.41091303 2.69835-2.94655317c.1788432-.21040373.4943901-.23598862.7047939-.05714545.2104037.17884318.2359886.49439014.0571454.70479387l-3.01637681 3.34277395c-.18039088.1999106-.48669547.2210637-.69285412.0478478l-1.93095347-1.62240047c-.21213845-.17678204-.24080048-.49206439-.06401844-.70420284.17678204-.21213844.49206439-.24080048.70420284-.06401844z" fill-rule="evenodd"/></symbol><symbol id="icon-expand"><path d="M7.498 11.918a.997.997 0 0 0-.003-1.411.995.995 0 0 0-1.412-.003l-4.102 4.102v-3.51A1 1 0 0 0 .98 10.09.992.992 0 0 0 0 11.092V17c0 .554.448 1.002 1.002 1.002h5.907c.554 0 1.002-.45 1.002-1.003 0-.539-.45-.978-1.006-.978h-3.51zm3.005-5.835a.997.997 0 0 0 .003 1.412.995.995 0 0 0 1.411.003l4.103-4.103v3.51a1 1 0 0 0 1.001 1.006A.992.992 0 0 0 18 6.91V1.002A1 1 0 0 0 17 0h-5.907a1.003 1.003 0 0 0-1.002 1.003c0 .539.45.978 1.006.978h3.51z" fill-rule="evenodd"/></symbol><symbol id="icon-explore" viewBox="0 0 18 18"><path d="m9 17c4.418278 0 8-3.581722 8-8s-3.581722-8-8-8-8 3.581722-8 8 3.581722 8 8 8zm0 1c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9zm0-2.5c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5c2.969509 0 5.400504-2.3575119 5.497023-5.31714844.0090007-.27599565.2400359-.49243782.5160315-.48343711.2759957.0090007.4924378.2400359.4834371.51603155-.114093 3.4985237-2.9869632 6.284554-6.4964916 6.284554zm-.29090657-12.99359748c.27587424-.01216621.50937715.20161139.52154336.47748563.01216621.27587423-.20161139.50937715-.47748563.52154336-2.93195733.12930094-5.25315116 2.54886451-5.25315116 5.49456849 0 .27614237-.22385763.5-.5.5s-.5-.22385763-.5-.5c0-3.48142406 2.74307146-6.34074398 6.20909343-6.49359748zm1.13784138 8.04763908-1.2004882-1.20048821c-.19526215-.19526215-.19526215-.51184463 0-.70710678s.51184463-.19526215.70710678 0l1.20048821 1.2004882 1.6006509-4.00162734-4.50670359 1.80268144-1.80268144 4.50670359zm4.10281269-6.50378907-2.6692597 6.67314927c-.1016411.2541026-.3029834.4554449-.557086.557086l-6.67314927 2.6692597 2.66925969-6.67314926c.10164107-.25410266.30298336-.45544495.55708602-.55708602z" fill-rule="evenodd"/></symbol><symbol id="icon-filter" viewBox="0 0 16 16"><path d="m14.9738641 0c.5667192 0 1.0261359.4477136 1.0261359 1 0 .24221858-.0902161.47620768-.2538899.65849851l-5.6938314 6.34147206v5.49997973c0 .3147562-.1520673.6111434-.4104543.7999971l-2.05227171 1.4999945c-.45337535.3313696-1.09655869.2418269-1.4365902-.1999993-.13321514-.1730955-.20522717-.3836284-.20522717-.5999978v-6.99997423l-5.69383133-6.34147206c-.3731872-.41563511-.32996891-1.0473954.09653074-1.41107611.18705584-.15950448.42716133-.2474224.67571519-.2474224zm-5.9218641 8.5h-2.105v6.491l.01238459.0070843.02053271.0015705.01955278-.0070558 2.0532976-1.4990996zm-8.02585008-7.5-.01564945.00240169 5.83249953 6.49759831h2.313l5.836-6.499z"/></symbol><symbol id="icon-home" viewBox="0 0 18 18"><path d="m9 5-6 6v5h4v-4h4v4h4v-5zm7 6.5857864v4.4142136c0 .5522847-.4477153 1-1 1h-5v-4h-2v4h-5c-.55228475 0-1-.4477153-1-1v-4.4142136c-.25592232 0-.51184464-.097631-.70710678-.2928932l-.58578644-.5857864c-.39052429-.3905243-.39052429-1.02368929 0-1.41421358l8.29289322-8.29289322 8.2928932 8.29289322c.3905243.39052429.3905243 1.02368928 0 1.41421358l-.5857864.5857864c-.1952622.1952622-.4511845.2928932-.7071068.2928932zm-7-9.17157284-7.58578644 7.58578644.58578644.5857864 7-6.99999996 7 6.99999996.5857864-.5857864z" fill-rule="evenodd"/></symbol><symbol id="icon-image" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm-3.49645283 10.1752453-3.89407257 6.7495552c.11705545.048464.24538859.0751995.37998328.0751995h10.60290092l-2.4329715-4.2154691-1.57494129 2.7288098zm8.49779013 6.8247547c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v13.98991071l4.50814957-7.81026689 3.08089884 5.33809539 1.57494129-2.7288097 3.5875735 6.2159812zm-3.0059397-11c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2-2-.8954305-2-2 .8954305-2 2-2zm0 1c-.5522847 0-1 .44771525-1 1s.4477153 1 1 1 1-.44771525 1-1-.4477153-1-1-1z" fill-rule="evenodd"/></symbol><symbol id="icon-info" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm0 7h-1.5l-.11662113.00672773c-.49733868.05776511-.88337887.48043643-.88337887.99327227 0 .47338693.32893365.86994729.77070917.97358929l.1126697.01968298.11662113.00672773h.5v3h-.5l-.11662113.0067277c-.42082504.0488782-.76196299.3590206-.85696816.7639815l-.01968298.1126697-.00672773.1166211.00672773.1166211c.04887817.4208251.35902055.761963.76398144.8569682l.1126697.019683.11662113.0067277h3l.1166211-.0067277c.4973387-.0577651.8833789-.4804365.8833789-.9932723 0-.4733869-.3289337-.8699473-.7707092-.9735893l-.1126697-.019683-.1166211-.0067277h-.5v-4l-.00672773-.11662113c-.04887817-.42082504-.35902055-.76196299-.76398144-.85696816l-.1126697-.01968298zm0-3.25c-.69035594 0-1.25.55964406-1.25 1.25s.55964406 1.25 1.25 1.25 1.25-.55964406 1.25-1.25-.55964406-1.25-1.25-1.25z" fill-rule="evenodd"/></symbol><symbol id="icon-institution" viewBox="0 0 18 18"><path d="m7 16.9998189v-2.0003623h4v2.0003623h2v-3.0005434h-8v3.0005434zm-3-10.00181122h-1.52632364c-.27614237 0-.5-.22389817-.5-.50009056 0-.13995446.05863589-.27350497.16166338-.36820841l1.23156713-1.13206327h-2.36690687v12.00217346h3v-2.0003623h-3v-1.0001811h3v-1.0001811h1v-4.00072448h-1zm10 0v2.00036224h-1v4.00072448h1v1.0001811h3v1.0001811h-3v2.0003623h3v-12.00217346h-2.3695309l1.2315671 1.13206327c.2033191.186892.2166633.50325042.0298051.70660631-.0946863.10304615-.2282126.16169266-.3681417.16169266zm3-3.00054336c.5522847 0 1 .44779634 1 1.00018112v13.00235456h-18v-13.00235456c0-.55238478.44771525-1.00018112 1-1.00018112h3.45499992l4.20535144-3.86558216c.19129876-.17584288.48537447-.17584288.67667324 0l4.2053514 3.86558216zm-4 3.00054336h-8v1.00018112h8zm-2 6.00108672h1v-4.00072448h-1zm-1 0v-4.00072448h-2v4.00072448zm-3 0v-4.00072448h-1v4.00072448zm8-4.00072448c.5522847 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.4477153-1.00018112 1-1.00018112zm-12 0c.55228475 0 1 .44779634 1 1.00018112v2.00036226h-2v-2.00036226c0-.55238478.44771525-1.00018112 1-1.00018112zm5.99868798-7.81907007-5.24205601 4.81852671h10.48411203zm.00131202 3.81834559c-.55228475 0-1-.44779634-1-1.00018112s.44771525-1.00018112 1-1.00018112 1 .44779634 1 1.00018112-.44771525 1.00018112-1 1.00018112zm-1 11.00199236v1.0001811h2v-1.0001811z" fill-rule="evenodd"/></symbol><symbol id="icon-location" viewBox="0 0 18 18"><path d="m9.39521328 16.2688008c.79596342-.7770119 1.59208152-1.6299956 2.33285652-2.5295081 1.4020032-1.7024324 2.4323601-3.3624519 2.9354918-4.871847.2228715-.66861448.3364384-1.29323246.3364384-1.8674457 0-3.3137085-2.6862915-6-6-6-3.36356866 0-6 2.60156856-6 6 0 .57421324.11356691 1.19883122.3364384 1.8674457.50313169 1.5093951 1.53348863 3.1694146 2.93549184 4.871847.74077492.8995125 1.53689309 1.7524962 2.33285648 2.5295081.13694479.1336842.26895677.2602648.39521328.3793207.12625651-.1190559.25826849-.2456365.39521328-.3793207zm-.39521328 1.7311992s-7-6-7-11c0-4 3.13400675-7 7-7 3.8659932 0 7 3.13400675 7 7 0 5-7 11-7 11zm0-8c-1.65685425 0-3-1.34314575-3-3s1.34314575-3 3-3c1.6568542 0 3 1.34314575 3 3s-1.3431458 3-3 3zm0-1c1.1045695 0 2-.8954305 2-2s-.8954305-2-2-2-2 .8954305-2 2 .8954305 2 2 2z" fill-rule="evenodd"/></symbol><symbol id="icon-minus" viewBox="0 0 16 16"><path d="m2.00087166 7h11.99825664c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-11.99825664c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-newsletter" viewBox="0 0 18 18"><path d="m9 11.8482489 2-1.1428571v-1.7053918h-4v1.7053918zm-3-1.7142857v-2.1339632h6v2.1339632l3-1.71428574v-6.41967746h-12v6.41967746zm10-5.3839632 1.5299989.95624934c.2923814.18273835.4700011.50320827.4700011.8479983v8.44575236c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-8.44575236c0-.34479003.1776197-.66525995.47000106-.8479983l1.52999894-.95624934v-2.75c0-.55228475.44771525-1 1-1h12c.5522847 0 1 .44771525 1 1zm0 1.17924764v3.07075236l-7 4-7-4v-3.07075236l-1 .625v8.44575236c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-8.44575236zm-10-1.92924764h6v1h-6zm-1 2h8v1h-8z" fill-rule="evenodd"/></symbol><symbol id="icon-orcid" viewBox="0 0 18 18"><path d="m9 1c4.418278 0 8 3.581722 8 8s-3.581722 8-8 8-8-3.581722-8-8 3.581722-8 8-8zm-2.90107518 5.2732337h-1.41865256v7.1712107h1.41865256zm4.55867178.02508949h-2.99247027v7.14612121h2.91062487c.7673039 0 1.4476365-.1483432 2.0410182-.445034s1.0511995-.7152915 1.3734671-1.2558144c.3222677-.540523.4833991-1.1603247.4833991-1.85942385 0-.68545815-.1602789-1.30270225-.4808414-1.85175082-.3205625-.54904856-.7707074-.97532211-1.3504481-1.27883343-.5797408-.30351132-1.2413173-.45526471-1.9847495-.45526471zm-.1892674 1.07933542c.7877654 0 1.4143875.22336734 1.8798852.67010873.4654977.44674138.698243 1.05546001.698243 1.82617415 0 .74343221-.2310402 1.34447791-.6931277 1.80315511-.4620874.4586773-1.0750688.6880124-1.8389625.6880124h-1.46810075v-4.98745039zm-5.08652545-3.71099194c-.21825533 0-.410525.08444276-.57681478.25333081-.16628977.16888806-.24943341.36245684-.24943341.58071218 0 .22345188.08314364.41961891.24943341.58850696.16628978.16888806.35855945.25333082.57681478.25333082.233845 0 .43390938-.08314364.60019916-.24943342.16628978-.16628977.24943342-.36375592.24943342-.59240436 0-.233845-.08314364-.43131115-.24943342-.59240437s-.36635416-.24163862-.60019916-.24163862z" fill-rule="evenodd"/></symbol><symbol id="icon-plus" viewBox="0 0 16 16"><path d="m2.00087166 7h4.99912834v-4.99912834c0-.55276616.44386482-1.00087166 1-1.00087166.55228475 0 1 .44463086 1 1.00087166v4.99912834h4.9991283c.5527662 0 1.0008717.44386482 1.0008717 1 0 .55228475-.4446309 1-1.0008717 1h-4.9991283v4.9991283c0 .5527662-.44386482 1.0008717-1 1.0008717-.55228475 0-1-.4446309-1-1.0008717v-4.9991283h-4.99912834c-.55276616 0-1.00087166-.44386482-1.00087166-1 0-.55228475.44463086-1 1.00087166-1z" fill-rule="evenodd"/></symbol><symbol id="icon-print" viewBox="0 0 18 18"><path d="m16.0049107 5h-14.00982141c-.54941618 0-.99508929.4467783-.99508929.99961498v6.00077002c0 .5570958.44271433.999615.99508929.999615h1.00491071v-3h12v3h1.0049107c.5494162 0 .9950893-.4467783.9950893-.999615v-6.00077002c0-.55709576-.4427143-.99961498-.9950893-.99961498zm-2.0049107-1v-2.00208688c0-.54777062-.4519464-.99791312-1.0085302-.99791312h-7.9829396c-.55661731 0-1.0085302.44910695-1.0085302.99791312v2.00208688zm1 10v2.0018986c0 1.103521-.9019504 1.9981014-2.0085302 1.9981014h-7.9829396c-1.1092806 0-2.0085302-.8867064-2.0085302-1.9981014v-2.0018986h-1.00491071c-1.10185739 0-1.99508929-.8874333-1.99508929-1.999615v-6.00077002c0-1.10435686.8926228-1.99961498 1.99508929-1.99961498h1.00491071v-2.00208688c0-1.10341695.90195036-1.99791312 2.0085302-1.99791312h7.9829396c1.1092806 0 2.0085302.89826062 2.0085302 1.99791312v2.00208688h1.0049107c1.1018574 0 1.9950893.88743329 1.9950893 1.99961498v6.00077002c0 1.1043569-.8926228 1.999615-1.9950893 1.999615zm-1-3h-10v5.0018986c0 .5546075.44702548.9981014 1.0085302.9981014h7.9829396c.5565964 0 1.0085302-.4491701 1.0085302-.9981014zm-9 1h8v1h-8zm0 2h5v1h-5zm9-5c-.5522847 0-1-.44771525-1-1s.4477153-1 1-1 1 .44771525 1 1-.4477153 1-1 1z" fill-rule="evenodd"/></symbol><symbol id="icon-search" viewBox="0 0 22 22"><path d="M21.697 20.261a1.028 1.028 0 01.01 1.448 1.034 1.034 0 01-1.448-.01l-4.267-4.267A9.812 9.811 0 010 9.812a9.812 9.811 0 1117.43 6.182zM9.812 18.222A8.41 8.41 0 109.81 1.403a8.41 8.41 0 000 16.82z" fill-rule="evenodd"/></symbol><symbol id="icon-social-facebook" viewBox="0 0 24 24"><path d="m6.00368507 20c-1.10660471 0-2.00368507-.8945138-2.00368507-1.9940603v-12.01187942c0-1.10128908.89451376-1.99406028 1.99406028-1.99406028h12.01187942c1.1012891 0 1.9940603.89451376 1.9940603 1.99406028v12.01187942c0 1.1012891-.88679 1.9940603-2.0032184 1.9940603h-2.9570132v-6.1960818h2.0797387l.3114113-2.414723h-2.39115v-1.54164807c0-.69911803.1941355-1.1755439 1.1966615-1.1755439l1.2786739-.00055875v-2.15974763l-.2339477-.02492088c-.3441234-.03134957-.9500153-.07025255-1.6293054-.07025255-1.8435726 0-3.1057323 1.12531866-3.1057323 3.19187953v1.78079225h-2.0850778v2.414723h2.0850778v6.1960818z" fill-rule="evenodd"/></symbol><symbol id="icon-social-twitter" viewBox="0 0 24 24"><path d="m18.8767135 6.87445248c.7638174-.46908424 1.351611-1.21167363 1.6250764-2.09636345-.7135248.43394112-1.50406.74870123-2.3464594.91677702-.6695189-.73342162-1.6297913-1.19486605-2.6922204-1.19486605-2.0399895 0-3.6933555 1.69603749-3.6933555 3.78628909 0 .29642457.0314329.58673729.0942985.8617704-3.06469922-.15890802-5.78835241-1.66547825-7.60988389-3.9574208-.3174714.56076194-.49978171 1.21167363-.49978171 1.90536824 0 1.31404706.65223085 2.47224203 1.64236444 3.15218497-.60350999-.0198635-1.17401554-.1925232-1.67222562-.47366811v.04583885c0 1.83355406 1.27302891 3.36609966 2.96411421 3.71294696-.31118484.0886217-.63651445.1329326-.97441718.1329326-.2357461 0-.47149219-.0229194-.69466516-.0672303.47149219 1.5065703 1.83253297 2.6036468 3.44975116 2.632678-1.2651707 1.0160946-2.85724264 1.6196394-4.5891906 1.6196394-.29861172 0-.59093688-.0152796-.88011875-.0504227 1.63450624 1.0726291 3.57548241 1.6990934 5.66104951 1.6990934 6.79263079 0 10.50641749-5.7711113 10.50641749-10.7751859l-.0094298-.48894775c.7229547-.53478659 1.3516109-1.20250585 1.8419628-1.96190282-.6632323.30100846-1.3751855.50422736-2.1217148.59590507z" fill-rule="evenodd"/></symbol><symbol id="icon-social-youtube" viewBox="0 0 24 24"><path d="m10.1415 14.3973208-.0005625-5.19318431 4.863375 2.60554491zm9.963-7.92753362c-.6845625-.73643756-1.4518125-.73990314-1.803375-.7826454-2.518875-.18714178-6.2971875-.18714178-6.2971875-.18714178-.007875 0-3.7861875 0-6.3050625.18714178-.352125.04274226-1.1188125.04620784-1.8039375.7826454-.5394375.56084773-.7149375 1.8344515-.7149375 1.8344515s-.18 1.49597903-.18 2.99138042v1.4024082c0 1.495979.18 2.9913804.18 2.9913804s.1755 1.2736038.7149375 1.8344515c.685125.7364376 1.5845625.7133337 1.9850625.7901542 1.44.1420891 6.12.1859866 6.12.1859866s3.78225-.005776 6.301125-.1929178c.3515625-.0433198 1.1188125-.0467854 1.803375-.783223.5394375-.5608477.7155-1.8344515.7155-1.8344515s.18-1.4954014.18-2.9913804v-1.4024082c0-1.49540139-.18-2.99138042-.18-2.99138042s-.1760625-1.27360377-.7155-1.8344515z" fill-rule="evenodd"/></symbol><symbol id="icon-subject-medicine" viewBox="0 0 18 18"><path d="m12.5 8h-6.5c-1.65685425 0-3 1.34314575-3 3v1c0 1.6568542 1.34314575 3 3 3h1v-2h-.5c-.82842712 0-1.5-.6715729-1.5-1.5s.67157288-1.5 1.5-1.5h1.5 2 1 2c1.6568542 0 3-1.34314575 3-3v-1c0-1.65685425-1.3431458-3-3-3h-2v2h1.5c.8284271 0 1.5.67157288 1.5 1.5s-.6715729 1.5-1.5 1.5zm-5.5-1v-1h-3.5c-1.38071187 0-2.5-1.11928813-2.5-2.5s1.11928813-2.5 2.5-2.5h1.02786405c.46573528 0 .92507448.10843528 1.34164078.31671843l1.13382424.56691212c.06026365-1.05041141.93116291-1.88363055 1.99667093-1.88363055 1.1045695 0 2 .8954305 2 2h2c2.209139 0 4 1.790861 4 4v1c0 2.209139-1.790861 4-4 4h-2v1h2c1.1045695 0 2 .8954305 2 2s-.8954305 2-2 2h-2c0 1.1045695-.8954305 2-2 2s-2-.8954305-2-2h-1c-2.209139 0-4-1.790861-4-4v-1c0-2.209139 1.790861-4 4-4zm0-2v-2.05652691c-.14564246-.03538148-.28733393-.08714006-.42229124-.15461871l-1.15541752-.57770876c-.27771087-.13885544-.583937-.21114562-.89442719-.21114562h-1.02786405c-.82842712 0-1.5.67157288-1.5 1.5s.67157288 1.5 1.5 1.5zm4 1v1h1.5c.2761424 0 .5-.22385763.5-.5s-.2238576-.5-.5-.5zm-1 1v-5c0-.55228475-.44771525-1-1-1s-1 .44771525-1 1v5zm-2 4v5c0 .5522847.44771525 1 1 1s1-.4477153 1-1v-5zm3 2v2h2c.5522847 0 1-.4477153 1-1s-.4477153-1-1-1zm-4-1v-1h-.5c-.27614237 0-.5.2238576-.5.5s.22385763.5.5.5zm-3.5-9h1c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5z" fill-rule="evenodd"/></symbol><symbol id="icon-success" viewBox="0 0 18 18"><path d="m9 0c4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9zm3.4860198 4.98163161-4.71802968 5.50657859-2.62834168-2.02300024c-.42862421-.36730544-1.06564993-.30775346-1.42283677.13301307-.35718685.44076653-.29927542 1.0958383.12934879 1.46314377l3.40735508 2.7323063c.42215801.3385221 1.03700951.2798252 1.38749189-.1324571l5.38450527-6.33394549c.3613513-.43716226.3096573-1.09278382-.115462-1.46437175-.4251192-.37158792-1.0626796-.31842941-1.4240309.11873285z" fill-rule="evenodd"/></symbol><symbol id="icon-table" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587l-4.0059107-.001.001.001h-1l-.001-.001h-5l.001.001h-1l-.001-.001-3.00391071.001c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm-11.0059107 5h-3.999v6.9941413c0 .5572961.44630695 1.0058587.99508929 1.0058587h3.00391071zm6 0h-5v8h5zm5.0059107-4h-4.0059107v3h5.001v1h-5.001v7.999l4.0059107.001c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-12.5049107 9c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.2238576.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.2238576-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.2238576.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.2238576-.5-.5s.22385763-.5.5-.5zm-6-2c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-1c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm12 0c.2761424 0 .5.22385763.5.5s-.2238576.5-.5.5h-2c-.2761424 0-.5-.22385763-.5-.5s.2238576-.5.5-.5zm-6 0c.27614237 0 .5.22385763.5.5s-.22385763.5-.5.5h-2c-.27614237 0-.5-.22385763-.5-.5s.22385763-.5.5-.5zm1.499-5h-5v3h5zm-6 0h-3.00391071c-.54871518 0-.99508929.44887827-.99508929 1.00585866v1.99414134h3.999z" fill-rule="evenodd"/></symbol><symbol id="icon-tick-circle" viewBox="0 0 24 24"><path d="m12 2c5.5228475 0 10 4.4771525 10 10s-4.4771525 10-10 10-10-4.4771525-10-10 4.4771525-10 10-10zm0 1c-4.97056275 0-9 4.02943725-9 9 0 4.9705627 4.02943725 9 9 9 4.9705627 0 9-4.0294373 9-9 0-4.97056275-4.0294373-9-9-9zm4.2199868 5.36606669c.3613514-.43716226.9989118-.49032077 1.424031-.11873285s.4768133 1.02720949.115462 1.46437175l-6.093335 6.94397871c-.3622945.4128716-.9897871.4562317-1.4054264.0971157l-3.89719065-3.3672071c-.42862421-.3673054-.48653564-1.0223772-.1293488-1.4631437s.99421256-.5003185 1.42283677-.1330131l3.11097438 2.6987741z" fill-rule="evenodd"/></symbol><symbol id="icon-tick" viewBox="0 0 16 16"><path d="m6.76799012 9.21106946-3.1109744-2.58349728c-.42862421-.35161617-1.06564993-.29460792-1.42283677.12733148s-.29927541 1.04903009.1293488 1.40064626l3.91576307 3.23873978c.41034319.3393961 1.01467563.2976897 1.37450571-.0948578l6.10568327-6.660841c.3613513-.41848908.3096572-1.04610608-.115462-1.4018218-.4251192-.35571573-1.0626796-.30482786-1.424031.11366122z" fill-rule="evenodd"/></symbol><symbol id="icon-update" viewBox="0 0 18 18"><path d="m1 13v1c0 .5522847.44771525 1 1 1h14c.5522847 0 1-.4477153 1-1v-1h-1v-10h-14v10zm16-1h1v2c0 1.1045695-.8954305 2-2 2h-14c-1.1045695 0-2-.8954305-2-2v-2h1v-9c0-.55228475.44771525-1 1-1h14c.5522847 0 1 .44771525 1 1zm-1 0v1h-4.5857864l-1 1h-2.82842716l-1-1h-4.58578644v-1h5l1 1h2l1-1zm-13-8h12v7h-12zm1 1v5h10v-5zm1 1h4v1h-4zm0 2h4v1h-4z" fill-rule="evenodd"/></symbol><symbol id="icon-upload" viewBox="0 0 18 18"><path d="m10.0046024 0c.5497429 0 1.3179837.32258606 1.707238.71184039l4.5763192 4.57631922c.3931386.39313859.7118404 1.16760135.7118404 1.71431368v8.98899651c0 1.1092806-.8945138 2.0085302-1.9940603 2.0085302h-12.01187942c-1.10128908 0-1.99406028-.8926228-1.99406028-1.9950893v-14.00982141c0-1.10185739.88743329-1.99508929 1.99961498-1.99508929zm0 1h-7.00498742c-.55709576 0-.99961498.44271433-.99961498.99508929v14.00982141c0 .5500396.44491393.9950893.99406028.9950893h12.01187942c.5463747 0 .9940603-.4506622.9940603-1.0085302v-8.98899651c0-.28393444-.2150684-.80332809-.4189472-1.0072069l-4.5763192-4.57631922c-.2038461-.20384606-.718603-.41894717-1.0001312-.41894717zm-1.85576936 4.14572769c.19483374-.19483375.51177826-.19377714.70556874.00001334l2.59099082 2.59099079c.1948411.19484112.1904373.51514474.0027906.70279143-.1932998.19329987-.5046517.19237083-.7001856-.00692852l-1.74638687-1.7800176v6.14827687c0 .2717771-.23193359.492096-.5.492096-.27614237 0-.5-.216372-.5-.492096v-6.14827641l-1.74627892 1.77990922c-.1933927.1971171-.51252214.19455839-.70016883.0069117-.19329987-.19329988-.19100584-.50899493.00277731-.70277808z" fill-rule="evenodd"/></symbol><symbol id="icon-video" viewBox="0 0 18 18"><path d="m16.0049107 2c1.1018574 0 1.9950893.89706013 1.9950893 2.00585866v9.98828264c0 1.1078052-.8926228 2.0058587-1.9950893 2.0058587h-14.00982141c-1.10185739 0-1.99508929-.8970601-1.99508929-2.0058587v-9.98828264c0-1.10780515.8926228-2.00585866 1.99508929-2.00585866zm0 1h-14.00982141c-.54871518 0-.99508929.44887827-.99508929 1.00585866v9.98828264c0 .5572961.44630695 1.0058587.99508929 1.0058587h14.00982141c.5487152 0 .9950893-.4488783.9950893-1.0058587v-9.98828264c0-.55729607-.446307-1.00585866-.9950893-1.00585866zm-8.30912922 2.24944486 4.60460462 2.73982242c.9365543.55726659.9290753 1.46522435 0 2.01804082l-4.60460462 2.7398224c-.93655425.5572666-1.69578148.1645632-1.69578148-.8937585v-5.71016863c0-1.05087579.76670616-1.446575 1.69578148-.89375851zm-.67492769.96085624v5.5750128c0 .2995102-.10753745.2442517.16578928.0847713l4.58452283-2.67497259c.3050619-.17799716.3051624-.21655446 0-.39461026l-4.58452283-2.67497264c-.26630747-.15538481-.16578928-.20699944-.16578928.08477139z" fill-rule="evenodd"/></symbol><symbol id="icon-warning" viewBox="0 0 18 18"><path d="m9 11.75c.69035594 0 1.25.5596441 1.25 1.25s-.55964406 1.25-1.25 1.25-1.25-.5596441-1.25-1.25.55964406-1.25 1.25-1.25zm.41320045-7.75c.55228475 0 1.00000005.44771525 1.00000005 1l-.0034543.08304548-.3333333 4c-.043191.51829212-.47645714.91695452-.99654578.91695452h-.15973424c-.52008864 0-.95335475-.3986624-.99654576-.91695452l-.33333333-4c-.04586475-.55037702.36312325-1.03372649.91350028-1.07959124l.04148683-.00259031zm-.41320045 14c-4.97056275 0-9-4.0294373-9-9 0-4.97056275 4.02943725-9 9-9 4.9705627 0 9 4.02943725 9 9 0 4.9705627-4.0294373 9-9 9z" fill-rule="evenodd"/></symbol><symbol id="icon-checklist-banner" viewBox="0 0 56.69 56.69"><path style="fill:none" d="M0 0h56.69v56.69H0z"/><clipPath id="b"><use xlink:href="#a" style="overflow:visible"/></clipPath><path d="M21.14 34.46c0-6.77 5.48-12.26 12.24-12.26s12.24 5.49 12.24 12.26-5.48 12.26-12.24 12.26c-6.76-.01-12.24-5.49-12.24-12.26zm19.33 10.66 10.23 9.22s1.21 1.09 2.3-.12l2.09-2.32s1.09-1.21-.12-2.3l-10.23-9.22m-19.29-5.92c0-4.38 3.55-7.94 7.93-7.94s7.93 3.55 7.93 7.94c0 4.38-3.55 7.94-7.93 7.94-4.38-.01-7.93-3.56-7.93-7.94zm17.58 12.99 4.14-4.81" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round"/><path d="M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5m14.42-5.2V4.86s0-2.93-2.93-2.93H4.13s-2.93 0-2.93 2.93v37.57s0 2.93 2.93 2.93h15.01M8.26 9.75H28.6M8.26 15.98H28.6m-20.34 6.2h12.5" style="clip-path:url(#b);fill:none;stroke:#01324b;stroke-width:2;stroke-linecap:round;stroke-linejoin:round"/></symbol><symbol id="icon-chevron-down" viewBox="0 0 16 16"><path d="m5.58578644 3-3.29289322-3.29289322c-.39052429-.39052429-.39052429-1.02368927 0-1.41421356s1.02368927-.39052429 1.41421356 0l4 4c.39052429.39052429.39052429 1.02368927 0 1.41421356l-4 4c-.39052429.39052429-1.02368927.39052429-1.41421356 0s-.39052429-1.02368927 0-1.41421356z" fill-rule="evenodd" transform="matrix(0 1 -1 0 11 1)"/></symbol><symbol id="icon-eds-menu" viewBox="0 0 24 24"><path d="M21.09 5c.503 0 .91.448.91 1s-.407 1-.91 1H2.91C2.406 7 2 6.552 2 6s.407-1 .91-1h18.18Zm-3.817 6c.401 0 .727.448.727 1s-.326 1-.727 1H2.727C2.326 13 2 12.552 2 12s.326-1 .727-1h14.546Zm3.818 6c.502 0 .909.448.909 1s-.407 1-.91 1H2.91c-.503 0-.91-.448-.91-1s.407-1 .91-1h18.18Z" fill-rule="nonzero"/></symbol><symbol id="icon-eds-search" viewBox="0 0 24 24"><path d="M11 1c5.523 0 10 4.477 10 10 0 2.4-.846 4.604-2.256 6.328l3.963 3.965a1 1 0 0 1-1.414 1.414l-3.965-3.963A9.959 9.959 0 0 1 11 21C5.477 21 1 16.523 1 11S5.477 1 11 1Zm0 2a8 8 0 1 0 0 16 8 8 0 0 0 0-16Z" fill-rule="nonzero"/></symbol><symbol id="icon-eds-user-single" viewBox="0 0 24 24"><path fill-rule="nonzero" d="M12 12c5.498 0 10 4.001 10 9a1 1 0 0 1-2 0c0-3.838-3.557-7-8-7s-8 3.162-8 7a1 1 0 0 1-2 0c0-4.999 4.502-9 10-9Zm0-11a5 5 0 1 0 0 10 5 5 0 0 0 0-10Zm0 2a3 3 0 1 1 0 6 3 3 0 0 1 0-6Z"/></symbol><symbol id="icon-expand-image" viewBox="0 0 18 18"><path d="m7.49754099 11.9178212c.38955542-.3895554.38761957-1.0207846-.00290473-1.4113089-.39324695-.3932469-1.02238878-.3918247-1.41130883-.0029047l-4.10273549 4.1027355.00055454-3.5103985c.00008852-.5603185-.44832171-1.006032-1.00155062-1.0059446-.53903074.0000852-.97857527.4487442-.97866268 1.0021075l-.00093318 5.9072465c-.00008751.553948.44841131 1.001882 1.00174994 1.0017946l5.906983-.0009331c.5539233-.0000875 1.00197907-.4486389 1.00206646-1.0018679.00008515-.5390307-.45026621-.9784332-1.00588841-.9783454l-3.51010549.0005545zm3.00571741-5.83449376c-.3895554.38955541-.3876196 1.02078454.0029047 1.41130883.393247.39324696 1.0223888.39182478 1.4113089.00290473l4.1027355-4.10273549-.0005546 3.5103985c-.0000885.56031852.4483217 1.006032 1.0015506 1.00594461.5390308-.00008516.9785753-.44874418.9786627-1.00210749l.0009332-5.9072465c.0000875-.553948-.4484113-1.00188204-1.0017499-1.00179463l-5.906983.00093313c-.5539233.00008751-1.0019791.44863892-1.0020665 1.00186784-.0000852.53903074.4502662.97843325 1.0058884.97834547l3.5101055-.00055449z" fill-rule="evenodd"/></symbol><symbol id="icon-github" viewBox="0 0 100 100"><path fill-rule="evenodd" clip-rule="evenodd" d="M48.854 0C21.839 0 0 22 0 49.217c0 21.756 13.993 40.172 33.405 46.69 2.427.49 3.316-1.059 3.316-2.362 0-1.141-.08-5.052-.08-9.127-13.59 2.934-16.42-5.867-16.42-5.867-2.184-5.704-5.42-7.17-5.42-7.17-4.448-3.015.324-3.015.324-3.015 4.934.326 7.523 5.052 7.523 5.052 4.367 7.496 11.404 5.378 14.235 4.074.404-3.178 1.699-5.378 3.074-6.6-10.839-1.141-22.243-5.378-22.243-24.283 0-5.378 1.94-9.778 5.014-13.2-.485-1.222-2.184-6.275.486-13.038 0 0 4.125-1.304 13.426 5.052a46.97 46.97 0 0 1 12.214-1.63c4.125 0 8.33.571 12.213 1.63 9.302-6.356 13.427-5.052 13.427-5.052 2.67 6.763.97 11.816.485 13.038 3.155 3.422 5.015 7.822 5.015 13.2 0 18.905-11.404 23.06-22.324 24.283 1.78 1.548 3.316 4.481 3.316 9.126 0 6.6-.08 11.897-.08 13.526 0 1.304.89 2.853 3.316 2.364 19.412-6.52 33.405-24.935 33.405-46.691C97.707 22 75.788 0 48.854 0z"/></symbol><symbol id="icon-search-filter" viewBox="0 0 29 29"><path d="M28 9H11a1 1 0 0 1 0-2h17a1 1 0 0 1 0 2ZM7 9H4a1 1 0 0 1 0-2h3a1 1 0 0 1 0 2Zm14 8H4a1 1 0 0 1 0-2h17a1 1 0 0 1 0 2Zm-10 8H4a1 1 0 0 1 0-2h7a1 1 0 0 1 0 2Z"/><path d="M9 11a3 3 0 1 1 3-3 3 3 0 0 1-3 3Zm0-4a1 1 0 1 0 1 1 1 1 0 0 0-1-1Zm14 12a3 3 0 1 1 3-3 3 3 0 0 1-3 3Zm0-4a1 1 0 1 0 1 1 1 1 0 0 0-1-1ZM13 27a3 3 0 1 1 3-3 3 3 0 0 1-3 3Zm0-4a1 1 0 1 0 1 1 1 1 0 0 0-1-1Z"/><path d="M28 17h-3a1 1 0 0 1 0-2h3a1 1 0 0 1 0 2Zm0 8H15a1 1 0 0 1 0-2h13a1 1 0 0 1 0 2Z"/><path d="M0 0h32v32H0z" style="fill:none"/></symbol><symbol id="icon-springer-arrow-left"><path d="M15 7a1 1 0 000-2H3.385l2.482-2.482a.994.994 0 00.02-1.403 1.001 1.001 0 00-1.417 0L.294 5.292a1.001 1.001 0 000 1.416l4.176 4.177a.991.991 0 001.4.016 1 1 0 00-.003-1.42L3.385 7H15z"/></symbol><symbol id="icon-springer-arrow-right"><path d="M1 7a1 1 0 010-2h11.615l-2.482-2.482a.994.994 0 01-.02-1.403 1.001 1.001 0 011.417 0l4.176 4.177a1.001 1.001 0 010 1.416l-4.176 4.177a.991.991 0 01-1.4.016 1 1 0 01.003-1.42L12.615 7H1z"/></symbol><symbol id="icon-submit-open" viewBox="0 0 16 17"><path d="M12 0c1.10457 0 2 .895431 2 2v5c0 .276142-.223858.5-.5.5S13 7.276142 13 7V2c0-.512836-.38604-.935507-.883379-.993272L12 1H6v3c0 1.10457-.89543 2-2 2H1v8c0 .512836.38604.935507.883379.993272L2 15h6.5c.276142 0 .5.223858.5.5s-.223858.5-.5.5H2c-1.104569 0-2-.89543-2-2V5.828427c0-.530433.210714-1.039141.585786-1.414213L4.414214.585786C4.789286.210714 5.297994 0 5.828427 0H12Zm3.41 11.14c.250899.250899.250274.659726 0 .91-.242954.242954-.649606.245216-.9-.01l-1.863671-1.900337.001043 5.869492c0 .356992-.289839.637138-.647372.637138-.347077 0-.647371-.285256-.647371-.637138l-.001043-5.869492L9.5 12.04c-.253166.258042-.649726.260274-.9.01-.242954-.242954-.252269-.657731 0-.91l2.942184-2.951303c.250908-.250909.66127-.252277.91353-.000017L15.41 11.14ZM5 1.413 1.413 5H4c.552285 0 1-.447715 1-1V1.413ZM11 3c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Zm0 2c.276142 0 .5.223858.5.5s-.223858.5-.5.5H7.5c-.276142 0-.5-.223858-.5-.5s.223858-.5.5-.5H11Z" fill-rule="nonzero"/></symbol></svg>
</div>
</body>
</html>


